// This is an unfortunate replacement for @sindresorhus/is that we need to
// re-implement for performance purposes. In particular the is.observable()
// check is expensive, and unnecessary for our purposes. The values returned
// are compatible with @sindresorhus/is, however.

const typeofs = ["string", "number", "bigint", "symbol"];

const objectTypeNames = [
  "Function",
  "Generator",
  "AsyncGenerator",
  "GeneratorFunction",
  "AsyncGeneratorFunction",
  "AsyncFunction",
  "Observable",
  "Array",
  "Buffer",
  "Object",
  "RegExp",
  "Date",
  "Error",
  "Map",
  "Set",
  "WeakMap",
  "WeakSet",
  "ArrayBuffer",
  "SharedArrayBuffer",
  "DataView",
  "Promise",
  "URL",
  "HTMLElement",
  "Int8Array",
  "Uint8Array",
  "Uint8ClampedArray",
  "Int16Array",
  "Uint16Array",
  "Int32Array",
  "Uint32Array",
  "Float32Array",
  "Float64Array",
  "BigInt64Array",
  "BigUint64Array",
];

/**
 * @param {any} value
 * @returns {string}
 */
function is(value) {
  if (value === null) {
    return "null";
  }
  if (value === undefined) {
    return "undefined";
  }
  if (value === true || value === false) {
    return "boolean";
  }
  const typeOf = typeof value;
  if (typeofs.includes(typeOf)) {
    return typeOf;
  }
  /* c8 ignore next 4 */
  // not going to bother testing this, it's not going to be valid anyway
  if (typeOf === "function") {
    return "Function";
  }
  if (Array.isArray(value)) {
    return "Array";
  }
  if (isBuffer$1(value)) {
    return "Buffer";
  }
  const objectType = getObjectType(value);
  if (objectType) {
    return objectType;
  }
  /* c8 ignore next */
  return "Object";
}

/**
 * @param {any} value
 * @returns {boolean}
 */
function isBuffer$1(value) {
  return (
    value &&
    value.constructor &&
    value.constructor.isBuffer &&
    value.constructor.isBuffer.call(null, value)
  );
}

/**
 * @param {any} value
 * @returns {string|undefined}
 */
function getObjectType(value) {
  const objectTypeName = Object.prototype.toString.call(value).slice(8, -1);
  if (objectTypeNames.includes(objectTypeName)) {
    return objectTypeName;
  }
  /* c8 ignore next */
  return undefined;
}

class Type {
  /**
   * @param {number} major
   * @param {string} name
   * @param {boolean} terminal
   */
  constructor(major, name, terminal) {
    this.major = major;
    this.majorEncoded = major << 5;
    this.name = name;
    this.terminal = terminal;
  }

  /* c8 ignore next 3 */
  toString() {
    return `Type[${this.major}].${this.name}`;
  }

  /**
   * @param {Type} typ
   * @returns {number}
   */
  compare(typ) {
    /* c8 ignore next 1 */
    return this.major < typ.major ? -1 : this.major > typ.major ? 1 : 0;
  }
}

// convert to static fields when better supported
Type.uint = new Type(0, "uint", true);
Type.negint = new Type(1, "negint", true);
Type.bytes = new Type(2, "bytes", true);
Type.string = new Type(3, "string", true);
Type.array = new Type(4, "array", false);
Type.map = new Type(5, "map", false);
Type.tag = new Type(6, "tag", false); // terminal?
Type.float = new Type(7, "float", true);
Type.false = new Type(7, "false", true);
Type.true = new Type(7, "true", true);
Type.null = new Type(7, "null", true);
Type.undefined = new Type(7, "undefined", true);
Type.break = new Type(7, "break", true);
// Type.indefiniteLength = new Type(0, 'indefiniteLength', true)

class Token {
  /**
   * @param {Type} type
   * @param {any} [value]
   * @param {number} [encodedLength]
   */
  constructor(type, value, encodedLength) {
    this.type = type;
    this.value = value;
    this.encodedLength = encodedLength;
    /** @type {Uint8Array|undefined} */
    this.encodedBytes = undefined;
    /** @type {Uint8Array|undefined} */
    this.byteValue = undefined;
  }

  /* c8 ignore next 3 */
  toString() {
    return `Token[${this.type}].${this.value}`;
  }
}

// Use Uint8Array directly in the browser, use Buffer in Node.js but don't
// speak its name directly to avoid bundlers pulling in the `Buffer` polyfill

// @ts-ignore
const useBuffer =
  globalThis.process &&
  // @ts-ignore
  !globalThis.process.browser &&
  // @ts-ignore
  globalThis.Buffer &&
  // @ts-ignore
  typeof globalThis.Buffer.isBuffer === "function";

const textDecoder$1 = new TextDecoder();
const textEncoder$2 = new TextEncoder();

/**
 * @param {Uint8Array} buf
 * @returns {boolean}
 */
function isBuffer(buf) {
  // @ts-ignore
  return useBuffer && globalThis.Buffer.isBuffer(buf);
}

/**
 * @param {Uint8Array|number[]} buf
 * @returns {Uint8Array}
 */
function asU8A(buf) {
  /* c8 ignore next */
  if (!(buf instanceof Uint8Array)) {
    return Uint8Array.from(buf);
  }
  return isBuffer(buf)
    ? new Uint8Array(buf.buffer, buf.byteOffset, buf.byteLength)
    : buf;
}

const toString$2 = useBuffer
  ? // eslint-disable-line operator-linebreak
    /**
     * @param {Uint8Array} bytes
     * @param {number} start
     * @param {number} end
     */
    (bytes, start, end) => {
      return end - start > 64
        ? // eslint-disable-line operator-linebreak
          // @ts-ignore
          globalThis.Buffer.from(bytes.subarray(start, end)).toString("utf8")
        : utf8Slice(bytes, start, end);
    }
  : /* c8 ignore next 11 */
    // eslint-disable-line operator-linebreak
    /**
     * @param {Uint8Array} bytes
     * @param {number} start
     * @param {number} end
     */
    (bytes, start, end) => {
      return end - start > 64
        ? textDecoder$1.decode(bytes.subarray(start, end))
        : utf8Slice(bytes, start, end);
    };

const fromString$1 = useBuffer
  ? // eslint-disable-line operator-linebreak
    /**
     * @param {string} string
     */
    (string) => {
      return string.length > 64
        ? // eslint-disable-line operator-linebreak
          // @ts-ignore
          globalThis.Buffer.from(string)
        : utf8ToBytes$1(string);
    }
  : /* c8 ignore next 7 */
    // eslint-disable-line operator-linebreak
    /**
     * @param {string} string
     */
    (string) => {
      return string.length > 64
        ? textEncoder$2.encode(string)
        : utf8ToBytes$1(string);
    };

/**
 * Buffer variant not fast enough for what we need
 * @param {number[]} arr
 * @returns {Uint8Array}
 */
const fromArray = (arr) => {
  return Uint8Array.from(arr);
};

const slice$1 = useBuffer
  ? // eslint-disable-line operator-linebreak
    /**
     * @param {Uint8Array} bytes
     * @param {number} start
     * @param {number} end
     */
    (bytes, start, end) => {
      if (isBuffer(bytes)) {
        return new Uint8Array(bytes.subarray(start, end));
      }
      return bytes.slice(start, end);
    }
  : /* c8 ignore next 9 */
    // eslint-disable-line operator-linebreak
    /**
     * @param {Uint8Array} bytes
     * @param {number} start
     * @param {number} end
     */
    (bytes, start, end) => {
      return bytes.slice(start, end);
    };

const concat$1 = useBuffer
  ? // eslint-disable-line operator-linebreak
    /**
     * @param {Uint8Array[]} chunks
     * @param {number} length
     * @returns {Uint8Array}
     */
    (chunks, length) => {
      // might get a stray plain Array here
      /* c8 ignore next 1 */
      chunks = chunks.map((c) =>
        c instanceof Uint8Array
          ? c
          : // this case is occasionally missed during test runs so becomes coverage-flaky
            /* c8 ignore next 4 */
            // eslint-disable-line operator-linebreak
            // @ts-ignore
            globalThis.Buffer.from(c)
      );
      // @ts-ignore
      return asU8A(globalThis.Buffer.concat(chunks, length));
    }
  : /* c8 ignore next 19 */
    // eslint-disable-line operator-linebreak
    /**
     * @param {Uint8Array[]} chunks
     * @param {number} length
     * @returns {Uint8Array}
     */
    (chunks, length) => {
      const out = new Uint8Array(length);
      let off = 0;
      for (let b of chunks) {
        if (off + b.length > out.length) {
          // final chunk that's bigger than we need
          b = b.subarray(0, out.length - off);
        }
        out.set(b, off);
        off += b.length;
      }
      return out;
    };

const alloc$1 = useBuffer
  ? // eslint-disable-line operator-linebreak
    /**
     * @param {number} size
     * @returns {Uint8Array}
     */
    (size) => {
      // we always write over the contents we expose so this should be safe
      // @ts-ignore
      return globalThis.Buffer.allocUnsafe(size);
    }
  : /* c8 ignore next 8 */
    // eslint-disable-line operator-linebreak
    /**
     * @param {number} size
     * @returns {Uint8Array}
     */
    (size) => {
      return new Uint8Array(size);
    };

/**
 * @param {Uint8Array} b1
 * @param {Uint8Array} b2
 * @returns {number}
 */
function compare$1(b1, b2) {
  /* c8 ignore next 5 */
  if (isBuffer(b1) && isBuffer(b2)) {
    // probably not possible to get here in the current API
    // @ts-ignore Buffer
    return b1.compare(b2);
  }
  for (let i = 0; i < b1.length; i++) {
    if (b1[i] === b2[i]) {
      continue;
    }
    return b1[i] < b2[i] ? -1 : 1;
  } /* c8 ignore next 3 */
  return 0;
}

// The below code is taken from https://github.com/google/closure-library/blob/8598d87242af59aac233270742c8984e2b2bdbe0/closure/goog/crypt/crypt.js#L117-L143
// Licensed Apache-2.0.

/**
 * @param {string} str
 * @returns {number[]}
 */
function utf8ToBytes$1(str) {
  const out = [];
  let p = 0;
  for (let i = 0; i < str.length; i++) {
    let c = str.charCodeAt(i);
    if (c < 128) {
      out[p++] = c;
    } else if (c < 2048) {
      out[p++] = (c >> 6) | 192;
      out[p++] = (c & 63) | 128;
    } else if (
      (c & 0xfc00) === 0xd800 &&
      i + 1 < str.length &&
      (str.charCodeAt(i + 1) & 0xfc00) === 0xdc00
    ) {
      // Surrogate Pair
      c = 0x10000 + ((c & 0x03ff) << 10) + (str.charCodeAt(++i) & 0x03ff);
      out[p++] = (c >> 18) | 240;
      out[p++] = ((c >> 12) & 63) | 128;
      out[p++] = ((c >> 6) & 63) | 128;
      out[p++] = (c & 63) | 128;
    } else {
      out[p++] = (c >> 12) | 224;
      out[p++] = ((c >> 6) & 63) | 128;
      out[p++] = (c & 63) | 128;
    }
  }
  return out;
}

// The below code is mostly taken from https://github.com/feross/buffer
// Licensed MIT. Copyright (c) Feross Aboukhadijeh

/**
 * @param {Uint8Array} buf
 * @param {number} offset
 * @param {number} end
 * @returns {string}
 */
function utf8Slice(buf, offset, end) {
  const res = [];

  while (offset < end) {
    const firstByte = buf[offset];
    let codePoint = null;
    let bytesPerSequence =
      firstByte > 0xef ? 4 : firstByte > 0xdf ? 3 : firstByte > 0xbf ? 2 : 1;

    if (offset + bytesPerSequence <= end) {
      let secondByte, thirdByte, fourthByte, tempCodePoint;

      switch (bytesPerSequence) {
        case 1:
          if (firstByte < 0x80) {
            codePoint = firstByte;
          }
          break;
        case 2:
          secondByte = buf[offset + 1];
          if ((secondByte & 0xc0) === 0x80) {
            tempCodePoint = ((firstByte & 0x1f) << 0x6) | (secondByte & 0x3f);
            if (tempCodePoint > 0x7f) {
              codePoint = tempCodePoint;
            }
          }
          break;
        case 3:
          secondByte = buf[offset + 1];
          thirdByte = buf[offset + 2];
          if ((secondByte & 0xc0) === 0x80 && (thirdByte & 0xc0) === 0x80) {
            tempCodePoint =
              ((firstByte & 0xf) << 0xc) |
              ((secondByte & 0x3f) << 0x6) |
              (thirdByte & 0x3f);
            /* c8 ignore next 3 */
            if (
              tempCodePoint > 0x7ff &&
              (tempCodePoint < 0xd800 || tempCodePoint > 0xdfff)
            ) {
              codePoint = tempCodePoint;
            }
          }
          break;
        case 4:
          secondByte = buf[offset + 1];
          thirdByte = buf[offset + 2];
          fourthByte = buf[offset + 3];
          if (
            (secondByte & 0xc0) === 0x80 &&
            (thirdByte & 0xc0) === 0x80 &&
            (fourthByte & 0xc0) === 0x80
          ) {
            tempCodePoint =
              ((firstByte & 0xf) << 0x12) |
              ((secondByte & 0x3f) << 0xc) |
              ((thirdByte & 0x3f) << 0x6) |
              (fourthByte & 0x3f);
            if (tempCodePoint > 0xffff && tempCodePoint < 0x110000) {
              codePoint = tempCodePoint;
            }
          }
      }
    }

    /* c8 ignore next 5 */
    if (codePoint === null) {
      // we did not generate a valid codePoint so insert a
      // replacement char (U+FFFD) and advance only 1 byte
      codePoint = 0xfffd;
      bytesPerSequence = 1;
    } else if (codePoint > 0xffff) {
      // encode to utf16 (surrogate pair dance)
      codePoint -= 0x10000;
      res.push(((codePoint >>> 10) & 0x3ff) | 0xd800);
      codePoint = 0xdc00 | (codePoint & 0x3ff);
    }

    res.push(codePoint);
    offset += bytesPerSequence;
  }

  return decodeCodePointsArray(res);
}

// Based on http://stackoverflow.com/a/22747272/680742, the browser with
// the lowest limit is Chrome, with 0x10000 args.
// We go 1 magnitude less, for safety
const MAX_ARGUMENTS_LENGTH = 0x1000;

/**
 * @param {number[]} codePoints
 * @returns {string}
 */
function decodeCodePointsArray(codePoints) {
  const len = codePoints.length;
  if (len <= MAX_ARGUMENTS_LENGTH) {
    return String.fromCharCode.apply(String, codePoints); // avoid extra slice()
  }
  /* c8 ignore next 10 */
  // Decode in chunks to avoid "call stack size exceeded".
  let res = "";
  let i = 0;
  while (i < len) {
    res += String.fromCharCode.apply(
      String,
      codePoints.slice(i, (i += MAX_ARGUMENTS_LENGTH))
    );
  }
  return res;
}

/**
 * Bl is a list of byte chunks, similar to https://github.com/rvagg/bl but for
 * writing rather than reading.
 * A Bl object accepts set() operations for individual bytes and copyTo() for
 * inserting byte arrays. These write operations don't automatically increment
 * the internal cursor so its "length" won't be changed. Instead, increment()
 * must be called to extend its length to cover the inserted data.
 * The toBytes() call will convert all internal memory to a single Uint8Array of
 * the correct length, truncating any data that is stored but hasn't been
 * included by an increment().
 * get() can retrieve a single byte.
 * All operations (except toBytes()) take an "offset" argument that will perform
 * the write at the offset _from the current cursor_. For most operations this
 * will be `0` to write at the current cursor position but it can be ahead of
 * the current cursor. Negative offsets probably work but are untested.
 */

// the ts-ignores in this file are almost all for the `Uint8Array|number[]` duality that exists
// for perf reasons. Consider better approaches to this or removing it entirely, it is quite
// risky because of some assumptions about small chunks === number[] and everything else === Uint8Array.

const defaultChunkSize = 256;

class Bl {
  /**
   * @param {number} [chunkSize]
   */
  constructor(chunkSize = defaultChunkSize) {
    this.chunkSize = chunkSize;
    /** @type {number} */
    this.cursor = 0;
    /** @type {number} */
    this.maxCursor = -1;
    /** @type {(Uint8Array|number[])[]} */
    this.chunks = [];
    // keep the first chunk around if we can to save allocations for future encodes
    /** @type {Uint8Array|number[]|null} */
    this._initReuseChunk = null;
  }

  reset() {
    this.cursor = 0;
    this.maxCursor = -1;
    if (this.chunks.length) {
      this.chunks = [];
    }
    if (this._initReuseChunk !== null) {
      this.chunks.push(this._initReuseChunk);
      this.maxCursor = this._initReuseChunk.length - 1;
    }
  }

  /**
   * @param {Uint8Array|number[]} bytes
   */
  push(bytes) {
    let topChunk = this.chunks[this.chunks.length - 1];
    const newMax = this.cursor + bytes.length;
    if (newMax <= this.maxCursor + 1) {
      // we have at least one chunk and we can fit these bytes into that chunk
      const chunkPos = topChunk.length - (this.maxCursor - this.cursor) - 1;
      // @ts-ignore
      topChunk.set(bytes, chunkPos);
    } else {
      // can't fit it in
      if (topChunk) {
        // trip the last chunk to `cursor` if we need to
        const chunkPos = topChunk.length - (this.maxCursor - this.cursor) - 1;
        if (chunkPos < topChunk.length) {
          // @ts-ignore
          this.chunks[this.chunks.length - 1] = topChunk.subarray(0, chunkPos);
          this.maxCursor = this.cursor - 1;
        }
      }
      if (bytes.length < 64 && bytes.length < this.chunkSize) {
        // make a new chunk and copy the new one into it
        topChunk = alloc$1(this.chunkSize);
        this.chunks.push(topChunk);
        this.maxCursor += topChunk.length;
        if (this._initReuseChunk === null) {
          this._initReuseChunk = topChunk;
        }
        // @ts-ignore
        topChunk.set(bytes, 0);
      } else {
        // push the new bytes in as its own chunk
        this.chunks.push(bytes);
        this.maxCursor += bytes.length;
      }
    }
    this.cursor += bytes.length;
  }

  /**
   * @param {boolean} [reset]
   * @returns {Uint8Array}
   */
  toBytes(reset = false) {
    let byts;
    if (this.chunks.length === 1) {
      const chunk = this.chunks[0];
      if (reset && this.cursor > chunk.length / 2) {
        /* c8 ignore next 2 */
        // @ts-ignore
        byts =
          this.cursor === chunk.length ? chunk : chunk.subarray(0, this.cursor);
        this._initReuseChunk = null;
        this.chunks = [];
      } else {
        // @ts-ignore
        byts = slice$1(chunk, 0, this.cursor);
      }
    } else {
      // @ts-ignore
      byts = concat$1(this.chunks, this.cursor);
    }
    if (reset) {
      this.reset();
    }
    return byts;
  }
}

const decodeErrPrefix = "CBOR decode error:";
const encodeErrPrefix = "CBOR encode error:";

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} need
 */
function assertEnoughData(data, pos, need) {
  if (data.length - pos < need) {
    throw new Error(`${decodeErrPrefix} not enough data for type`);
  }
}

/* globals BigInt */

const uintBoundaries = [
  24,
  256,
  65536,
  4294967296,
  BigInt("18446744073709551616"),
];

/**
 * @typedef {import('./bl.js').Bl} Bl
 * @typedef {import('../interface').DecodeOptions} DecodeOptions
 */

/**
 * @param {Uint8Array} data
 * @param {number} offset
 * @param {DecodeOptions} options
 * @returns {number}
 */
function readUint8(data, offset, options) {
  assertEnoughData(data, offset, 1);
  const value = data[offset];
  if (options.strict === true && value < uintBoundaries[0]) {
    throw new Error(
      `${decodeErrPrefix} integer encoded in more bytes than necessary (strict decode)`
    );
  }
  return value;
}

/**
 * @param {Uint8Array} data
 * @param {number} offset
 * @param {DecodeOptions} options
 * @returns {number}
 */
function readUint16(data, offset, options) {
  assertEnoughData(data, offset, 2);
  const value = (data[offset] << 8) | data[offset + 1];
  if (options.strict === true && value < uintBoundaries[1]) {
    throw new Error(
      `${decodeErrPrefix} integer encoded in more bytes than necessary (strict decode)`
    );
  }
  return value;
}

/**
 * @param {Uint8Array} data
 * @param {number} offset
 * @param {DecodeOptions} options
 * @returns {number}
 */
function readUint32(data, offset, options) {
  assertEnoughData(data, offset, 4);
  const value =
    data[offset] * 16777216 /* 2 ** 24 */ +
    (data[offset + 1] << 16) +
    (data[offset + 2] << 8) +
    data[offset + 3];
  if (options.strict === true && value < uintBoundaries[2]) {
    throw new Error(
      `${decodeErrPrefix} integer encoded in more bytes than necessary (strict decode)`
    );
  }
  return value;
}

/**
 * @param {Uint8Array} data
 * @param {number} offset
 * @param {DecodeOptions} options
 * @returns {number|bigint}
 */
function readUint64(data, offset, options) {
  // assume BigInt, convert back to Number if within safe range
  assertEnoughData(data, offset, 8);
  const hi =
    data[offset] * 16777216 /* 2 ** 24 */ +
    (data[offset + 1] << 16) +
    (data[offset + 2] << 8) +
    data[offset + 3];
  const lo =
    data[offset + 4] * 16777216 /* 2 ** 24 */ +
    (data[offset + 5] << 16) +
    (data[offset + 6] << 8) +
    data[offset + 7];
  const value = (BigInt(hi) << BigInt(32)) + BigInt(lo);
  if (options.strict === true && value < uintBoundaries[3]) {
    throw new Error(
      `${decodeErrPrefix} integer encoded in more bytes than necessary (strict decode)`
    );
  }
  if (value <= Number.MAX_SAFE_INTEGER) {
    return Number(value);
  }
  if (options.allowBigInt === true) {
    return value;
  }
  throw new Error(
    `${decodeErrPrefix} integers outside of the safe integer range are not supported`
  );
}

/* not required thanks to quick[] list
const oneByteTokens = new Array(24).fill(0).map((v, i) => new Token(Type.uint, i, 1))
export function decodeUintCompact (data, pos, minor, options) {
  return oneByteTokens[minor]
}
*/

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeUint8(data, pos, _minor, options) {
  return new Token(Type.uint, readUint8(data, pos + 1, options), 2);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeUint16(data, pos, _minor, options) {
  return new Token(Type.uint, readUint16(data, pos + 1, options), 3);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeUint32(data, pos, _minor, options) {
  return new Token(Type.uint, readUint32(data, pos + 1, options), 5);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeUint64(data, pos, _minor, options) {
  return new Token(Type.uint, readUint64(data, pos + 1, options), 9);
}

/**
 * @param {Bl} buf
 * @param {Token} token
 */
function encodeUint(buf, token) {
  return encodeUintValue(buf, 0, token.value);
}

/**
 * @param {Bl} buf
 * @param {number} major
 * @param {number|bigint} uint
 */
function encodeUintValue(buf, major, uint) {
  if (uint < uintBoundaries[0]) {
    const nuint = Number(uint);
    // pack into one byte, minor=0, additional=value
    buf.push([major | nuint]);
  } else if (uint < uintBoundaries[1]) {
    const nuint = Number(uint);
    // pack into two byte, minor=0, additional=24
    buf.push([major | 24, nuint]);
  } else if (uint < uintBoundaries[2]) {
    const nuint = Number(uint);
    // pack into three byte, minor=0, additional=25
    buf.push([major | 25, nuint >>> 8, nuint & 0xff]);
  } else if (uint < uintBoundaries[3]) {
    const nuint = Number(uint);
    // pack into five byte, minor=0, additional=26
    buf.push([
      major | 26,
      (nuint >>> 24) & 0xff,
      (nuint >>> 16) & 0xff,
      (nuint >>> 8) & 0xff,
      nuint & 0xff,
    ]);
  } else {
    const buint = BigInt(uint);
    if (buint < uintBoundaries[4]) {
      // pack into nine byte, minor=0, additional=27
      const set = [major | 27, 0, 0, 0, 0, 0, 0, 0];
      // simulate bitwise above 32 bits
      let lo = Number(buint & BigInt(0xffffffff));
      let hi = Number((buint >> BigInt(32)) & BigInt(0xffffffff));
      set[8] = lo & 0xff;
      lo = lo >> 8;
      set[7] = lo & 0xff;
      lo = lo >> 8;
      set[6] = lo & 0xff;
      lo = lo >> 8;
      set[5] = lo & 0xff;
      set[4] = hi & 0xff;
      hi = hi >> 8;
      set[3] = hi & 0xff;
      hi = hi >> 8;
      set[2] = hi & 0xff;
      hi = hi >> 8;
      set[1] = hi & 0xff;
      buf.push(set);
    } else {
      throw new Error(
        `${decodeErrPrefix} encountered BigInt larger than allowable range`
      );
    }
  }
}

/**
 * @param {Token} token
 * @returns {number}
 */
encodeUint.encodedSize = function encodedSize(token) {
  return encodeUintValue.encodedSize(token.value);
};

/**
 * @param {number} uint
 * @returns {number}
 */
encodeUintValue.encodedSize = function encodedSize(uint) {
  if (uint < uintBoundaries[0]) {
    return 1;
  }
  if (uint < uintBoundaries[1]) {
    return 2;
  }
  if (uint < uintBoundaries[2]) {
    return 3;
  }
  if (uint < uintBoundaries[3]) {
    return 5;
  }
  return 9;
};

/**
 * @param {Token} tok1
 * @param {Token} tok2
 * @returns {number}
 */
encodeUint.compareTokens = function compareTokens(tok1, tok2) {
  return tok1.value < tok2.value
    ? -1
    : tok1.value > tok2.value
    ? 1
    : /* c8 ignore next */ 0;
};

/* eslint-env es2020 */

/**
 * @typedef {import('./bl.js').Bl} Bl
 * @typedef {import('../interface').DecodeOptions} DecodeOptions
 */

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeNegint8(data, pos, _minor, options) {
  return new Token(Type.negint, -1 - readUint8(data, pos + 1, options), 2);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeNegint16(data, pos, _minor, options) {
  return new Token(Type.negint, -1 - readUint16(data, pos + 1, options), 3);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeNegint32(data, pos, _minor, options) {
  return new Token(Type.negint, -1 - readUint32(data, pos + 1, options), 5);
}

const neg1b = BigInt(-1);
const pos1b = BigInt(1);

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeNegint64(data, pos, _minor, options) {
  const int = readUint64(data, pos + 1, options);
  if (typeof int !== "bigint") {
    const value = -1 - int;
    if (value >= Number.MIN_SAFE_INTEGER) {
      return new Token(Type.negint, value, 9);
    }
  }
  if (options.allowBigInt !== true) {
    throw new Error(
      `${decodeErrPrefix} integers outside of the safe integer range are not supported`
    );
  }
  return new Token(Type.negint, neg1b - BigInt(int), 9);
}

/**
 * @param {Bl} buf
 * @param {Token} token
 */
function encodeNegint(buf, token) {
  const negint = token.value;
  const unsigned =
    typeof negint === "bigint" ? negint * neg1b - pos1b : negint * -1 - 1;
  encodeUintValue(buf, token.type.majorEncoded, unsigned);
}

/**
 * @param {Token} token
 * @returns {number}
 */
encodeNegint.encodedSize = function encodedSize(token) {
  const negint = token.value;
  const unsigned =
    typeof negint === "bigint" ? negint * neg1b - pos1b : negint * -1 - 1;
  /* c8 ignore next 4 */
  // handled by quickEncode, we shouldn't get here but it's included for completeness
  if (unsigned < uintBoundaries[0]) {
    return 1;
  }
  if (unsigned < uintBoundaries[1]) {
    return 2;
  }
  if (unsigned < uintBoundaries[2]) {
    return 3;
  }
  if (unsigned < uintBoundaries[3]) {
    return 5;
  }
  return 9;
};

/**
 * @param {Token} tok1
 * @param {Token} tok2
 * @returns {number}
 */
encodeNegint.compareTokens = function compareTokens(tok1, tok2) {
  // opposite of the uint comparison since we store the uint version in bytes
  return tok1.value < tok2.value
    ? 1
    : tok1.value > tok2.value
    ? -1
    : /* c8 ignore next */ 0;
};

/**
 * @typedef {import('./bl.js').Bl} Bl
 * @typedef {import('../interface').DecodeOptions} DecodeOptions
 */

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} prefix
 * @param {number} length
 * @returns {Token}
 */
function toToken$3(data, pos, prefix, length) {
  assertEnoughData(data, pos, prefix + length);
  const buf = slice$1(data, pos + prefix, pos + prefix + length);
  return new Token(Type.bytes, buf, prefix + length);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} minor
 * @param {DecodeOptions} _options
 * @returns {Token}
 */
function decodeBytesCompact(data, pos, minor, _options) {
  return toToken$3(data, pos, 1, minor);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeBytes8(data, pos, _minor, options) {
  return toToken$3(data, pos, 2, readUint8(data, pos + 1, options));
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeBytes16(data, pos, _minor, options) {
  return toToken$3(data, pos, 3, readUint16(data, pos + 1, options));
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeBytes32(data, pos, _minor, options) {
  return toToken$3(data, pos, 5, readUint32(data, pos + 1, options));
}

// TODO: maybe we shouldn't support this ..
/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeBytes64(data, pos, _minor, options) {
  const l = readUint64(data, pos + 1, options);
  if (typeof l === "bigint") {
    throw new Error(
      `${decodeErrPrefix} 64-bit integer bytes lengths not supported`
    );
  }
  return toToken$3(data, pos, 9, l);
}

/**
 * `encodedBytes` allows for caching when we do a byte version of a string
 * for key sorting purposes
 * @param {Token} token
 * @returns {Uint8Array}
 */
function tokenBytes(token) {
  if (token.encodedBytes === undefined) {
    token.encodedBytes =
      token.type === Type.string ? fromString$1(token.value) : token.value;
  }
  // @ts-ignore c'mon
  return token.encodedBytes;
}

/**
 * @param {Bl} buf
 * @param {Token} token
 */
function encodeBytes(buf, token) {
  const bytes = tokenBytes(token);
  encodeUintValue(buf, token.type.majorEncoded, bytes.length);
  buf.push(bytes);
}

/**
 * @param {Token} token
 * @returns {number}
 */
encodeBytes.encodedSize = function encodedSize(token) {
  const bytes = tokenBytes(token);
  return encodeUintValue.encodedSize(bytes.length) + bytes.length;
};

/**
 * @param {Token} tok1
 * @param {Token} tok2
 * @returns {number}
 */
encodeBytes.compareTokens = function compareTokens(tok1, tok2) {
  return compareBytes(tokenBytes(tok1), tokenBytes(tok2));
};

/**
 * @param {Uint8Array} b1
 * @param {Uint8Array} b2
 * @returns {number}
 */
function compareBytes(b1, b2) {
  return b1.length < b2.length
    ? -1
    : b1.length > b2.length
    ? 1
    : compare$1(b1, b2);
}

/**
 * @typedef {import('./bl.js').Bl} Bl
 * @typedef {import('../interface').DecodeOptions} DecodeOptions
 */

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} prefix
 * @param {number} length
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function toToken$2(data, pos, prefix, length, options) {
  const totLength = prefix + length;
  assertEnoughData(data, pos, totLength);
  const tok = new Token(
    Type.string,
    toString$2(data, pos + prefix, pos + totLength),
    totLength
  );
  if (options.retainStringBytes === true) {
    tok.byteValue = slice$1(data, pos + prefix, pos + totLength);
  }
  return tok;
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeStringCompact(data, pos, minor, options) {
  return toToken$2(data, pos, 1, minor, options);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeString8(data, pos, _minor, options) {
  return toToken$2(data, pos, 2, readUint8(data, pos + 1, options), options);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeString16(data, pos, _minor, options) {
  return toToken$2(data, pos, 3, readUint16(data, pos + 1, options), options);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeString32(data, pos, _minor, options) {
  return toToken$2(data, pos, 5, readUint32(data, pos + 1, options), options);
}

// TODO: maybe we shouldn't support this ..
/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeString64(data, pos, _minor, options) {
  const l = readUint64(data, pos + 1, options);
  if (typeof l === "bigint") {
    throw new Error(
      `${decodeErrPrefix} 64-bit integer string lengths not supported`
    );
  }
  return toToken$2(data, pos, 9, l, options);
}

const encodeString = encodeBytes;

/**
 * @typedef {import('./bl.js').Bl} Bl
 * @typedef {import('../interface').DecodeOptions} DecodeOptions
 */

/**
 * @param {Uint8Array} _data
 * @param {number} _pos
 * @param {number} prefix
 * @param {number} length
 * @returns {Token}
 */
function toToken$1(_data, _pos, prefix, length) {
  return new Token(Type.array, length, prefix);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} minor
 * @param {DecodeOptions} _options
 * @returns {Token}
 */
function decodeArrayCompact(data, pos, minor, _options) {
  return toToken$1(data, pos, 1, minor);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeArray8(data, pos, _minor, options) {
  return toToken$1(data, pos, 2, readUint8(data, pos + 1, options));
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeArray16(data, pos, _minor, options) {
  return toToken$1(data, pos, 3, readUint16(data, pos + 1, options));
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeArray32(data, pos, _minor, options) {
  return toToken$1(data, pos, 5, readUint32(data, pos + 1, options));
}

// TODO: maybe we shouldn't support this ..
/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeArray64(data, pos, _minor, options) {
  const l = readUint64(data, pos + 1, options);
  if (typeof l === "bigint") {
    throw new Error(
      `${decodeErrPrefix} 64-bit integer array lengths not supported`
    );
  }
  return toToken$1(data, pos, 9, l);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeArrayIndefinite(data, pos, _minor, options) {
  if (options.allowIndefinite === false) {
    throw new Error(`${decodeErrPrefix} indefinite length items not allowed`);
  }
  return toToken$1(data, pos, 1, Infinity);
}

/**
 * @param {Bl} buf
 * @param {Token} token
 */
function encodeArray(buf, token) {
  encodeUintValue(buf, Type.array.majorEncoded, token.value);
}

// using an array as a map key, are you sure about this? we can only sort
// by map length here, it's up to the encoder to decide to look deeper
encodeArray.compareTokens = encodeUint.compareTokens;

/**
 * @param {Token} token
 * @returns {number}
 */
encodeArray.encodedSize = function encodedSize(token) {
  return encodeUintValue.encodedSize(token.value);
};

/**
 * @typedef {import('./bl.js').Bl} Bl
 * @typedef {import('../interface').DecodeOptions} DecodeOptions
 */

/**
 * @param {Uint8Array} _data
 * @param {number} _pos
 * @param {number} prefix
 * @param {number} length
 * @returns {Token}
 */
function toToken(_data, _pos, prefix, length) {
  return new Token(Type.map, length, prefix);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} minor
 * @param {DecodeOptions} _options
 * @returns {Token}
 */
function decodeMapCompact(data, pos, minor, _options) {
  return toToken(data, pos, 1, minor);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeMap8(data, pos, _minor, options) {
  return toToken(data, pos, 2, readUint8(data, pos + 1, options));
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeMap16(data, pos, _minor, options) {
  return toToken(data, pos, 3, readUint16(data, pos + 1, options));
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeMap32(data, pos, _minor, options) {
  return toToken(data, pos, 5, readUint32(data, pos + 1, options));
}

// TODO: maybe we shouldn't support this ..
/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeMap64(data, pos, _minor, options) {
  const l = readUint64(data, pos + 1, options);
  if (typeof l === "bigint") {
    throw new Error(
      `${decodeErrPrefix} 64-bit integer map lengths not supported`
    );
  }
  return toToken(data, pos, 9, l);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeMapIndefinite(data, pos, _minor, options) {
  if (options.allowIndefinite === false) {
    throw new Error(`${decodeErrPrefix} indefinite length items not allowed`);
  }
  return toToken(data, pos, 1, Infinity);
}

/**
 * @param {Bl} buf
 * @param {Token} token
 */
function encodeMap(buf, token) {
  encodeUintValue(buf, Type.map.majorEncoded, token.value);
}

// using a map as a map key, are you sure about this? we can only sort
// by map length here, it's up to the encoder to decide to look deeper
encodeMap.compareTokens = encodeUint.compareTokens;

/**
 * @param {Token} token
 * @returns {number}
 */
encodeMap.encodedSize = function encodedSize(token) {
  return encodeUintValue.encodedSize(token.value);
};

/**
 * @typedef {import('./bl.js').Bl} Bl
 * @typedef {import('../interface').DecodeOptions} DecodeOptions
 */

/**
 * @param {Uint8Array} _data
 * @param {number} _pos
 * @param {number} minor
 * @param {DecodeOptions} _options
 * @returns {Token}
 */
function decodeTagCompact(_data, _pos, minor, _options) {
  return new Token(Type.tag, minor, 1);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeTag8(data, pos, _minor, options) {
  return new Token(Type.tag, readUint8(data, pos + 1, options), 2);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeTag16(data, pos, _minor, options) {
  return new Token(Type.tag, readUint16(data, pos + 1, options), 3);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeTag32(data, pos, _minor, options) {
  return new Token(Type.tag, readUint32(data, pos + 1, options), 5);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeTag64(data, pos, _minor, options) {
  return new Token(Type.tag, readUint64(data, pos + 1, options), 9);
}

/**
 * @param {Bl} buf
 * @param {Token} token
 */
function encodeTag(buf, token) {
  encodeUintValue(buf, Type.tag.majorEncoded, token.value);
}

encodeTag.compareTokens = encodeUint.compareTokens;

/**
 * @param {Token} token
 * @returns {number}
 */
encodeTag.encodedSize = function encodedSize(token) {
  return encodeUintValue.encodedSize(token.value);
};

// TODO: shift some of the bytes logic to bytes-utils so we can use Buffer
// where possible

/**
 * @typedef {import('./bl.js').Bl} Bl
 * @typedef {import('../interface').DecodeOptions} DecodeOptions
 * @typedef {import('../interface').EncodeOptions} EncodeOptions
 */

const MINOR_FALSE = 20;
const MINOR_TRUE = 21;
const MINOR_NULL = 22;
const MINOR_UNDEFINED = 23;

/**
 * @param {Uint8Array} _data
 * @param {number} _pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeUndefined(_data, _pos, _minor, options) {
  if (options.allowUndefined === false) {
    throw new Error(`${decodeErrPrefix} undefined values are not supported`);
  } else if (options.coerceUndefinedToNull === true) {
    return new Token(Type.null, null, 1);
  }
  return new Token(Type.undefined, undefined, 1);
}

/**
 * @param {Uint8Array} _data
 * @param {number} _pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeBreak(_data, _pos, _minor, options) {
  if (options.allowIndefinite === false) {
    throw new Error(`${decodeErrPrefix} indefinite length items not allowed`);
  }
  return new Token(Type.break, undefined, 1);
}

/**
 * @param {number} value
 * @param {number} bytes
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function createToken(value, bytes, options) {
  if (options) {
    if (options.allowNaN === false && Number.isNaN(value)) {
      throw new Error(`${decodeErrPrefix} NaN values are not supported`);
    }
    if (
      options.allowInfinity === false &&
      (value === Infinity || value === -Infinity)
    ) {
      throw new Error(`${decodeErrPrefix} Infinity values are not supported`);
    }
  }
  return new Token(Type.float, value, bytes);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeFloat16(data, pos, _minor, options) {
  return createToken(readFloat16(data, pos + 1), 3, options);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeFloat32(data, pos, _minor, options) {
  return createToken(readFloat32(data, pos + 1), 5, options);
}

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} _minor
 * @param {DecodeOptions} options
 * @returns {Token}
 */
function decodeFloat64(data, pos, _minor, options) {
  return createToken(readFloat64(data, pos + 1), 9, options);
}

/**
 * @param {Bl} buf
 * @param {Token} token
 * @param {EncodeOptions} options
 */
function encodeFloat(buf, token, options) {
  const float = token.value;

  if (float === false) {
    buf.push([Type.float.majorEncoded | MINOR_FALSE]);
  } else if (float === true) {
    buf.push([Type.float.majorEncoded | MINOR_TRUE]);
  } else if (float === null) {
    buf.push([Type.float.majorEncoded | MINOR_NULL]);
  } else if (float === undefined) {
    buf.push([Type.float.majorEncoded | MINOR_UNDEFINED]);
  } else {
    let decoded;
    let success = false;
    if (!options || options.float64 !== true) {
      encodeFloat16(float);
      decoded = readFloat16(ui8a, 1);
      if (float === decoded || Number.isNaN(float)) {
        ui8a[0] = 0xf9;
        buf.push(ui8a.slice(0, 3));
        success = true;
      } else {
        encodeFloat32(float);
        decoded = readFloat32(ui8a, 1);
        if (float === decoded) {
          ui8a[0] = 0xfa;
          buf.push(ui8a.slice(0, 5));
          success = true;
        }
      }
    }
    if (!success) {
      encodeFloat64(float);
      decoded = readFloat64(ui8a, 1);
      ui8a[0] = 0xfb;
      buf.push(ui8a.slice(0, 9));
    }
  }
}

/**
 * @param {Token} token
 * @param {EncodeOptions} options
 * @returns {number}
 */
encodeFloat.encodedSize = function encodedSize(token, options) {
  const float = token.value;

  if (
    float === false ||
    float === true ||
    float === null ||
    float === undefined
  ) {
    return 1;
  }

  if (!options || options.float64 !== true) {
    encodeFloat16(float);
    let decoded = readFloat16(ui8a, 1);
    if (float === decoded || Number.isNaN(float)) {
      return 3;
    }
    encodeFloat32(float);
    decoded = readFloat32(ui8a, 1);
    if (float === decoded) {
      return 5;
    }
  }
  return 9;
};

const buffer = new ArrayBuffer(9);
const dataView = new DataView(buffer, 1);
const ui8a = new Uint8Array(buffer, 0);

/**
 * @param {number} inp
 */
function encodeFloat16(inp) {
  if (inp === Infinity) {
    dataView.setUint16(0, 0x7c00, false);
  } else if (inp === -Infinity) {
    dataView.setUint16(0, 0xfc00, false);
  } else if (Number.isNaN(inp)) {
    dataView.setUint16(0, 0x7e00, false);
  } else {
    dataView.setFloat32(0, inp);
    const valu32 = dataView.getUint32(0);
    const exponent = (valu32 & 0x7f800000) >> 23;
    const mantissa = valu32 & 0x7fffff;

    /* c8 ignore next 6 */
    if (exponent === 0xff) {
      // too big, Infinity, but this should be hard (impossible?) to trigger
      dataView.setUint16(0, 0x7c00, false);
    } else if (exponent === 0x00) {
      // 0.0, -0.0 and subnormals, shouldn't be possible to get here because 0.0 should be counted as an int
      dataView.setUint16(
        0,
        ((inp & 0x80000000) >> 16) | (mantissa >> 13),
        false
      );
    } else {
      // standard numbers
      // chunks of logic here borrowed from https://github.com/PJK/libcbor/blob/c78f437182533e3efa8d963ff4b945bb635c2284/src/cbor/encoding.c#L127
      const logicalExponent = exponent - 127;
      // Now we know that 2^exponent <= 0 logically
      /* c8 ignore next 6 */
      if (logicalExponent < -24) {
        /* No unambiguous representation exists, this float is not a half float
          and is too small to be represented using a half, round off to zero.
          Consistent with the reference implementation. */
        // should be difficult (impossible?) to get here in JS
        dataView.setUint16(0, 0);
      } else if (logicalExponent < -14) {
        /* Offset the remaining decimal places by shifting the significand, the
          value is lost. This is an implementation decision that works around the
          absence of standard half-float in the language. */
        dataView.setUint16(
          0,
          ((valu32 & 0x80000000) >> 16) |
            /* sign bit */ (1 << (24 + logicalExponent)),
          false
        );
      } else {
        dataView.setUint16(
          0,
          ((valu32 & 0x80000000) >> 16) |
            ((logicalExponent + 15) << 10) |
            (mantissa >> 13),
          false
        );
      }
    }
  }
}

/**
 * @param {Uint8Array} ui8a
 * @param {number} pos
 * @returns {number}
 */
function readFloat16(ui8a, pos) {
  if (ui8a.length - pos < 2) {
    throw new Error(`${decodeErrPrefix} not enough data for float16`);
  }

  const half = (ui8a[pos] << 8) + ui8a[pos + 1];
  if (half === 0x7c00) {
    return Infinity;
  }
  if (half === 0xfc00) {
    return -Infinity;
  }
  if (half === 0x7e00) {
    return NaN;
  }
  const exp = (half >> 10) & 0x1f;
  const mant = half & 0x3ff;
  let val;
  if (exp === 0) {
    val = mant * 2 ** -24;
  } else if (exp !== 31) {
    val = (mant + 1024) * 2 ** (exp - 25);
    /* c8 ignore next 4 */
  } else {
    // may not be possible to get here
    val = mant === 0 ? Infinity : NaN;
  }
  return half & 0x8000 ? -val : val;
}

/**
 * @param {number} inp
 */
function encodeFloat32(inp) {
  dataView.setFloat32(0, inp, false);
}

/**
 * @param {Uint8Array} ui8a
 * @param {number} pos
 * @returns {number}
 */
function readFloat32(ui8a, pos) {
  if (ui8a.length - pos < 4) {
    throw new Error(`${decodeErrPrefix} not enough data for float32`);
  }
  const offset = (ui8a.byteOffset || 0) + pos;
  return new DataView(ui8a.buffer, offset, 4).getFloat32(0, false);
}

/**
 * @param {number} inp
 */
function encodeFloat64(inp) {
  dataView.setFloat64(0, inp, false);
}

/**
 * @param {Uint8Array} ui8a
 * @param {number} pos
 * @returns {number}
 */
function readFloat64(ui8a, pos) {
  if (ui8a.length - pos < 8) {
    throw new Error(`${decodeErrPrefix} not enough data for float64`);
  }
  const offset = (ui8a.byteOffset || 0) + pos;
  return new DataView(ui8a.buffer, offset, 8).getFloat64(0, false);
}

/**
 * @param {Token} _tok1
 * @param {Token} _tok2
 * @returns {number}
 */
encodeFloat.compareTokens = encodeUint.compareTokens;
/*
encodeFloat.compareTokens = function compareTokens (_tok1, _tok2) {
  return _tok1
  throw new Error(`${encodeErrPrefix} cannot use floats as map keys`)
}
*/

/**
 * @typedef {import('../interface').DecodeOptions} DecodeOptions
 */

/**
 * @param {Uint8Array} data
 * @param {number} pos
 * @param {number} minor
 */
function invalidMinor(data, pos, minor) {
  throw new Error(
    `${decodeErrPrefix} encountered invalid minor (${minor}) for major ${
      data[pos] >>> 5
    }`
  );
}

/**
 * @param {string} msg
 * @returns {()=>any}
 */
function errorer(msg) {
  return () => {
    throw new Error(`${decodeErrPrefix} ${msg}`);
  };
}

/** @type {((data:Uint8Array, pos:number, minor:number, options?:DecodeOptions) => any)[]} */
const jump = [];

// unsigned integer, 0x00..0x17 (0..23)
for (let i = 0; i <= 0x17; i++) {
  jump[i] = invalidMinor; // uint.decodeUintCompact, handled by quick[]
}
jump[0x18] = decodeUint8; // unsigned integer, one-byte uint8_t follows
jump[0x19] = decodeUint16; // unsigned integer, two-byte uint16_t follows
jump[0x1a] = decodeUint32; // unsigned integer, four-byte uint32_t follows
jump[0x1b] = decodeUint64; // unsigned integer, eight-byte uint64_t follows
jump[0x1c] = invalidMinor;
jump[0x1d] = invalidMinor;
jump[0x1e] = invalidMinor;
jump[0x1f] = invalidMinor;
// negative integer, -1-0x00..-1-0x17 (-1..-24)
for (let i = 0x20; i <= 0x37; i++) {
  jump[i] = invalidMinor; // negintDecode, handled by quick[]
}
jump[0x38] = decodeNegint8; // negative integer, -1-n one-byte uint8_t for n follows
jump[0x39] = decodeNegint16; // negative integer, -1-n two-byte uint16_t for n follows
jump[0x3a] = decodeNegint32; // negative integer, -1-n four-byte uint32_t for follows
jump[0x3b] = decodeNegint64; // negative integer, -1-n eight-byte uint64_t for follows
jump[0x3c] = invalidMinor;
jump[0x3d] = invalidMinor;
jump[0x3e] = invalidMinor;
jump[0x3f] = invalidMinor;
// byte string, 0x00..0x17 bytes follow
for (let i = 0x40; i <= 0x57; i++) {
  jump[i] = decodeBytesCompact;
}
jump[0x58] = decodeBytes8; // byte string, one-byte uint8_t for n, and then n bytes follow
jump[0x59] = decodeBytes16; // byte string, two-byte uint16_t for n, and then n bytes follow
jump[0x5a] = decodeBytes32; // byte string, four-byte uint32_t for n, and then n bytes follow
jump[0x5b] = decodeBytes64; // byte string, eight-byte uint64_t for n, and then n bytes follow
jump[0x5c] = invalidMinor;
jump[0x5d] = invalidMinor;
jump[0x5e] = invalidMinor;
jump[0x5f] = errorer("indefinite length bytes/strings are not supported"); // byte string, byte strings follow, terminated by "break"
// UTF-8 string 0x00..0x17 bytes follow
for (let i = 0x60; i <= 0x77; i++) {
  jump[i] = decodeStringCompact;
}
jump[0x78] = decodeString8; // UTF-8 string, one-byte uint8_t for n, and then n bytes follow
jump[0x79] = decodeString16; // UTF-8 string, two-byte uint16_t for n, and then n bytes follow
jump[0x7a] = decodeString32; // UTF-8 string, four-byte uint32_t for n, and then n bytes follow
jump[0x7b] = decodeString64; // UTF-8 string, eight-byte uint64_t for n, and then n bytes follow
jump[0x7c] = invalidMinor;
jump[0x7d] = invalidMinor;
jump[0x7e] = invalidMinor;
jump[0x7f] = errorer("indefinite length bytes/strings are not supported"); // UTF-8 strings follow, terminated by "break"
// array, 0x00..0x17 data items follow
for (let i = 0x80; i <= 0x97; i++) {
  jump[i] = decodeArrayCompact;
}
jump[0x98] = decodeArray8; // array, one-byte uint8_t for n, and then n data items follow
jump[0x99] = decodeArray16; // array, two-byte uint16_t for n, and then n data items follow
jump[0x9a] = decodeArray32; // array, four-byte uint32_t for n, and then n data items follow
jump[0x9b] = decodeArray64; // array, eight-byte uint64_t for n, and then n data items follow
jump[0x9c] = invalidMinor;
jump[0x9d] = invalidMinor;
jump[0x9e] = invalidMinor;
jump[0x9f] = decodeArrayIndefinite; // array, data items follow, terminated by "break"
// map, 0x00..0x17 pairs of data items follow
for (let i = 0xa0; i <= 0xb7; i++) {
  jump[i] = decodeMapCompact;
}
jump[0xb8] = decodeMap8; // map, one-byte uint8_t for n, and then n pairs of data items follow
jump[0xb9] = decodeMap16; // map, two-byte uint16_t for n, and then n pairs of data items follow
jump[0xba] = decodeMap32; // map, four-byte uint32_t for n, and then n pairs of data items follow
jump[0xbb] = decodeMap64; // map, eight-byte uint64_t for n, and then n pairs of data items follow
jump[0xbc] = invalidMinor;
jump[0xbd] = invalidMinor;
jump[0xbe] = invalidMinor;
jump[0xbf] = decodeMapIndefinite; // map, pairs of data items follow, terminated by "break"
// tags
for (let i = 0xc0; i <= 0xd7; i++) {
  jump[i] = decodeTagCompact;
}
jump[0xd8] = decodeTag8;
jump[0xd9] = decodeTag16;
jump[0xda] = decodeTag32;
jump[0xdb] = decodeTag64;
jump[0xdc] = invalidMinor;
jump[0xdd] = invalidMinor;
jump[0xde] = invalidMinor;
jump[0xdf] = invalidMinor;
// 0xe0..0xf3 simple values, unsupported
for (let i = 0xe0; i <= 0xf3; i++) {
  jump[i] = errorer("simple values are not supported");
}
jump[0xf4] = invalidMinor; // false, handled by quick[]
jump[0xf5] = invalidMinor; // true, handled by quick[]
jump[0xf6] = invalidMinor; // null, handled by quick[]
jump[0xf7] = decodeUndefined; // undefined
jump[0xf8] = errorer("simple values are not supported"); // simple value, one byte follows, unsupported
jump[0xf9] = decodeFloat16; // half-precision float (two-byte IEEE 754)
jump[0xfa] = decodeFloat32; // single-precision float (four-byte IEEE 754)
jump[0xfb] = decodeFloat64; // double-precision float (eight-byte IEEE 754)
jump[0xfc] = invalidMinor;
jump[0xfd] = invalidMinor;
jump[0xfe] = invalidMinor;
jump[0xff] = decodeBreak; // "break" stop code

/** @type {Token[]} */
const quick = [];
// ints <24
for (let i = 0; i < 24; i++) {
  quick[i] = new Token(Type.uint, i, 1);
}
// negints >= -24
for (let i = -1; i >= -24; i--) {
  quick[31 - i] = new Token(Type.negint, i, 1);
}
// empty bytes
quick[0x40] = new Token(Type.bytes, new Uint8Array(0), 1);
// empty string
quick[0x60] = new Token(Type.string, "", 1);
// empty list
quick[0x80] = new Token(Type.array, 0, 1);
// empty map
quick[0xa0] = new Token(Type.map, 0, 1);
// false
quick[0xf4] = new Token(Type.false, false, 1);
// true
quick[0xf5] = new Token(Type.true, true, 1);
// null
quick[0xf6] = new Token(Type.null, null, 1);

/**
 * @param {Token} token
 * @returns {Uint8Array|undefined}
 */
function quickEncodeToken(token) {
  switch (token.type) {
    case Type.false:
      return fromArray([0xf4]);
    case Type.true:
      return fromArray([0xf5]);
    case Type.null:
      return fromArray([0xf6]);
    case Type.bytes:
      if (!token.value.length) {
        return fromArray([0x40]);
      }
      return;
    case Type.string:
      if (token.value === "") {
        return fromArray([0x60]);
      }
      return;
    case Type.array:
      if (token.value === 0) {
        return fromArray([0x80]);
      }
      /* c8 ignore next 2 */
      // shouldn't be possible if this were called when there was only one token
      return;
    case Type.map:
      if (token.value === 0) {
        return fromArray([0xa0]);
      }
      /* c8 ignore next 2 */
      // shouldn't be possible if this were called when there was only one token
      return;
    case Type.uint:
      if (token.value < 24) {
        return fromArray([Number(token.value)]);
      }
      return;
    case Type.negint:
      if (token.value >= -24) {
        return fromArray([31 - Number(token.value)]);
      }
  }
}

/**
 * @typedef {import('../interface').EncodeOptions} EncodeOptions
 * @typedef {import('../interface').OptionalTypeEncoder} OptionalTypeEncoder
 * @typedef {import('../interface').Reference} Reference
 * @typedef {import('../interface').StrictTypeEncoder} StrictTypeEncoder
 * @typedef {import('../interface').TokenTypeEncoder} TokenTypeEncoder
 * @typedef {import('../interface').TokenOrNestedTokens} TokenOrNestedTokens
 */

/** @type {EncodeOptions} */
const defaultEncodeOptions$2 = {
  float64: false,
  mapSorter: mapSorter$1,
  quickEncodeToken,
};

/** @returns {TokenTypeEncoder[]} */
function makeCborEncoders() {
  const encoders = [];
  encoders[Type.uint.major] = encodeUint;
  encoders[Type.negint.major] = encodeNegint;
  encoders[Type.bytes.major] = encodeBytes;
  encoders[Type.string.major] = encodeString;
  encoders[Type.array.major] = encodeArray;
  encoders[Type.map.major] = encodeMap;
  encoders[Type.tag.major] = encodeTag;
  encoders[Type.float.major] = encodeFloat;
  return encoders;
}

const cborEncoders$1 = makeCborEncoders();

const buf = new Bl();

/** @implements {Reference} */
class Ref {
  /**
   * @param {object|any[]} obj
   * @param {Reference|undefined} parent
   */
  constructor(obj, parent) {
    this.obj = obj;
    this.parent = parent;
  }

  /**
   * @param {object|any[]} obj
   * @returns {boolean}
   */
  includes(obj) {
    /** @type {Reference|undefined} */
    let p = this;
    do {
      if (p.obj === obj) {
        return true;
      }
    } while ((p = p.parent)); // eslint-disable-line
    return false;
  }

  /**
   * @param {Reference|undefined} stack
   * @param {object|any[]} obj
   * @returns {Reference}
   */
  static createCheck(stack, obj) {
    if (stack && stack.includes(obj)) {
      throw new Error(`${encodeErrPrefix} object contains circular references`);
    }
    return new Ref(obj, stack);
  }
}

const simpleTokens = {
  null: new Token(Type.null, null),
  undefined: new Token(Type.undefined, undefined),
  true: new Token(Type.true, true),
  false: new Token(Type.false, false),
  emptyArray: new Token(Type.array, 0),
  emptyMap: new Token(Type.map, 0),
};

/** @type {{[typeName: string]: StrictTypeEncoder}} */
const typeEncoders = {
  /**
   * @param {any} obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  number(obj, _typ, _options, _refStack) {
    if (!Number.isInteger(obj) || !Number.isSafeInteger(obj)) {
      return new Token(Type.float, obj);
    } else if (obj >= 0) {
      return new Token(Type.uint, obj);
    } else {
      return new Token(Type.negint, obj);
    }
  },

  /**
   * @param {any} obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  bigint(obj, _typ, _options, _refStack) {
    if (obj >= BigInt(0)) {
      return new Token(Type.uint, obj);
    } else {
      return new Token(Type.negint, obj);
    }
  },

  /**
   * @param {any} obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  Uint8Array(obj, _typ, _options, _refStack) {
    return new Token(Type.bytes, obj);
  },

  /**
   * @param {any} obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  string(obj, _typ, _options, _refStack) {
    return new Token(Type.string, obj);
  },

  /**
   * @param {any} obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  boolean(obj, _typ, _options, _refStack) {
    return obj ? simpleTokens.true : simpleTokens.false;
  },

  /**
   * @param {any} _obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  null(_obj, _typ, _options, _refStack) {
    return simpleTokens.null;
  },

  /**
   * @param {any} _obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  undefined(_obj, _typ, _options, _refStack) {
    return simpleTokens.undefined;
  },

  /**
   * @param {any} obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  ArrayBuffer(obj, _typ, _options, _refStack) {
    return new Token(Type.bytes, new Uint8Array(obj));
  },

  /**
   * @param {any} obj
   * @param {string} _typ
   * @param {EncodeOptions} _options
   * @param {Reference} [_refStack]
   * @returns {TokenOrNestedTokens}
   */
  DataView(obj, _typ, _options, _refStack) {
    return new Token(
      Type.bytes,
      new Uint8Array(obj.buffer, obj.byteOffset, obj.byteLength)
    );
  },

  /**
   * @param {any} obj
   * @param {string} _typ
   * @param {EncodeOptions} options
   * @param {Reference} [refStack]
   * @returns {TokenOrNestedTokens}
   */
  Array(obj, _typ, options, refStack) {
    if (!obj.length) {
      if (options.addBreakTokens === true) {
        return [simpleTokens.emptyArray, new Token(Type.break)];
      }
      return simpleTokens.emptyArray;
    }
    refStack = Ref.createCheck(refStack, obj);
    const entries = [];
    let i = 0;
    for (const e of obj) {
      entries[i++] = objectToTokens(e, options, refStack);
    }
    if (options.addBreakTokens) {
      return [
        new Token(Type.array, obj.length),
        entries,
        new Token(Type.break),
      ];
    }
    return [new Token(Type.array, obj.length), entries];
  },

  /**
   * @param {any} obj
   * @param {string} typ
   * @param {EncodeOptions} options
   * @param {Reference} [refStack]
   * @returns {TokenOrNestedTokens}
   */
  Object(obj, typ, options, refStack) {
    // could be an Object or a Map
    const isMap = typ !== "Object";
    // it's slightly quicker to use Object.keys() than Object.entries()
    const keys = isMap ? obj.keys() : Object.keys(obj);
    const length = isMap ? obj.size : keys.length;
    if (!length) {
      if (options.addBreakTokens === true) {
        return [simpleTokens.emptyMap, new Token(Type.break)];
      }
      return simpleTokens.emptyMap;
    }
    refStack = Ref.createCheck(refStack, obj);
    /** @type {TokenOrNestedTokens[]} */
    const entries = [];
    let i = 0;
    for (const key of keys) {
      entries[i++] = [
        objectToTokens(key, options, refStack),
        objectToTokens(isMap ? obj.get(key) : obj[key], options, refStack),
      ];
    }
    sortMapEntries(entries, options);
    if (options.addBreakTokens) {
      return [new Token(Type.map, length), entries, new Token(Type.break)];
    }
    return [new Token(Type.map, length), entries];
  },
};

typeEncoders.Map = typeEncoders.Object;
typeEncoders.Buffer = typeEncoders.Uint8Array;
for (const typ of "Uint8Clamped Uint16 Uint32 Int8 Int16 Int32 BigUint64 BigInt64 Float32 Float64".split(
  " "
)) {
  typeEncoders[`${typ}Array`] = typeEncoders.DataView;
}

/**
 * @param {any} obj
 * @param {EncodeOptions} [options]
 * @param {Reference} [refStack]
 * @returns {TokenOrNestedTokens}
 */
function objectToTokens(obj, options = {}, refStack) {
  const typ = is(obj);
  const customTypeEncoder =
    (options &&
      options.typeEncoders &&
      /** @type {OptionalTypeEncoder} */ options.typeEncoders[typ]) ||
    typeEncoders[typ];
  if (typeof customTypeEncoder === "function") {
    const tokens = customTypeEncoder(obj, typ, options, refStack);
    if (tokens != null) {
      return tokens;
    }
  }
  const typeEncoder = typeEncoders[typ];
  if (!typeEncoder) {
    throw new Error(`${encodeErrPrefix} unsupported type: ${typ}`);
  }
  return typeEncoder(obj, typ, options, refStack);
}

/*
CBOR key sorting is a mess.

The canonicalisation recommendation from https://tools.ietf.org/html/rfc7049#section-3.9
includes the wording:

> The keys in every map must be sorted lowest value to highest.
> Sorting is performed on the bytes of the representation of the key
> data items without paying attention to the 3/5 bit splitting for
> major types.
> ...
>  *  If two keys have different lengths, the shorter one sorts
      earlier;
>  *  If two keys have the same length, the one with the lower value
      in (byte-wise) lexical order sorts earlier.

1. It is not clear what "bytes of the representation of the key" means: is it
   the CBOR representation, or the binary representation of the object itself?
   Consider the int and uint difference here.
2. It is not clear what "without paying attention to" means: do we include it
   and compare on that? Or do we omit the special prefix byte, (mostly) treating
   the key in its plain binary representation form.

The FIDO 2.0: Client To Authenticator Protocol spec takes the original CBOR
wording and clarifies it according to their understanding.
https://fidoalliance.org/specs/fido-v2.0-rd-20170927/fido-client-to-authenticator-protocol-v2.0-rd-20170927.html#message-encoding

> The keys in every map must be sorted lowest value to highest. Sorting is
> performed on the bytes of the representation of the key data items without
> paying attention to the 3/5 bit splitting for major types. The sorting rules
> are:
>  * If the major types are different, the one with the lower value in numerical
>    order sorts earlier.
>  * If two keys have different lengths, the shorter one sorts earlier;
>  * If two keys have the same length, the one with the lower value in
>    (byte-wise) lexical order sorts earlier.

Some other implementations, such as borc, do a full encode then do a
length-first, byte-wise-second comparison:
https://github.com/dignifiedquire/borc/blob/b6bae8b0bcde7c3976b0f0f0957208095c392a36/src/encoder.js#L358
https://github.com/dignifiedquire/borc/blob/b6bae8b0bcde7c3976b0f0f0957208095c392a36/src/utils.js#L143-L151

This has the benefit of being able to easily handle arbitrary keys, including
complex types (maps and arrays).

We'll opt for the FIDO approach, since it affords some efficies since we don't
need a full encode of each key to determine order and can defer to the types
to determine how to most efficiently order their values (i.e. int and uint
ordering can be done on the numbers, no need for byte-wise, for example).

Recommendation: stick to single key types or you'll get into trouble, and prefer
string keys because it's much simpler that way.
*/

/*
(UPDATE, Dec 2020)
https://tools.ietf.org/html/rfc8949 is the updated CBOR spec and clarifies some
of the questions above with a new recommendation for sorting order being much
closer to what would be expected in other environments (i.e. no length-first
weirdness).
This new sorting order is not yet implemented here but could be added as an
option. "Determinism" (canonicity) is system dependent and it's difficult to
change existing systems that are built with existing expectations. So if a new
ordering is introduced here, the old needs to be kept as well with the user
having the option.
*/

/**
 * @param {TokenOrNestedTokens[]} entries
 * @param {EncodeOptions} options
 */
function sortMapEntries(entries, options) {
  if (options.mapSorter) {
    entries.sort(options.mapSorter);
  }
}

/**
 * @param {(Token|Token[])[]} e1
 * @param {(Token|Token[])[]} e2
 * @returns {number}
 */
function mapSorter$1(e1, e2) {
  // the key position ([0]) could have a single token or an array
  // almost always it'll be a single token but complex key might get involved
  /* c8 ignore next 2 */
  const keyToken1 = Array.isArray(e1[0]) ? e1[0][0] : e1[0];
  const keyToken2 = Array.isArray(e2[0]) ? e2[0][0] : e2[0];

  // different key types
  if (keyToken1.type !== keyToken2.type) {
    return keyToken1.type.compare(keyToken2.type);
  }

  const major = keyToken1.type.major;
  // TODO: handle case where cmp === 0 but there are more keyToken e. complex type)
  const tcmp = cborEncoders$1[major].compareTokens(keyToken1, keyToken2);
  /* c8 ignore next 5 */
  if (tcmp === 0) {
    // duplicate key or complex type where the first token matched,
    // i.e. a map or array and we're only comparing the opening token
    console.warn(
      "WARNING: complex key types used, CBOR key sorting guarantees are gone"
    );
  }
  return tcmp;
}

/**
 * @param {Bl} buf
 * @param {TokenOrNestedTokens} tokens
 * @param {TokenTypeEncoder[]} encoders
 * @param {EncodeOptions} options
 */
function tokensToEncoded(buf, tokens, encoders, options) {
  if (Array.isArray(tokens)) {
    for (const token of tokens) {
      tokensToEncoded(buf, token, encoders, options);
    }
  } else {
    encoders[tokens.type.major](buf, tokens, options);
  }
}

/**
 * @param {any} data
 * @param {TokenTypeEncoder[]} encoders
 * @param {EncodeOptions} options
 * @returns {Uint8Array}
 */
function encodeCustom(data, encoders, options) {
  const tokens = objectToTokens(data, options);
  if (!Array.isArray(tokens) && options.quickEncodeToken) {
    const quickBytes = options.quickEncodeToken(tokens);
    if (quickBytes) {
      return quickBytes;
    }
    const encoder = encoders[tokens.type.major];
    if (encoder.encodedSize) {
      const size = encoder.encodedSize(tokens, options);
      const buf = new Bl(size);
      encoder(buf, tokens, options);
      /* c8 ignore next 4 */
      // this would be a problem with encodedSize() functions
      if (buf.chunks.length !== 1) {
        throw new Error(
          `Unexpected error: pre-calculated length for ${tokens} was wrong`
        );
      }
      return asU8A(buf.chunks[0]);
    }
  }
  buf.reset();
  tokensToEncoded(buf, tokens, encoders, options);
  return buf.toBytes(true);
}

/**
 * @param {any} data
 * @param {EncodeOptions} [options]
 * @returns {Uint8Array}
 */
function encode$t(data, options) {
  options = Object.assign({}, defaultEncodeOptions$2, options);
  return encodeCustom(data, cborEncoders$1, options);
}

/**
 * @typedef {import('./token.js').Token} Token
 * @typedef {import('../interface').DecodeOptions} DecodeOptions
 * @typedef {import('../interface').DecodeTokenizer} DecodeTokenizer
 */

const defaultDecodeOptions = {
  strict: false,
  allowIndefinite: true,
  allowUndefined: true,
  allowBigInt: true,
};

/**
 * @implements {DecodeTokenizer}
 */
class Tokeniser {
  /**
   * @param {Uint8Array} data
   * @param {DecodeOptions} options
   */
  constructor(data, options = {}) {
    this._pos = 0;
    this.data = data;
    this.options = options;
  }

  pos() {
    return this._pos;
  }

  done() {
    return this._pos >= this.data.length;
  }

  next() {
    const byt = this.data[this._pos];
    let token = quick[byt];
    if (token === undefined) {
      const decoder = jump[byt];
      /* c8 ignore next 4 */
      // if we're here then there's something wrong with our jump or quick lists!
      if (!decoder) {
        throw new Error(
          `${decodeErrPrefix} no decoder for major type ${
            byt >>> 5
          } (byte 0x${byt.toString(16).padStart(2, "0")})`
        );
      }
      const minor = byt & 31;
      token = decoder(this.data, this._pos, minor, this.options);
    }
    // @ts-ignore we get to assume encodedLength is set (crossing fingers slightly)
    this._pos += token.encodedLength;
    return token;
  }
}

const DONE = Symbol.for("DONE");
const BREAK = Symbol.for("BREAK");

/**
 * @param {Token} token
 * @param {DecodeTokenizer} tokeniser
 * @param {DecodeOptions} options
 * @returns {any|BREAK|DONE}
 */
function tokenToArray(token, tokeniser, options) {
  const arr = [];
  for (let i = 0; i < token.value; i++) {
    const value = tokensToObject(tokeniser, options);
    if (value === BREAK) {
      if (token.value === Infinity) {
        // normal end to indefinite length array
        break;
      }
      throw new Error(
        `${decodeErrPrefix} got unexpected break to lengthed array`
      );
    }
    if (value === DONE) {
      throw new Error(
        `${decodeErrPrefix} found array but not enough entries (got ${i}, expected ${token.value})`
      );
    }
    arr[i] = value;
  }
  return arr;
}

/**
 * @param {Token} token
 * @param {DecodeTokenizer} tokeniser
 * @param {DecodeOptions} options
 * @returns {any|BREAK|DONE}
 */
function tokenToMap(token, tokeniser, options) {
  const useMaps = options.useMaps === true;
  const obj = useMaps ? undefined : {};
  const m = useMaps ? new Map() : undefined;
  for (let i = 0; i < token.value; i++) {
    const key = tokensToObject(tokeniser, options);
    if (key === BREAK) {
      if (token.value === Infinity) {
        // normal end to indefinite length map
        break;
      }
      throw new Error(
        `${decodeErrPrefix} got unexpected break to lengthed map`
      );
    }
    if (key === DONE) {
      throw new Error(
        `${decodeErrPrefix} found map but not enough entries (got ${i} [no key], expected ${token.value})`
      );
    }
    if (useMaps !== true && typeof key !== "string") {
      throw new Error(
        `${decodeErrPrefix} non-string keys not supported (got ${typeof key})`
      );
    }
    if (options.rejectDuplicateMapKeys === true) {
      // @ts-ignore
      if ((useMaps && m.has(key)) || (!useMaps && key in obj)) {
        throw new Error(`${decodeErrPrefix} found repeat map key "${key}"`);
      }
    }
    const value = tokensToObject(tokeniser, options);
    if (value === DONE) {
      throw new Error(
        `${decodeErrPrefix} found map but not enough entries (got ${i} [no value], expected ${token.value})`
      );
    }
    if (useMaps) {
      // @ts-ignore TODO reconsider this .. maybe needs to be strict about key types
      m.set(key, value);
    } else {
      // @ts-ignore TODO reconsider this .. maybe needs to be strict about key types
      obj[key] = value;
    }
  }
  // @ts-ignore c'mon man
  return useMaps ? m : obj;
}

/**
 * @param {DecodeTokenizer} tokeniser
 * @param {DecodeOptions} options
 * @returns {any|BREAK|DONE}
 */
function tokensToObject(tokeniser, options) {
  // should we support array as an argument?
  // check for tokenIter[Symbol.iterator] and replace tokenIter with what that returns?
  if (tokeniser.done()) {
    return DONE;
  }

  const token = tokeniser.next();

  if (token.type === Type.break) {
    return BREAK;
  }

  if (token.type.terminal) {
    return token.value;
  }

  if (token.type === Type.array) {
    return tokenToArray(token, tokeniser, options);
  }

  if (token.type === Type.map) {
    return tokenToMap(token, tokeniser, options);
  }

  if (token.type === Type.tag) {
    if (options.tags && typeof options.tags[token.value] === "function") {
      const tagged = tokensToObject(tokeniser, options);
      return options.tags[token.value](tagged);
    }
    throw new Error(`${decodeErrPrefix} tag not supported (${token.value})`);
  }
  /* c8 ignore next */
  throw new Error("unsupported");
}

/**
 * @param {Uint8Array} data
 * @param {DecodeOptions} [options]
 * @returns {[any, Uint8Array]}
 */
function decodeFirst(data, options) {
  if (!(data instanceof Uint8Array)) {
    throw new Error(`${decodeErrPrefix} data to decode must be a Uint8Array`);
  }
  options = Object.assign({}, defaultDecodeOptions, options);
  const tokeniser = options.tokenizer || new Tokeniser(data, options);
  const decoded = tokensToObject(tokeniser, options);
  if (decoded === DONE) {
    throw new Error(`${decodeErrPrefix} did not find any content to decode`);
  }
  if (decoded === BREAK) {
    throw new Error(`${decodeErrPrefix} got unexpected break`);
  }
  return [decoded, data.subarray(tokeniser.pos())];
}

/**
 * @param {Uint8Array} data
 * @param {DecodeOptions} [options]
 * @returns {any}
 */
function decode$D(data, options) {
  const [decoded, remainder] = decodeFirst(data, options);
  if (remainder.length > 0) {
    throw new Error(
      `${decodeErrPrefix} too many terminals, data makes no sense`
    );
  }
  return decoded;
}

const empty$5 = new Uint8Array(0);
function fromHex(hex) {
  const hexes = hex.match(/../g);
  return hexes != null
    ? new Uint8Array(hexes.map((b) => parseInt(b, 16)))
    : empty$5;
}
function equals$6(aa, bb) {
  if (aa === bb) return true;
  if (aa.byteLength !== bb.byteLength) {
    return false;
  }
  for (let ii = 0; ii < aa.byteLength; ii++) {
    if (aa[ii] !== bb[ii]) {
      return false;
    }
  }
  return true;
}
function coerce$2(o) {
  if (o instanceof Uint8Array && o.constructor.name === "Uint8Array") return o;
  if (o instanceof ArrayBuffer) return new Uint8Array(o);
  if (ArrayBuffer.isView(o)) {
    return new Uint8Array(o.buffer, o.byteOffset, o.byteLength);
  }
  throw new Error("Unknown type, must be binary type");
}
function fromString(str) {
  return new TextEncoder().encode(str);
}
function toString$1(b) {
  return new TextDecoder().decode(b);
}

/* eslint-disable */
// base-x encoding / decoding
// Copyright (c) 2018 base-x contributors
// Copyright (c) 2014-2018 The Bitcoin Core developers (base58.cpp)
// Distributed under the MIT software license, see the accompanying
// file LICENSE or http://www.opensource.org/licenses/mit-license.php.
/**
 * @param {string} ALPHABET
 * @param {any} name
 */
function base$2(ALPHABET, name) {
  if (ALPHABET.length >= 255) {
    throw new TypeError("Alphabet too long");
  }
  var BASE_MAP = new Uint8Array(256);
  for (var j = 0; j < BASE_MAP.length; j++) {
    BASE_MAP[j] = 255;
  }
  for (var i = 0; i < ALPHABET.length; i++) {
    var x = ALPHABET.charAt(i);
    var xc = x.charCodeAt(0);
    if (BASE_MAP[xc] !== 255) {
      throw new TypeError(x + " is ambiguous");
    }
    BASE_MAP[xc] = i;
  }
  var BASE = ALPHABET.length;
  var LEADER = ALPHABET.charAt(0);
  var FACTOR = Math.log(BASE) / Math.log(256); // log(BASE) / log(256), rounded up
  var iFACTOR = Math.log(256) / Math.log(BASE); // log(256) / log(BASE), rounded up
  /**
   * @param {any[] | Iterable<number>} source
   */
  function encode(source) {
    // @ts-ignore
    if (source instanceof Uint8Array);
    else if (ArrayBuffer.isView(source)) {
      source = new Uint8Array(
        source.buffer,
        source.byteOffset,
        source.byteLength
      );
    } else if (Array.isArray(source)) {
      source = Uint8Array.from(source);
    }
    if (!(source instanceof Uint8Array)) {
      throw new TypeError("Expected Uint8Array");
    }
    if (source.length === 0) {
      return "";
    }
    // Skip & count leading zeroes.
    var zeroes = 0;
    var length = 0;
    var pbegin = 0;
    var pend = source.length;
    while (pbegin !== pend && source[pbegin] === 0) {
      pbegin++;
      zeroes++;
    }
    // Allocate enough space in big-endian base58 representation.
    var size = ((pend - pbegin) * iFACTOR + 1) >>> 0;
    var b58 = new Uint8Array(size);
    // Process the bytes.
    while (pbegin !== pend) {
      var carry = source[pbegin];
      // Apply "b58 = b58 * 256 + ch".
      var i = 0;
      for (
        var it1 = size - 1;
        (carry !== 0 || i < length) && it1 !== -1;
        it1--, i++
      ) {
        carry += (256 * b58[it1]) >>> 0;
        b58[it1] = carry % BASE >>> 0;
        carry = (carry / BASE) >>> 0;
      }
      if (carry !== 0) {
        throw new Error("Non-zero carry");
      }
      length = i;
      pbegin++;
    }
    // Skip leading zeroes in base58 result.
    var it2 = size - length;
    while (it2 !== size && b58[it2] === 0) {
      it2++;
    }
    // Translate the result into a string.
    var str = LEADER.repeat(zeroes);
    for (; it2 < size; ++it2) {
      str += ALPHABET.charAt(b58[it2]);
    }
    return str;
  }
  /**
   * @param {string | string[]} source
   */
  function decodeUnsafe(source) {
    if (typeof source !== "string") {
      throw new TypeError("Expected String");
    }
    if (source.length === 0) {
      return new Uint8Array();
    }
    var psz = 0;
    // Skip leading spaces.
    if (source[psz] === " ") {
      return;
    }
    // Skip and count leading '1's.
    var zeroes = 0;
    var length = 0;
    while (source[psz] === LEADER) {
      zeroes++;
      psz++;
    }
    // Allocate enough space in big-endian base256 representation.
    var size = ((source.length - psz) * FACTOR + 1) >>> 0; // log(58) / log(256), rounded up.
    var b256 = new Uint8Array(size);
    // Process the characters.
    while (source[psz]) {
      // Decode character
      var carry = BASE_MAP[source.charCodeAt(psz)];
      // Invalid character
      if (carry === 255) {
        return;
      }
      var i = 0;
      for (
        var it3 = size - 1;
        (carry !== 0 || i < length) && it3 !== -1;
        it3--, i++
      ) {
        carry += (BASE * b256[it3]) >>> 0;
        b256[it3] = carry % 256 >>> 0;
        carry = (carry / 256) >>> 0;
      }
      if (carry !== 0) {
        throw new Error("Non-zero carry");
      }
      length = i;
      psz++;
    }
    // Skip trailing spaces.
    if (source[psz] === " ") {
      return;
    }
    // Skip leading zeroes in b256.
    var it4 = size - length;
    while (it4 !== size && b256[it4] === 0) {
      it4++;
    }
    var vch = new Uint8Array(zeroes + (size - it4));
    var j = zeroes;
    while (it4 !== size) {
      vch[j++] = b256[it4++];
    }
    return vch;
  }
  /**
   * @param {string | string[]} string
   */
  function decode(string) {
    var buffer = decodeUnsafe(string);
    if (buffer) {
      return buffer;
    }
    throw new Error(`Non-${name} character`);
  }
  return {
    encode: encode,
    decodeUnsafe: decodeUnsafe,
    decode: decode,
  };
}
var src$2 = base$2;
var _brrp__multiformats_scope_baseX$2 = src$2;

/**
 * Class represents both BaseEncoder and MultibaseEncoder meaning it
 * can be used to encode to multibase or base encode without multibase
 * prefix.
 */
let Encoder$2 = class Encoder {
  name;
  prefix;
  baseEncode;
  constructor(name, prefix, baseEncode) {
    this.name = name;
    this.prefix = prefix;
    this.baseEncode = baseEncode;
  }
  encode(bytes) {
    if (bytes instanceof Uint8Array) {
      return `${this.prefix}${this.baseEncode(bytes)}`;
    } else {
      throw Error("Unknown type, must be binary type");
    }
  }
};
/**
 * Class represents both BaseDecoder and MultibaseDecoder so it could be used
 * to decode multibases (with matching prefix) or just base decode strings
 * with corresponding base encoding.
 */
let Decoder$2 = class Decoder {
  name;
  prefix;
  baseDecode;
  prefixCodePoint;
  constructor(name, prefix, baseDecode) {
    this.name = name;
    this.prefix = prefix;
    const prefixCodePoint = prefix.codePointAt(0);
    /* c8 ignore next 3 */
    if (prefixCodePoint === undefined) {
      throw new Error("Invalid prefix character");
    }
    this.prefixCodePoint = prefixCodePoint;
    this.baseDecode = baseDecode;
  }
  decode(text) {
    if (typeof text === "string") {
      if (text.codePointAt(0) !== this.prefixCodePoint) {
        throw Error(
          `Unable to decode multibase string ${JSON.stringify(text)}, ${
            this.name
          } decoder only supports inputs prefixed with ${this.prefix}`
        );
      }
      return this.baseDecode(text.slice(this.prefix.length));
    } else {
      throw Error("Can only multibase decode strings");
    }
  }
  or(decoder) {
    return or$a(this, decoder);
  }
};
let ComposedDecoder$2 = class ComposedDecoder {
  decoders;
  constructor(decoders) {
    this.decoders = decoders;
  }
  or(decoder) {
    return or$a(this, decoder);
  }
  decode(input) {
    const prefix = input[0];
    const decoder = this.decoders[prefix];
    if (decoder != null) {
      return decoder.decode(input);
    } else {
      throw RangeError(
        `Unable to decode multibase string ${JSON.stringify(
          input
        )}, only inputs prefixed with ${Object.keys(
          this.decoders
        )} are supported`
      );
    }
  }
};
function or$a(left, right) {
  // eslint-disable-next-line @typescript-eslint/consistent-type-assertions
  return new ComposedDecoder$2({
    ...(left.decoders ?? { [left.prefix]: left }),
    ...(right.decoders ?? { [right.prefix]: right }),
  });
}
let Codec$2 = class Codec {
  name;
  prefix;
  baseEncode;
  baseDecode;
  encoder;
  decoder;
  constructor(name, prefix, baseEncode, baseDecode) {
    this.name = name;
    this.prefix = prefix;
    this.baseEncode = baseEncode;
    this.baseDecode = baseDecode;
    this.encoder = new Encoder$2(name, prefix, baseEncode);
    this.decoder = new Decoder$2(name, prefix, baseDecode);
  }
  encode(input) {
    return this.encoder.encode(input);
  }
  decode(input) {
    return this.decoder.decode(input);
  }
};
function from$g({ name, prefix, encode, decode }) {
  return new Codec$2(name, prefix, encode, decode);
}
function baseX$2({ name, prefix, alphabet }) {
  const { encode, decode } = _brrp__multiformats_scope_baseX$2(alphabet, name);
  return from$g({
    prefix,
    name,
    encode,
    decode: (text) => coerce$2(decode(text)),
  });
}
function decode$C(string, alphabet, bitsPerChar, name) {
  // Build the character lookup table:
  const codes = {};
  for (let i = 0; i < alphabet.length; ++i) {
    codes[alphabet[i]] = i;
  }
  // Count the padding bytes:
  let end = string.length;
  while (string[end - 1] === "=") {
    --end;
  }
  // Allocate the output:
  const out = new Uint8Array(((end * bitsPerChar) / 8) | 0);
  // Parse the data:
  let bits = 0; // Number of bits currently in the buffer
  let buffer = 0; // Bits waiting to be written out, MSB first
  let written = 0; // Next byte to write
  for (let i = 0; i < end; ++i) {
    // Read one character from the string:
    const value = codes[string[i]];
    if (value === undefined) {
      throw new SyntaxError(`Non-${name} character`);
    }
    // Append the bits to the buffer:
    buffer = (buffer << bitsPerChar) | value;
    bits += bitsPerChar;
    // Write out some bits if the buffer has a byte's worth:
    if (bits >= 8) {
      bits -= 8;
      out[written++] = 0xff & (buffer >> bits);
    }
  }
  // Verify that we have received just enough bits:
  if (bits >= bitsPerChar || (0xff & (buffer << (8 - bits))) !== 0) {
    throw new SyntaxError("Unexpected end of data");
  }
  return out;
}
function encode$s(data, alphabet, bitsPerChar) {
  const pad = alphabet[alphabet.length - 1] === "=";
  const mask = (1 << bitsPerChar) - 1;
  let out = "";
  let bits = 0; // Number of bits currently in the buffer
  let buffer = 0; // Bits waiting to be written out, MSB first
  for (let i = 0; i < data.length; ++i) {
    // Slurp data into the buffer:
    buffer = (buffer << 8) | data[i];
    bits += 8;
    // Write out as much as we can:
    while (bits > bitsPerChar) {
      bits -= bitsPerChar;
      out += alphabet[mask & (buffer >> bits)];
    }
  }
  // Partial character:
  if (bits !== 0) {
    out += alphabet[mask & (buffer << (bitsPerChar - bits))];
  }
  // Add padding characters until we hit a byte boundary:
  if (pad) {
    while (((out.length * bitsPerChar) & 7) !== 0) {
      out += "=";
    }
  }
  return out;
}
/**
 * RFC4648 Factory
 */
function rfc4648$2({ name, prefix, bitsPerChar, alphabet }) {
  return from$g({
    prefix,
    name,
    encode(input) {
      return encode$s(input, alphabet, bitsPerChar);
    },
    decode(input) {
      return decode$C(input, alphabet, bitsPerChar, name);
    },
  });
}

const base32$2 = rfc4648$2({
  prefix: "b",
  name: "base32",
  alphabet: "abcdefghijklmnopqrstuvwxyz234567",
  bitsPerChar: 5,
});
rfc4648$2({
  prefix: "B",
  name: "base32upper",
  alphabet: "ABCDEFGHIJKLMNOPQRSTUVWXYZ234567",
  bitsPerChar: 5,
});
rfc4648$2({
  prefix: "c",
  name: "base32pad",
  alphabet: "abcdefghijklmnopqrstuvwxyz234567=",
  bitsPerChar: 5,
});
rfc4648$2({
  prefix: "C",
  name: "base32padupper",
  alphabet: "ABCDEFGHIJKLMNOPQRSTUVWXYZ234567=",
  bitsPerChar: 5,
});
rfc4648$2({
  prefix: "v",
  name: "base32hex",
  alphabet: "0123456789abcdefghijklmnopqrstuv",
  bitsPerChar: 5,
});
rfc4648$2({
  prefix: "V",
  name: "base32hexupper",
  alphabet: "0123456789ABCDEFGHIJKLMNOPQRSTUV",
  bitsPerChar: 5,
});
rfc4648$2({
  prefix: "t",
  name: "base32hexpad",
  alphabet: "0123456789abcdefghijklmnopqrstuv=",
  bitsPerChar: 5,
});
rfc4648$2({
  prefix: "T",
  name: "base32hexpadupper",
  alphabet: "0123456789ABCDEFGHIJKLMNOPQRSTUV=",
  bitsPerChar: 5,
});
rfc4648$2({
  prefix: "h",
  name: "base32z",
  alphabet: "ybndrfg8ejkmcpqxot1uwisza345h769",
  bitsPerChar: 5,
});

const base36 = baseX$2({
  prefix: "k",
  name: "base36",
  alphabet: "0123456789abcdefghijklmnopqrstuvwxyz",
});
baseX$2({
  prefix: "K",
  name: "base36upper",
  alphabet: "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ",
});

const base58btc$2 = baseX$2({
  name: "base58btc",
  prefix: "z",
  alphabet: "123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz",
});
baseX$2({
  name: "base58flickr",
  prefix: "Z",
  alphabet: "123456789abcdefghijkmnopqrstuvwxyzABCDEFGHJKLMNPQRSTUVWXYZ",
});

/* eslint-disable */
var encode_1$4 = encode$r;
var MSB$5 = 0x80,
  MSBALL$4 = -128,
  INT$4 = Math.pow(2, 31);
/**
 * @param {number} num
 * @param {number[]} out
 * @param {number} offset
 */
function encode$r(num, out, offset) {
  out = out || [];
  offset = offset || 0;
  var oldOffset = offset;
  while (num >= INT$4) {
    out[offset++] = (num & 0xff) | MSB$5;
    num /= 128;
  }
  while (num & MSBALL$4) {
    out[offset++] = (num & 0xff) | MSB$5;
    num >>>= 7;
  }
  out[offset] = num | 0;
  // @ts-ignore
  encode$r.bytes = offset - oldOffset + 1;
  return out;
}
var decode$B = read$6;
var MSB$1$3 = 0x80,
  REST$1$3 = 0x7f;
/**
 * @param {string | any[]} buf
 * @param {number} offset
 */
function read$6(buf, offset) {
  var res = 0,
    offset = offset || 0,
    shift = 0,
    counter = offset,
    b,
    l = buf.length;
  do {
    if (counter >= l) {
      // @ts-ignore
      read$6.bytes = 0;
      throw new RangeError("Could not decode varint");
    }
    b = buf[counter++];
    res +=
      shift < 28
        ? (b & REST$1$3) << shift
        : (b & REST$1$3) * Math.pow(2, shift);
    shift += 7;
  } while (b >= MSB$1$3);
  // @ts-ignore
  read$6.bytes = counter - offset;
  return res;
}
var N1$3 = Math.pow(2, 7);
var N2$3 = Math.pow(2, 14);
var N3$3 = Math.pow(2, 21);
var N4$3 = Math.pow(2, 28);
var N5$3 = Math.pow(2, 35);
var N6$3 = Math.pow(2, 42);
var N7$3 = Math.pow(2, 49);
var N8$3 = Math.pow(2, 56);
var N9$3 = Math.pow(2, 63);
var length$4 = function (/** @type {number} */ value) {
  return value < N1$3
    ? 1
    : value < N2$3
    ? 2
    : value < N3$3
    ? 3
    : value < N4$3
    ? 4
    : value < N5$3
    ? 5
    : value < N6$3
    ? 6
    : value < N7$3
    ? 7
    : value < N8$3
    ? 8
    : value < N9$3
    ? 9
    : 10;
};
var varint$5 = {
  encode: encode_1$4,
  decode: decode$B,
  encodingLength: length$4,
};
var _brrp_varint$3 = varint$5;

function decode$A(data, offset = 0) {
  const code = _brrp_varint$3.decode(data, offset);
  return [code, _brrp_varint$3.decode.bytes];
}
function encodeTo$3(int, target, offset = 0) {
  _brrp_varint$3.encode(int, target, offset);
  return target;
}
function encodingLength$3(int) {
  return _brrp_varint$3.encodingLength(int);
}

/**
 * Creates a multihash digest.
 */
function create$h(code, digest) {
  const size = digest.byteLength;
  const sizeOffset = encodingLength$3(code);
  const digestOffset = sizeOffset + encodingLength$3(size);
  const bytes = new Uint8Array(digestOffset + size);
  encodeTo$3(code, bytes, 0);
  encodeTo$3(size, bytes, sizeOffset);
  bytes.set(digest, digestOffset);
  return new Digest$5(code, size, digest, bytes);
}
/**
 * Turns bytes representation of multihash digest into an instance.
 */
function decode$z(multihash) {
  const bytes = coerce$2(multihash);
  const [code, sizeOffset] = decode$A(bytes);
  const [size, digestOffset] = decode$A(bytes.subarray(sizeOffset));
  const digest = bytes.subarray(sizeOffset + digestOffset);
  if (digest.byteLength !== size) {
    throw new Error("Incorrect length");
  }
  return new Digest$5(code, size, digest, bytes);
}
function equals$5(a, b) {
  if (a === b) {
    return true;
  } else {
    const data = b;
    return (
      a.code === data.code &&
      a.size === data.size &&
      data.bytes instanceof Uint8Array &&
      equals$6(a.bytes, data.bytes)
    );
  }
}
/**
 * Represents a multihash digest which carries information about the
 * hashing algorithm and an actual hash digest.
 */
let Digest$5 = class Digest {
  code;
  size;
  digest;
  bytes;
  /**
   * Creates a multihash digest.
   */
  constructor(code, size, digest, bytes) {
    this.code = code;
    this.size = size;
    this.digest = digest;
    this.bytes = bytes;
  }
};

function format$6(link, base) {
  const { bytes, version } = link;
  switch (version) {
    case 0:
      return toStringV0$2(
        bytes,
        baseCache$2(link),
        base ?? base58btc$2.encoder
      );
    default:
      return toStringV1$2(bytes, baseCache$2(link), base ?? base32$2.encoder);
  }
}
const cache$3 = new WeakMap();
function baseCache$2(cid) {
  const baseCache = cache$3.get(cid);
  if (baseCache == null) {
    const baseCache = new Map();
    cache$3.set(cid, baseCache);
    return baseCache;
  }
  return baseCache;
}
let CID$2 = class CID {
  code;
  version;
  multihash;
  bytes;
  "/";
  /**
   * @param version - Version of the CID
   * @param code - Code of the codec content is encoded in, see https://github.com/multiformats/multicodec/blob/master/table.csv
   * @param multihash - (Multi)hash of the of the content.
   */
  constructor(version, code, multihash, bytes) {
    this.code = code;
    this.version = version;
    this.multihash = multihash;
    this.bytes = bytes;
    // flag to serializers that this is a CID and
    // should be treated specially
    this["/"] = bytes;
  }
  /**
   * Signalling `cid.asCID === cid` has been replaced with `cid['/'] === cid.bytes`
   * please either use `CID.asCID(cid)` or switch to new signalling mechanism
   *
   * @deprecated
   */
  get asCID() {
    return this;
  }
  // ArrayBufferView
  get byteOffset() {
    return this.bytes.byteOffset;
  }
  // ArrayBufferView
  get byteLength() {
    return this.bytes.byteLength;
  }
  toV0() {
    switch (this.version) {
      case 0: {
        return this;
      }
      case 1: {
        const { code, multihash } = this;
        if (code !== DAG_PB_CODE$3) {
          throw new Error("Cannot convert a non dag-pb CID to CIDv0");
        }
        // sha2-256
        if (multihash.code !== SHA_256_CODE$2) {
          throw new Error("Cannot convert non sha2-256 multihash CID to CIDv0");
        }
        return CID.createV0(multihash);
      }
      default: {
        throw Error(
          `Can not convert CID version ${this.version} to version 0. This is a bug please report`
        );
      }
    }
  }
  toV1() {
    switch (this.version) {
      case 0: {
        const { code, digest } = this.multihash;
        const multihash = create$h(code, digest);
        return CID.createV1(this.code, multihash);
      }
      case 1: {
        return this;
      }
      default: {
        throw Error(
          `Can not convert CID version ${this.version} to version 1. This is a bug please report`
        );
      }
    }
  }
  equals(other) {
    return CID.equals(this, other);
  }
  static equals(self, other) {
    const unknown = other;
    return (
      unknown != null &&
      self.code === unknown.code &&
      self.version === unknown.version &&
      equals$5(self.multihash, unknown.multihash)
    );
  }
  toString(base) {
    return format$6(this, base);
  }
  toJSON() {
    return { "/": format$6(this) };
  }
  link() {
    return this;
  }
  [Symbol.toStringTag] = "CID";
  // Legacy
  [Symbol.for("nodejs.util.inspect.custom")]() {
    return `CID(${this.toString()})`;
  }
  /**
   * Takes any input `value` and returns a `CID` instance if it was
   * a `CID` otherwise returns `null`. If `value` is instanceof `CID`
   * it will return value back. If `value` is not instance of this CID
   * class, but is compatible CID it will return new instance of this
   * `CID` class. Otherwise returns null.
   *
   * This allows two different incompatible versions of CID library to
   * co-exist and interop as long as binary interface is compatible.
   */
  static asCID(input) {
    if (input == null) {
      return null;
    }
    const value = input;
    if (value instanceof CID) {
      // If value is instance of CID then we're all set.
      return value;
    } else if (
      (value["/"] != null && value["/"] === value.bytes) ||
      value.asCID === value
    ) {
      // If value isn't instance of this CID class but `this.asCID === this` or
      // `value['/'] === value.bytes` is true it is CID instance coming from a
      // different implementation (diff version or duplicate). In that case we
      // rebase it to this `CID` implementation so caller is guaranteed to get
      // instance with expected API.
      const { version, code, multihash, bytes } = value;
      return new CID(
        version,
        code,
        multihash,
        bytes ?? encodeCID$2(version, code, multihash.bytes)
      );
    } else if (value[cidSymbol$2] === true) {
      // If value is a CID from older implementation that used to be tagged via
      // symbol we still rebase it to the this `CID` implementation by
      // delegating that to a constructor.
      const { version, multihash, code } = value;
      const digest = decode$z(multihash);
      return CID.create(version, code, digest);
    } else {
      // Otherwise value is not a CID (or an incompatible version of it) in
      // which case we return `null`.
      return null;
    }
  }
  /**
   * @param version - Version of the CID
   * @param code - Code of the codec content is encoded in, see https://github.com/multiformats/multicodec/blob/master/table.csv
   * @param digest - (Multi)hash of the of the content.
   */
  static create(version, code, digest) {
    if (typeof code !== "number") {
      throw new Error("String codecs are no longer supported");
    }
    if (!(digest.bytes instanceof Uint8Array)) {
      throw new Error("Invalid digest");
    }
    switch (version) {
      case 0: {
        if (code !== DAG_PB_CODE$3) {
          throw new Error(
            `Version 0 CID must use dag-pb (code: ${DAG_PB_CODE$3}) block encoding`
          );
        } else {
          return new CID(version, code, digest, digest.bytes);
        }
      }
      case 1: {
        const bytes = encodeCID$2(version, code, digest.bytes);
        return new CID(version, code, digest, bytes);
      }
      default: {
        throw new Error("Invalid version");
      }
    }
  }
  /**
   * Simplified version of `create` for CIDv0.
   */
  static createV0(digest) {
    return CID.create(0, DAG_PB_CODE$3, digest);
  }
  /**
   * Simplified version of `create` for CIDv1.
   *
   * @param code - Content encoding format code.
   * @param digest - Multihash of the content.
   */
  static createV1(code, digest) {
    return CID.create(1, code, digest);
  }
  /**
   * Decoded a CID from its binary representation. The byte array must contain
   * only the CID with no additional bytes.
   *
   * An error will be thrown if the bytes provided do not contain a valid
   * binary representation of a CID.
   */
  static decode(bytes) {
    const [cid, remainder] = CID.decodeFirst(bytes);
    if (remainder.length !== 0) {
      throw new Error("Incorrect length");
    }
    return cid;
  }
  /**
   * Decoded a CID from its binary representation at the beginning of a byte
   * array.
   *
   * Returns an array with the first element containing the CID and the second
   * element containing the remainder of the original byte array. The remainder
   * will be a zero-length byte array if the provided bytes only contained a
   * binary CID representation.
   */
  static decodeFirst(bytes) {
    const specs = CID.inspectBytes(bytes);
    const prefixSize = specs.size - specs.multihashSize;
    const multihashBytes = coerce$2(
      bytes.subarray(prefixSize, prefixSize + specs.multihashSize)
    );
    if (multihashBytes.byteLength !== specs.multihashSize) {
      throw new Error("Incorrect length");
    }
    const digestBytes = multihashBytes.subarray(
      specs.multihashSize - specs.digestSize
    );
    const digest = new Digest$5(
      specs.multihashCode,
      specs.digestSize,
      digestBytes,
      multihashBytes
    );
    const cid =
      specs.version === 0
        ? CID.createV0(digest)
        : CID.createV1(specs.codec, digest);
    return [cid, bytes.subarray(specs.size)];
  }
  /**
   * Inspect the initial bytes of a CID to determine its properties.
   *
   * Involves decoding up to 4 varints. Typically this will require only 4 to 6
   * bytes but for larger multicodec code values and larger multihash digest
   * lengths these varints can be quite large. It is recommended that at least
   * 10 bytes be made available in the `initialBytes` argument for a complete
   * inspection.
   */
  static inspectBytes(initialBytes) {
    let offset = 0;
    const next = () => {
      const [i, length] = decode$A(initialBytes.subarray(offset));
      offset += length;
      return i;
    };
    let version = next();
    let codec = DAG_PB_CODE$3;
    if (version === 18) {
      // CIDv0
      version = 0;
      offset = 0;
    } else {
      codec = next();
    }
    if (version !== 0 && version !== 1) {
      throw new RangeError(`Invalid CID version ${version}`);
    }
    const prefixSize = offset;
    const multihashCode = next(); // multihash code
    const digestSize = next(); // multihash length
    const size = offset + digestSize;
    const multihashSize = size - prefixSize;
    return { version, codec, multihashCode, digestSize, multihashSize, size };
  }
  /**
   * Takes cid in a string representation and creates an instance. If `base`
   * decoder is not provided will use a default from the configuration. It will
   * throw an error if encoding of the CID is not compatible with supplied (or
   * a default decoder).
   */
  static parse(source, base) {
    const [prefix, bytes] = parseCIDtoBytes$2(source, base);
    const cid = CID.decode(bytes);
    if (cid.version === 0 && source[0] !== "Q") {
      throw Error("Version 0 CID string must not include multibase prefix");
    }
    // Cache string representation to avoid computing it on `this.toString()`
    baseCache$2(cid).set(prefix, source);
    return cid;
  }
};
function parseCIDtoBytes$2(source, base) {
  switch (source[0]) {
    // CIDv0 is parsed differently
    case "Q": {
      const decoder = base ?? base58btc$2;
      return [
        base58btc$2.prefix,
        decoder.decode(`${base58btc$2.prefix}${source}`),
      ];
    }
    case base58btc$2.prefix: {
      const decoder = base ?? base58btc$2;
      return [base58btc$2.prefix, decoder.decode(source)];
    }
    case base32$2.prefix: {
      const decoder = base ?? base32$2;
      return [base32$2.prefix, decoder.decode(source)];
    }
    case base36.prefix: {
      const decoder = base ?? base36;
      return [base36.prefix, decoder.decode(source)];
    }
    default: {
      if (base == null) {
        throw Error(
          "To parse non base32, base36 or base58btc encoded CID multibase decoder must be provided"
        );
      }
      return [source[0], base.decode(source)];
    }
  }
}
function toStringV0$2(bytes, cache, base) {
  const { prefix } = base;
  if (prefix !== base58btc$2.prefix) {
    throw Error(`Cannot string encode V0 in ${base.name} encoding`);
  }
  const cid = cache.get(prefix);
  if (cid == null) {
    const cid = base.encode(bytes).slice(1);
    cache.set(prefix, cid);
    return cid;
  } else {
    return cid;
  }
}
function toStringV1$2(bytes, cache, base) {
  const { prefix } = base;
  const cid = cache.get(prefix);
  if (cid == null) {
    const cid = base.encode(bytes);
    cache.set(prefix, cid);
    return cid;
  } else {
    return cid;
  }
}
const DAG_PB_CODE$3 = 0x70;
const SHA_256_CODE$2 = 0x12;
function encodeCID$2(version, code, multihash) {
  const codeOffset = encodingLength$3(version);
  const hashOffset = codeOffset + encodingLength$3(code);
  const bytes = new Uint8Array(hashOffset + multihash.byteLength);
  encodeTo$3(version, bytes, 0);
  encodeTo$3(code, bytes, codeOffset);
  bytes.set(multihash, hashOffset);
  return bytes;
}
const cidSymbol$2 = Symbol.for("@ipld/js-cid/CID");

// https://github.com/ipfs/go-ipfs/issues/3570#issuecomment-273931692
const CID_CBOR_TAG = 42;

/**
 * @template T
 * @typedef {import('multiformats/codecs/interface').ByteView<T>} ByteView
 */

/**
 * @template T
 * @typedef {import('multiformats/codecs/interface').ArrayBufferView<T>} ArrayBufferView
 */

/**
 * @template T
 * @param {ByteView<T> | ArrayBufferView<T>} buf
 * @returns {ByteView<T>}
 */
function toByteView$2(buf) {
  if (buf instanceof ArrayBuffer) {
    return new Uint8Array(buf, 0, buf.byteLength);
  }

  return buf;
}

/**
 * cidEncoder will receive all Objects during encode, it needs to filter out
 * anything that's not a CID and return `null` for that so it's encoded as
 * normal.
 *
 * @param {any} obj
 * @returns {cborg.Token[]|null}
 */
function cidEncoder$1(obj) {
  if (obj.asCID !== obj && obj["/"] !== obj.bytes) {
    return null; // any other kind of object
  }
  const cid = CID$2.asCID(obj);
  /* c8 ignore next 4 */
  // very unlikely case, and it'll probably throw a recursion error in cborg
  if (!cid) {
    return null;
  }
  const bytes = new Uint8Array(cid.bytes.byteLength + 1);
  bytes.set(cid.bytes, 1); // prefix is 0x00, for historical reasons
  return [new Token(Type.tag, CID_CBOR_TAG), new Token(Type.bytes, bytes)];
}

// eslint-disable-next-line jsdoc/require-returns-check
/**
 * Intercept all `undefined` values from an object walk and reject the entire
 * object if we find one.
 *
 * @returns {null}
 */
function undefinedEncoder$1() {
  throw new Error(
    "`undefined` is not supported by the IPLD Data Model and cannot be encoded"
  );
}

/**
 * Intercept all `number` values from an object walk and reject the entire
 * object if we find something that doesn't fit the IPLD data model (NaN &
 * Infinity).
 *
 * @param {number} num
 * @returns {null}
 */
function numberEncoder$1(num) {
  if (Number.isNaN(num)) {
    throw new Error(
      "`NaN` is not supported by the IPLD Data Model and cannot be encoded"
    );
  }
  if (num === Infinity || num === -Infinity) {
    throw new Error(
      "`Infinity` and `-Infinity` is not supported by the IPLD Data Model and cannot be encoded"
    );
  }
  return null;
}

const _encodeOptions = {
  float64: true,
  typeEncoders: {
    Object: cidEncoder$1,
    undefined: undefinedEncoder$1,
    number: numberEncoder$1,
  },
};

({
  ..._encodeOptions,
  typeEncoders: {
    ..._encodeOptions.typeEncoders,
  },
});

/**
 * @param {Uint8Array} bytes
 * @returns {CID}
 */
function cidDecoder(bytes) {
  if (bytes[0] !== 0) {
    throw new Error("Invalid CID for CBOR tag 42; expected leading 0x00");
  }
  return CID$2.decode(bytes.subarray(1)); // ignore leading 0x00
}

const _decodeOptions = {
  allowIndefinite: false,
  coerceUndefinedToNull: true,
  allowNaN: false,
  allowInfinity: false,
  allowBigInt: true, // this will lead to BigInt for ints outside of
  // safe-integer range, which may surprise users
  strict: true,
  useMaps: false,
  rejectDuplicateMapKeys: true,
  /** @type {import('cborg').TagDecoder[]} */
  tags: [],
};
_decodeOptions.tags[CID_CBOR_TAG] = cidDecoder;

({
  ..._decodeOptions,
  tags: _decodeOptions.tags.slice(),
});

const name$9 = "dag-cbor";
const code$g = 0x71;

/**
 * @template T
 * @param {T} node
 * @returns {ByteView<T>}
 */
const encode$q = (node) => encode$t(node, _encodeOptions);

/**
 * @template T
 * @param {ByteView<T> | ArrayBufferView<T>} data
 * @returns {T}
 */
const decode$y = (data) => decode$D(toByteView$2(data), _decodeOptions);

const encoder = new TextEncoder();
const decoder = new TextDecoder();

/**
 * @template T
 * @param {import('./ucan').ToString<T>} text
 * @returns {import('./ucan').ByteView<T>}
 */
const encode$p = (text) => encoder.encode(text);

/**
 * @template T
 * @param {import('./ucan').ByteView<T>} bytes
 * @returns {import('./ucan').ToString<T>}
 */
const decode$x = (bytes) => decoder.decode(bytes);

const DAG_PB_CODE$2 = 0x70;
/**
 * Simplified version of `create` for CIDv0.
 */
function createLegacy(digest) {
  return CID$2.create(0, DAG_PB_CODE$2, digest);
}
/**
 * Simplified version of `create` for CIDv1.
 *
 * @param code - Content encoding format code.
 * @param digest - Miltihash of the content.
 */
function create$g(code, digest) {
  return CID$2.create(1, code, digest);
}
/**
 * Type predicate returns true if value is the link.
 */
function isLink(value) {
  if (value == null) {
    return false;
  }
  const withSlash = value;
  if (withSlash["/"] != null && withSlash["/"] === withSlash.bytes) {
    return true;
  }
  const withAsCID = value;
  if (withAsCID.asCID === value) {
    return true;
  }
  return false;
}
/**
 * Takes cid in a string representation and creates an instance. If `base`
 * decoder is not provided will use a default from the configuration. It will
 * throw an error if encoding of the CID is not compatible with supplied (or
 * a default decoder).
 */
function parse$2(source, base) {
  return CID$2.parse(source, base);
}

const code$f = 0x0;
const name$8 = "identity";
const encode$o = coerce$2;
function digest$3(input) {
  return create$h(code$f, encode$o(input));
}
const identity = {
  code: code$f,
  name: name$8,
  encode: encode$o,
  digest: digest$3,
};

function from$f({ name, code, encode }) {
  return new Hasher$4(name, code, encode);
}
/**
 * Hasher represents a hashing algorithm implementation that produces as
 * `MultihashDigest`.
 */
let Hasher$4 = class Hasher {
  name;
  code;
  encode;
  constructor(name, code, encode) {
    this.name = name;
    this.code = code;
    this.encode = encode;
  }
  digest(input) {
    if (input instanceof Uint8Array) {
      const result = this.encode(input);
      return result instanceof Uint8Array
        ? create$h(this.code, result)
        : /* c8 ignore next 1 */
          result.then((digest) => create$h(this.code, digest));
    } else {
      throw Error("Unknown type, must be binary type");
      /* c8 ignore next 1 */
    }
  }
};

const DID_PREFIX = "did:";
const DID_PREFIX_SIZE = DID_PREFIX.length;
const DID_KEY_PREFIX = `did:key:`;
const DID_KEY_PREFIX_SIZE = DID_KEY_PREFIX.length;

const ED25519 = 0xed;
const RSA$1 = 0x1205;
const P256 = 0x1200;
const P384 = 0x1201;
const P521 = 0x1202;
const SECP256K1 = 0xe7;
const BLS12381G1$1 = 0xea;
const BLS12381G2$1 = 0xeb;
const DID_CORE = 0x0d1d;
const METHOD_OFFSET = encodingLength$3(DID_CORE);

/**
 * @typedef {typeof ED25519|typeof RSA|typeof P256|typeof P384|typeof P521|typeof DID_CORE} Code
 */

/**
 * Parses a DID string into a DID buffer view
 *
 * @template {UCAN.DID} ID
 * @param {ID|UCAN.ToString<unknown>} did
 * @returns {UCAN.PrincipalView<ID>}
 */
const parse$1 = (did) => {
  if (!did.startsWith(DID_PREFIX)) {
    throw new RangeError(`Invalid DID "${did}", must start with 'did:'`);
  } else if (did.startsWith(DID_KEY_PREFIX)) {
    const key = base58btc$2.decode(did.slice(DID_KEY_PREFIX_SIZE));
    return decode$w(key);
  } else {
    const suffix = encode$p(did.slice(DID_PREFIX_SIZE));
    const bytes = new Uint8Array(suffix.byteLength + METHOD_OFFSET);
    encodeTo$3(DID_CORE, bytes);
    bytes.set(suffix, METHOD_OFFSET);
    return new DID$1(bytes);
  }
};

/**
 * @template {UCAN.DID} ID
 * @param {UCAN.Principal<ID>} id
 * @returns {ID}
 */
const format$5 = (id) => id.did();

/**
 * @template {UCAN.DID} ID
 * @param {UCAN.PrincipalView<ID>|UCAN.ByteView<UCAN.Principal<ID>>|UCAN.Principal<ID>|ID|UCAN.ToJSONString<unknown>} principal
 * @returns {UCAN.PrincipalView<ID>}
 */
const from$e = (principal) => {
  if (principal instanceof DID$1) {
    return principal;
  } else if (principal instanceof Uint8Array) {
    return decode$w(principal);
  } else if (typeof principal === "string") {
    return parse$1(principal);
  } else {
    return parse$1(principal.did());
  }
};

/**
 * @template {UCAN.DID} ID
 * @param {UCAN.ByteView<UCAN.Principal<ID>>} bytes
 * @returns {UCAN.PrincipalView<ID>}
 */
const decode$w = (bytes) => {
  const [code] = decode$A(bytes);
  const { buffer, byteOffset, byteLength } = bytes;
  switch (code) {
    case P256:
      if (bytes.length > 35) {
        throw new RangeError(`Only p256-pub compressed is supported.`);
      }
    case ED25519:
    case RSA$1:
    case P384:
    case P521:
    case BLS12381G1$1:
    case BLS12381G2$1:
    case SECP256K1:
      return /** @type {UCAN.PrincipalView<any>} */ (
        new DIDKey(buffer, byteOffset, byteLength)
      );
    case DID_CORE:
      return new DID$1(buffer, byteOffset, byteLength);
    default:
      throw new RangeError(
        `Unsupported DID encoding, unknown multicode 0x${code.toString(16)}.`
      );
  }
};

/**
 * @template {UCAN.DID} ID
 * @implements {UCAN.PrincipalView<ID>}
 * @extends {Uint8Array}
 */
let DID$1 = class DID extends Uint8Array {
  /**
   * @returns {ID}
   */
  did() {
    const bytes = new Uint8Array(this.buffer, this.byteOffset + METHOD_OFFSET);
    return /** @type {ID} */ (`did:${decode$x(bytes)}`);
  }

  toJSON() {
    return this.did();
  }
};

/**
 * @implements {UCAN.PrincipalView<UCAN.DID<"key">>}
 * @extends {DID<UCAN.DID<"key">>}
 */
class DIDKey extends DID$1 {
  /**
   * @return {`did:key:${string}`}
   */
  did() {
    return `did:key:${base58btc$2.encode(this)}`;
  }
}

const code$e = 0x55;

const base64$1 = rfc4648$2({
  prefix: "m",
  name: "base64",
  alphabet: "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/",
  bitsPerChar: 6,
});
rfc4648$2({
  prefix: "M",
  name: "base64pad",
  alphabet: "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=",
  bitsPerChar: 6,
});
const base64url = rfc4648$2({
  prefix: "u",
  name: "base64url",
  alphabet: "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_",
  bitsPerChar: 6,
});
rfc4648$2({
  prefix: "U",
  name: "base64urlpad",
  alphabet: "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_=",
  bitsPerChar: 6,
});

const NON_STANDARD = 0xd000;
const ES256K = 0xd0e7;
const BLS12381G1 = 0xd0ea;
const BLS12381G2 = 0xd0eb;
const EdDSA = 0xd0ed;
const ES256 = 0xd01200;
const ES384 = 0xd01201;
const ES512 = 0xd01202;
const RS256 = 0xd01205;
const EIP191 = 0xd191;

/**
 * @param {number} code
 * @returns {string}
 */
const codeName = (code) => {
  switch (code) {
    case ES256K:
      return "ES256K";
    case BLS12381G1:
      return "BLS12381G1";
    case BLS12381G2:
      return "BLS12381G2";
    case EdDSA:
      return "EdDSA";
    case ES256:
      return "ES256";
    case ES384:
      return "ES384";
    case ES512:
      return "ES512";
    case RS256:
      return "RS256";
    case EIP191:
      return "EIP191";
    default:
      throw new RangeError(
        `Unknown signature algorithm code 0x${code.toString(16)}`
      );
  }
};

/**
 *
 * @param {string} name
 */
const nameCode = (name) => {
  switch (name) {
    case "ES256K":
      return ES256K;
    case "BLS12381G1":
      return BLS12381G1;
    case "BLS12381G2":
      return BLS12381G2;
    case "EdDSA":
      return EdDSA;
    case "ES256":
      return ES256;
    case "ES384":
      return ES384;
    case "ES512":
      return ES512;
    case "RS256":
      return RS256;
    case "EIP191":
      return EIP191;
    default:
      return NON_STANDARD;
  }
};

/**
 * @template {unknown} T
 * @template {number} A
 * @implements {UCAN.SignatureView<T, A>}
 */
let Signature$1 = class Signature extends Uint8Array {
  get code() {
    const [code] = decode$A(this);
    Object.defineProperties(this, { code: { value: code } });
    return /** @type {A} */ (code);
  }

  get size() {
    const value = size$3(this);
    Object.defineProperties(this, { size: { value } });
    return value;
  }
  get algorithm() {
    const value = algorithm(this);
    Object.defineProperties(this, { algorithm: { value } });
    return value;
  }

  get raw() {
    const { buffer, byteOffset, size, code } = this;
    const codeSize = encodingLength$3(code);
    const rawSize = encodingLength$3(size);
    const value = new Uint8Array(buffer, byteOffset + codeSize + rawSize, size);
    Object.defineProperties(this, { raw: { value } });
    return value;
  }

  /**
   * Verify that this signature was created by the given key.
   *
   * @param {UCAN.Crypto.Verifier<A>} signer
   * @param {UCAN.ByteView<T>} payload
   */
  async verify(signer, payload) {
    try {
      if ((await signer.verify(payload, this)) === true) {
        return { ok: {} };
      } else {
        throw new Error("Invalid signature");
      }
    } catch (cause) {
      return { error: /** @type {Error} */ (cause) };
    }
  }

  toJSON() {
    return toJSON$1(this);
  }
};

/**
 * @param {UCAN.Signature} signature
 */
const algorithm = (signature) => {
  const { code, raw, buffer, byteOffset } = signature;
  if (code === NON_STANDARD) {
    const offset =
      raw.byteLength +
      encodingLength$3(code) +
      encodingLength$3(raw.byteLength);
    const bytes = new Uint8Array(buffer, byteOffset + offset);
    return decode$x(bytes);
  } else {
    return codeName(code);
  }
};

/**
 * @param {UCAN.Signature} signature
 */
const size$3 = (signature) => {
  const offset = encodingLength$3(signature.code);
  const [size] = decode$A(
    new Uint8Array(signature.buffer, signature.byteOffset + offset)
  );
  return size;
};

/**
 * @template {unknown} T
 * @template {number} A
 * @param {A} code
 * @param {Uint8Array} raw
 * @returns {UCAN.SignatureView<T, A>}
 */
const create$f = (code, raw) => {
  codeName(code);
  const codeSize = encodingLength$3(code);
  const rawSize = encodingLength$3(raw.byteLength);

  /** @type {Signature<T, A>} */
  const signature = new Signature$1(codeSize + rawSize + raw.byteLength);
  encodeTo$3(code, signature);
  encodeTo$3(raw.byteLength, signature, codeSize);
  signature.set(raw, codeSize + rawSize);
  Object.defineProperties(signature, {
    code: { value: code },
    size: { value: raw.byteLength },
  });
  return signature;
};

/**
 * @template {unknown} T
 * @param {string} name
 * @param {Uint8Array} raw
 * @return {UCAN.SignatureView<T>}
 */
const createNamed = (name, raw) => {
  const code = nameCode(name);
  return code === NON_STANDARD
    ? createNonStandard(name, raw)
    : create$f(code, raw);
};

/**
 * @template {unknown} T
 * @param {string} name
 * @param {Uint8Array} raw
 * @return {UCAN.SignatureView<T, typeof NON_STANDARD>}
 */
const createNonStandard = (name, raw) => {
  const code = NON_STANDARD;
  const codeSize = encodingLength$3(code);
  const rawSize = encodingLength$3(raw.byteLength);
  const nameBytes = encode$p(name);
  /** @type {Signature<T, typeof NON_STANDARD>} */
  const signature = new Signature$1(
    codeSize + rawSize + raw.byteLength + nameBytes.byteLength
  );
  encodeTo$3(code, signature);
  encodeTo$3(raw.byteLength, signature, codeSize);
  signature.set(raw, codeSize + rawSize);
  signature.set(nameBytes, codeSize + rawSize + raw.byteLength);

  return signature;
};

/**
 * @template {unknown} T
 * @template {number} A
 * @param {UCAN.ByteView<UCAN.Signature<T, A>>} bytes
 * @returns {UCAN.SignatureView<T, A>}
 */
const view$4 = (bytes) =>
  new Signature$1(bytes.buffer, bytes.byteOffset, bytes.byteLength);

/**
 * @template {unknown} T
 * @template {number} A
 * @param {UCAN.ByteView<UCAN.Signature<T, A>>} bytes
 * @returns {UCAN.SignatureView<T, A>}
 */
const decode$v = (bytes) => {
  if (!(bytes instanceof Uint8Array)) {
    throw new TypeError(
      `Can only decode Uint8Array into a Signature, instead got ${JSON.stringify(
        bytes
      )}`
    );
  }

  /** @type {UCAN.SignatureView<T, A>} */
  const signature = view$4(bytes);
  const { code, algorithm, raw } = signature;
  return signature;
};

/**
 * @template {unknown} T
 * @template {number} A
 * @param {UCAN.Signature<T, A>} signature
 * @returns {UCAN.ByteView<UCAN.Signature<T, A>>}
 */
const encode$n = (signature) => decode$v(signature);

/**
 * @template {UCAN.Signature} Signature
 * @param {Signature} signature
 * @returns {UCAN.SignatureJSON<Signature>}
 */
const toJSON$1 = (signature) => ({
  "/": { bytes: base64$1.baseEncode(signature) },
});

/**
 * @template {UCAN.Capabilities} C
 * @param {Record<string, unknown>|UCAN.Payload<C>} data
 * @returns {UCAN.Payload<C>}
 */
const readPayload = (data) =>
  readPayloadWith(data, {
    readPrincipal,
    readProof,
  });

/**
 * @template {UCAN.Capabilities} C
 * @param {Record<string, unknown>|UCAN.Payload<C>} data
 * @returns {UCAN.Payload<C>}
 */
const readJWTPayload = (data) =>
  readPayloadWith(data, {
    readPrincipal: readStringPrincipal,
    readProof: readStringProof,
  });
/**
 *
 * @template {UCAN.Capabilities} C
 * @param {Record<string, unknown>|UCAN.Payload<C>} data
 * @param {object} readers
 * @param {(source:unknown, context:string) => UCAN.Principal} readers.readPrincipal
 * @param {(source:unknown, context:string) => UCAN.Link} readers.readProof
 * @returns {UCAN.Payload<C>}
 */
const readPayloadWith = (data, { readPrincipal, readProof }) => ({
  iss: readPrincipal(data.iss, "iss"),
  aud: readPrincipal(data.aud, "aud"),
  att: readCapabilities(data.att, "att"),
  prf: readOptionalArray(data.prf, readProof, "prf") || [],
  exp: readNullable(data.exp === Infinity ? null : data.exp, readInt$2, "exp"),
  nbf: readOptional(data.nbf, readInt$2, "nbf"),
  fct: readOptionalArray(data.fct, readFact, "fct") || [],
  nnc: readOptional(data.nnc, readString, "nnc"),
});

/**
 * @template {unknown} T
 * @template {number} A
 * @param {UCAN.ByteView<UCAN.Signature<T, A>>|unknown} source
 */
const readSignature = (source) => {
  if (source instanceof Uint8Array) {
    return decode$v(source);
  } else {
    throw new TypeError(
      `Can only decode Uint8Array into a Signature, instead got ${JSON.stringify(
        source
      )}`
    );
  }
};

/**
 * @param {unknown} input
 * @param {string} name
 * @returns {number}
 */
const readInt$2 = (input, name) =>
  Number.isInteger(input)
    ? /** @type {number} */ (input)
    : ParseError.throw(
        `Expected ${name} to be integer, instead got ${JSON.stringify(input)}`
      );

/**
 * @param {unknown} input
 * @param {string} context
 */

const readCapability = (input, context) =>
  readStruct(input, asCapability, context);

/**
 * @template {UCAN.Capabilities} C
 * @param {unknown|C} input
 * @param {string} context
 * @returns {C}
 */
const readCapabilities = (input, context) =>
  /** @type {C} */ (readArray(input, readCapability, context));

/**
 * @template {UCAN.Capability} C
 * @param {object & {can?:unknown, with?:unknown}|C} input
 * @returns {C}
 */
const asCapability = (input) =>
  /** @type {C} */ ({
    ...input,
    can: readAbility(input.can),
    with: readResource(input.with),
  });

/**
 * @param {unknown} input
 */
const readAbility = (input) =>
  typeof input !== "string"
    ? ParseError.throw(
        `Capability has invalid 'can: ${JSON.stringify(
          input
        )}', value must be a string`
      )
    : input.slice(1, -1).includes("/")
    ? /** @type {UCAN.Ability} */ (input.toLocaleLowerCase())
    : input === "*"
    ? input
    : ParseError.throw(
        `Capability has invalid 'can: "${input}"', value must have at least one path segment`
      );

/**
 * @param {unknown} input
 */
const readResource = (input) =>
  typeof input !== "string"
    ? ParseError.throw(
        `Capability has invalid 'with: ${JSON.stringify(
          input
        )}', value must be a string`
      )
    : parseURL(input) ||
      ParseError.throw(
        `Capability has invalid 'with: "${input}"', value must be a valid URI string`
      );

/**
 * @param {string} input
 */
const parseURL = (input) => {
  try {
    new URL(input);
    return input;
  } catch (_) {
    return null;
  }
};
/**
 * @template T
 * @param {unknown} input
 * @param {(input:unknown, context:string) => T} read
 * @param {string} context
 * @returns {T[]}
 */
const readArray = (input, read, context) =>
  Array.isArray(input)
    ? input.map((element, n) => read(element, `${context}[${n}]`))
    : ParseError.throw(`${context} must be an array`);

/**
 * @template T
 * @param {unknown} input
 * @param {(input:unknown, context: string) => T} reader
 * @param {string} context
 * @returns {T[]|undefined}
 */
const readOptionalArray = (input, reader, context) =>
  input === undefined ? input : readArray(input, reader, context);

/**
 * @template T
 * @param {unknown} input
 * @param {(input:object) => T} reader
 * @param {string} context
 * @returns {T}
 */
const readStruct = (input, reader, context) =>
  input != null && typeof input === "object"
    ? reader(input)
    : ParseError.throw(
        `${context} must be of type object, instead got ${input}`
      );

/**
 * @param {unknown} input
 * @param {string} context
 * @returns {UCAN.Fact}
 */
const readFact = (input, context) => readStruct(input, Object, context);

/**
 * @param {unknown} source
 * @param {string} context
 * @returns {UCAN.Link}
 */
const readProof = (source, context) =>
  isLink(source)
    ? /** @type {UCAN.Link} */ (source)
    : fail$1(
        `Expected ${context} to be IPLD link, instead got ${JSON.stringify(
          source
        )}`
      );

/**
 * @param {unknown} source
 * @param {string} context
 * @returns {UCAN.Link}
 */
const readStringProof = (source, context) =>
  parseProof(readString(source, context));

/**
 * @param {string} source
 * @returns {UCAN.Link}
 */
const parseProof = (source) => {
  // First we attempt to read proof as CID, if we fail fallback to reading it as
  // an inline proof.
  try {
    return parse$2(source);
  } catch (error) {
    return create$g(code$e, identity.digest(encode$p(source)));
  }
};

/**
 * @param {unknown} input
 * @param {string} context
 */
const readPrincipal = (input, context) => decode$w(readBytes$1(input, context));

/**
 * @param {unknown} source
 * @param {string} context
 */
const readStringPrincipal = (source, context) =>
  parse$1(readString(source, context));

/**
 * @template T
 * @param {unknown} source
 * @param {(source:unknown, context:string) => T} read
 * @param {string} [context]
 * @returns {T|undefined}
 */
const readOptional = (source, read, context = "Field") =>
  source !== undefined ? read(source, context) : undefined;

/**
 * @template T
 * @param {unknown} source
 * @param {(source:unknown, context:string) => T} read
 * @param {string} context
 * @returns {T|null}
 */
const readNullable = (source, read, context) =>
  source === null ? null : read(source, context);

/**
 * @param {unknown} source
 * @param {string} [context]
 * @returns {string}
 */
const readString = (source, context = "Field") =>
  typeof source === "string"
    ? source
    : fail$1(`${context} has invalid value ${source}`);

/**
 *
 * @param {unknown} source
 * @param {string} context
 * @returns {Uint8Array}
 */
const readBytes$1 = (source, context) =>
  source instanceof Uint8Array
    ? source
    : fail$1(
        `Expected ${context} to be Uint8Array, instead got ${JSON.stringify(
          source
        )}`
      );

/**
 * @param {unknown} input
 * @param {string} context
 * @returns {UCAN.Version}
 */
const readVersion = (input, context) =>
  /\d+\.\d+\.\d+/.test(/** @type {string} */ (input))
    ? /** @type {UCAN.Version} */ (input)
    : ParseError.throw(
        `Invalid version '${context}: ${JSON.stringify(input)}'`
      );

/**
 * @template {string|number|boolean|null} T
 * @param {unknown} input
 * @param {T} literal
 * @param {string} context
 * @returns {T}
 */
const readLiteral = (input, literal, context) =>
  input === literal
    ? literal
    : ParseError.throw(
        `Expected ${context} to be a ${JSON.stringify(
          literal
        )} instead got ${JSON.stringify(input)}`
      );

class ParseError extends TypeError {
  get name() {
    return "ParseError";
  }
  /**
   * @param {string} message
   * @returns {never}
   */
  static throw(message) {
    throw new this(message);
  }
}

/**
 * @param {string} reason
 */
const fail$1 = (reason) => ParseError.throw(reason);

/**
 * @typedef {import('../../interface').EncodeOptions} EncodeOptions
 * @typedef {import('../token').Token} Token
 * @typedef {import('../bl').Bl} Bl
 */

class JSONEncoder extends Array {
  constructor() {
    super();
    /** @type {{type:Type,elements:number}[]} */
    this.inRecursive = [];
  }

  /**
   * @param {Bl} buf
   */
  prefix(buf) {
    const recurs = this.inRecursive[this.inRecursive.length - 1];
    if (recurs) {
      if (recurs.type === Type.array) {
        recurs.elements++;
        if (recurs.elements !== 1) {
          // >first
          buf.push([44]); // ','
        }
      }
      if (recurs.type === Type.map) {
        recurs.elements++;
        if (recurs.elements !== 1) {
          // >first
          if (recurs.elements % 2 === 1) {
            // key
            buf.push([44]); // ','
          } else {
            buf.push([58]); // ':'
          }
        }
      }
    }
  }

  /**
   * @param {Bl} buf
   * @param {Token} token
   */
  [Type.uint.major](buf, token) {
    this.prefix(buf);
    const is = String(token.value);
    const isa = [];
    for (let i = 0; i < is.length; i++) {
      isa[i] = is.charCodeAt(i);
    }
    buf.push(isa);
  }

  /**
   * @param {Bl} buf
   * @param {Token} token
   */
  [Type.negint.major](buf, token) {
    // @ts-ignore hack
    this[Type.uint.major](buf, token);
  }

  /**
   * @param {Bl} _buf
   * @param {Token} _token
   */
  [Type.bytes.major](_buf, _token) {
    throw new Error(`${encodeErrPrefix} unsupported type: Uint8Array`);
  }

  /**
   * @param {Bl} buf
   * @param {Token} token
   */
  [Type.string.major](buf, token) {
    this.prefix(buf);
    // buf.push(34) // '"'
    // encodeUtf8(token.value, byts)
    // buf.push(34) // '"'
    const byts = fromString$1(JSON.stringify(token.value));
    buf.push(byts.length > 32 ? asU8A(byts) : byts);
  }

  /**
   * @param {Bl} buf
   * @param {Token} _token
   */
  [Type.array.major](buf, _token) {
    this.prefix(buf);
    this.inRecursive.push({ type: Type.array, elements: 0 });
    buf.push([91]); // '['
  }

  /**
   * @param {Bl} buf
   * @param {Token} _token
   */
  [Type.map.major](buf, _token) {
    this.prefix(buf);
    this.inRecursive.push({ type: Type.map, elements: 0 });
    buf.push([123]); // '{'
  }

  /**
   * @param {Bl} _buf
   * @param {Token} _token
   */
  [Type.tag.major](_buf, _token) {}

  /**
   * @param {Bl} buf
   * @param {Token} token
   */
  [Type.float.major](buf, token) {
    if (token.type.name === "break") {
      const recurs = this.inRecursive.pop();
      if (recurs) {
        if (recurs.type === Type.array) {
          buf.push([93]); // ']'
        } else if (recurs.type === Type.map) {
          buf.push([125]); // '}'
          /* c8 ignore next 3 */
        } else {
          throw new Error("Unexpected recursive type; this should not happen!");
        }
        return;
      }
      /* c8 ignore next 2 */
      throw new Error("Unexpected break; this should not happen!");
    }
    if (token.value === undefined) {
      throw new Error(`${encodeErrPrefix} unsupported type: undefined`);
    }

    this.prefix(buf);
    if (token.type.name === "true") {
      buf.push([116, 114, 117, 101]); // 'true'
      return;
    } else if (token.type.name === "false") {
      buf.push([102, 97, 108, 115, 101]); // 'false'
      return;
    } else if (token.type.name === "null") {
      buf.push([110, 117, 108, 108]); // 'null'
      return;
    }

    // number
    const is = String(token.value);
    const isa = [];
    let dp = false;
    for (let i = 0; i < is.length; i++) {
      isa[i] = is.charCodeAt(i);
      if (!dp && (isa[i] === 46 || isa[i] === 101 || isa[i] === 69)) {
        // '[.eE]'
        dp = true;
      }
    }
    if (!dp) {
      // need a decimal point for floats
      isa.push(46); // '.'
      isa.push(48); // '0'
    }
    buf.push(isa);
  }
}

// The below code is mostly taken and modified from https://github.com/feross/buffer
// Licensed MIT. Copyright (c) Feross Aboukhadijeh
// function encodeUtf8 (string, byts) {
//   let codePoint
//   const length = string.length
//   let leadSurrogate = null

//   for (let i = 0; i < length; ++i) {
//     codePoint = string.charCodeAt(i)

//     // is surrogate component
//     if (codePoint > 0xd7ff && codePoint < 0xe000) {
//       // last char was a lead
//       if (!leadSurrogate) {
//         // no lead yet
//         /* c8 ignore next 9 */
//         if (codePoint > 0xdbff) {
//           // unexpected trail
//           byts.push(0xef, 0xbf, 0xbd)
//           continue
//         } else if (i + 1 === length) {
//           // unpaired lead
//           byts.push(0xef, 0xbf, 0xbd)
//           continue
//         }

//         // valid lead
//         leadSurrogate = codePoint

//         continue
//       }

//       // 2 leads in a row
//       /* c8 ignore next 5 */
//       if (codePoint < 0xdc00) {
//         byts.push(0xef, 0xbf, 0xbd)
//         leadSurrogate = codePoint
//         continue
//       }

//       // valid surrogate pair
//       codePoint = (leadSurrogate - 0xd800 << 10 | codePoint - 0xdc00) + 0x10000
//     /* c8 ignore next 4 */
//     } else if (leadSurrogate) {
//       // valid bmp char, but last char was a lead
//       byts.push(0xef, 0xbf, 0xbd)
//     }

//     leadSurrogate = null

//     // encode utf8
//     if (codePoint < 0x80) {
//       // special JSON escapes
//       switch (codePoint) {
//         case 8: // '\b'
//           byts.push(92, 98) // '\\b'
//           continue
//         case 9: // '\t'
//           byts.push(92, 116) // '\\t'
//           continue
//         case 10: // '\n'
//           byts.push(92, 110) // '\\n'
//           continue
//         case 12: // '\f'
//           byts.push(92, 102) // '\\f'
//           continue
//         case 13: // '\r'
//           byts.push(92, 114) // '\\r'
//           continue
//         case 34: // '"'
//           byts.push(92, 34) // '\\"'
//           continue
//         case 92: // '\\'
//           byts.push(92, 92) // '\\\\'
//           continue
//       }

//       byts.push(codePoint)
//     } else if (codePoint < 0x800) {
//       /* c8 ignore next 1 */
//       byts.push(
//         codePoint >> 0x6 | 0xc0,
//         codePoint & 0x3f | 0x80
//       )
//     } else if (codePoint < 0x10000) {
//       /* c8 ignore next 1 */
//       byts.push(
//         codePoint >> 0xc | 0xe0,
//         codePoint >> 0x6 & 0x3f | 0x80,
//         codePoint & 0x3f | 0x80
//       )
//     /* c8 ignore next 9 */
//     } else if (codePoint < 0x110000) {
//       byts.push(
//         codePoint >> 0x12 | 0xf0,
//         codePoint >> 0xc & 0x3f | 0x80,
//         codePoint >> 0x6 & 0x3f | 0x80,
//         codePoint & 0x3f | 0x80
//       )
//     } else {
//       /* c8 ignore next 2 */
//       throw new Error('Invalid code point')
//     }
//   }
// }

/**
 * @param {(Token|Token[])[]} e1
 * @param {(Token|Token[])[]} e2
 * @returns {number}
 */
function mapSorter(e1, e2) {
  if (Array.isArray(e1[0]) || Array.isArray(e2[0])) {
    throw new Error(`${encodeErrPrefix} complex map keys are not supported`);
  }
  const keyToken1 = e1[0];
  const keyToken2 = e2[0];
  if (keyToken1.type !== Type.string || keyToken2.type !== Type.string) {
    throw new Error(`${encodeErrPrefix} non-string map keys are not supported`);
  }
  if (keyToken1 < keyToken2) {
    return -1;
  }
  if (keyToken1 > keyToken2) {
    return 1;
  }
  /* c8 ignore next 1 */
  throw new Error(
    `${encodeErrPrefix} unexpected duplicate map keys, this is not supported`
  );
}

const defaultEncodeOptions$1 = { addBreakTokens: true, mapSorter };

/**
 * @param {any} data
 * @param {EncodeOptions} [options]
 * @returns {Uint8Array}
 */
function encode$m(data, options) {
  options = Object.assign({}, defaultEncodeOptions$1, options);
  // @ts-ignore TokenTypeEncoder[] requires compareTokens() on each encoder, we don't use them here
  return encodeCustom(data, new JSONEncoder(), options);
}

/**
 * @typedef {import('../../interface').DecodeOptions} DecodeOptions
 * @typedef {import('../../interface').DecodeTokenizer} DecodeTokenizer
 */

/**
 * @implements {DecodeTokenizer}
 */
class Tokenizer {
  /**
   * @param {Uint8Array} data
   * @param {DecodeOptions} options
   */
  constructor(data, options = {}) {
    this._pos = 0;
    this.data = data;
    this.options = options;
    /** @type {string[]} */
    this.modeStack = ["value"];
    this.lastToken = "";
  }

  pos() {
    return this._pos;
  }

  /**
   * @returns {boolean}
   */
  done() {
    return this._pos >= this.data.length;
  }

  /**
   * @returns {number}
   */
  ch() {
    return this.data[this._pos];
  }

  /**
   * @returns {string}
   */
  currentMode() {
    return this.modeStack[this.modeStack.length - 1];
  }

  skipWhitespace() {
    let c = this.ch();
    // @ts-ignore
    while (
      c === 32 /* ' ' */ ||
      c === 9 /* '\t' */ ||
      c === 13 /* '\r' */ ||
      c === 10 /* '\n' */
    ) {
      c = this.data[++this._pos];
    }
  }

  /**
   * @param {number[]} str
   */
  expect(str) {
    if (this.data.length - this._pos < str.length) {
      throw new Error(
        `${decodeErrPrefix} unexpected end of input at position ${this._pos}`
      );
    }
    for (let i = 0; i < str.length; i++) {
      if (this.data[this._pos++] !== str[i]) {
        throw new Error(
          `${decodeErrPrefix} unexpected token at position ${
            this._pos
          }, expected to find '${String.fromCharCode(...str)}'`
        );
      }
    }
  }

  parseNumber() {
    const startPos = this._pos;
    let negative = false;
    let float = false;

    /**
     * @param {number[]} chars
     */
    const swallow = (chars) => {
      while (!this.done()) {
        const ch = this.ch();
        if (chars.includes(ch)) {
          this._pos++;
        } else {
          break;
        }
      }
    };

    // lead
    if (this.ch() === 45) {
      // '-'
      negative = true;
      this._pos++;
    }
    if (this.ch() === 48) {
      // '0'
      this._pos++;
      if (this.ch() === 46) {
        // '.'
        this._pos++;
        float = true;
      } else {
        return new Token(Type.uint, 0, this._pos - startPos);
      }
    }
    swallow([48, 49, 50, 51, 52, 53, 54, 55, 56, 57]); // DIGIT
    if (negative && this._pos === startPos + 1) {
      throw new Error(
        `${decodeErrPrefix} unexpected token at position ${this._pos}`
      );
    }
    if (!this.done() && this.ch() === 46) {
      // '.'
      if (float) {
        throw new Error(
          `${decodeErrPrefix} unexpected token at position ${this._pos}`
        );
      }
      float = true;
      this._pos++;
      swallow([48, 49, 50, 51, 52, 53, 54, 55, 56, 57]); // DIGIT
    }
    if (!this.done() && (this.ch() === 101 || this.ch() === 69)) {
      // '[eE]'
      float = true;
      this._pos++;
      if (!this.done() && (this.ch() === 43 || this.ch() === 45)) {
        // '+', '-'
        this._pos++;
      }
      swallow([48, 49, 50, 51, 52, 53, 54, 55, 56, 57]); // DIGIT
    }
    // @ts-ignore
    const numStr = String.fromCharCode.apply(
      null,
      this.data.subarray(startPos, this._pos)
    );
    const num = parseFloat(numStr);
    if (float) {
      return new Token(Type.float, num, this._pos - startPos);
    }
    if (this.options.allowBigInt !== true || Number.isSafeInteger(num)) {
      return new Token(
        num >= 0 ? Type.uint : Type.negint,
        num,
        this._pos - startPos
      );
    }
    return new Token(
      num >= 0 ? Type.uint : Type.negint,
      BigInt(numStr),
      this._pos - startPos
    );
  }

  /**
   * @returns {Token}
   */
  parseString() {
    /* c8 ignore next 4 */
    if (this.ch() !== 34) {
      // '"'
      // this would be a programming error
      throw new Error(
        `${decodeErrPrefix} unexpected character at position ${this._pos}; this shouldn't happen`
      );
    }
    this._pos++;

    // check for simple fast-path, all printable ascii, no escapes
    // >0x10000 elements may fail fn.apply() (http://stackoverflow.com/a/22747272/680742)
    for (
      let i = this._pos, l = 0;
      i < this.data.length && l < 0x10000;
      i++, l++
    ) {
      const ch = this.data[i];
      if (ch === 92 || ch < 32 || ch >= 128) {
        // '\', ' ', control-chars or non-trivial
        break;
      }
      if (ch === 34) {
        // '"'
        // @ts-ignore
        const str = String.fromCharCode.apply(
          null,
          this.data.subarray(this._pos, i)
        );
        this._pos = i + 1;
        return new Token(Type.string, str, l);
      }
    }

    const startPos = this._pos;
    const chars = [];

    const readu4 = () => {
      if (this._pos + 4 >= this.data.length) {
        throw new Error(
          `${decodeErrPrefix} unexpected end of unicode escape sequence at position ${this._pos}`
        );
      }
      let u4 = 0;
      for (let i = 0; i < 4; i++) {
        let ch = this.ch();
        if (ch >= 48 && ch <= 57) {
          // '0' && '9'
          ch -= 48;
        } else if (ch >= 97 && ch <= 102) {
          // 'a' && 'f'
          ch = ch - 97 + 10;
        } else if (ch >= 65 && ch <= 70) {
          // 'A' && 'F'
          ch = ch - 65 + 10;
        } else {
          throw new Error(
            `${decodeErrPrefix} unexpected unicode escape character at position ${this._pos}`
          );
        }
        u4 = u4 * 16 + ch;
        this._pos++;
      }
      return u4;
    };

    // mostly taken from feross/buffer and adjusted to fit
    const readUtf8Char = () => {
      const firstByte = this.ch();
      let codePoint = null;
      /* c8 ignore next 1 */
      let bytesPerSequence =
        firstByte > 0xef ? 4 : firstByte > 0xdf ? 3 : firstByte > 0xbf ? 2 : 1;

      if (this._pos + bytesPerSequence > this.data.length) {
        throw new Error(
          `${decodeErrPrefix} unexpected unicode sequence at position ${this._pos}`
        );
      }

      let secondByte, thirdByte, fourthByte, tempCodePoint;

      switch (bytesPerSequence) {
        /* c8 ignore next 6 */
        // this case is dealt with by the caller function
        case 1:
          if (firstByte < 0x80) {
            codePoint = firstByte;
          }
          break;
        case 2:
          secondByte = this.data[this._pos + 1];
          if ((secondByte & 0xc0) === 0x80) {
            tempCodePoint = ((firstByte & 0x1f) << 0x6) | (secondByte & 0x3f);
            if (tempCodePoint > 0x7f) {
              codePoint = tempCodePoint;
            }
          }
          break;
        case 3:
          secondByte = this.data[this._pos + 1];
          thirdByte = this.data[this._pos + 2];
          if ((secondByte & 0xc0) === 0x80 && (thirdByte & 0xc0) === 0x80) {
            tempCodePoint =
              ((firstByte & 0xf) << 0xc) |
              ((secondByte & 0x3f) << 0x6) |
              (thirdByte & 0x3f);
            /* c8 ignore next 3 */
            if (
              tempCodePoint > 0x7ff &&
              (tempCodePoint < 0xd800 || tempCodePoint > 0xdfff)
            ) {
              codePoint = tempCodePoint;
            }
          }
          break;
        case 4:
          secondByte = this.data[this._pos + 1];
          thirdByte = this.data[this._pos + 2];
          fourthByte = this.data[this._pos + 3];
          if (
            (secondByte & 0xc0) === 0x80 &&
            (thirdByte & 0xc0) === 0x80 &&
            (fourthByte & 0xc0) === 0x80
          ) {
            tempCodePoint =
              ((firstByte & 0xf) << 0x12) |
              ((secondByte & 0x3f) << 0xc) |
              ((thirdByte & 0x3f) << 0x6) |
              (fourthByte & 0x3f);
            if (tempCodePoint > 0xffff && tempCodePoint < 0x110000) {
              codePoint = tempCodePoint;
            }
          }
      }

      /* c8 ignore next 5 */
      if (codePoint === null) {
        // we did not generate a valid codePoint so insert a
        // replacement char (U+FFFD) and advance only 1 byte
        codePoint = 0xfffd;
        bytesPerSequence = 1;
      } else if (codePoint > 0xffff) {
        // encode to utf16 (surrogate pair dance)
        codePoint -= 0x10000;
        chars.push(((codePoint >>> 10) & 0x3ff) | 0xd800);
        codePoint = 0xdc00 | (codePoint & 0x3ff);
      }

      chars.push(codePoint);
      this._pos += bytesPerSequence;
    };

    // TODO: could take the approach of a quick first scan for special chars like encoding/json/decode.go#unquoteBytes
    // and converting all of the ascii chars from the base array in bulk
    while (!this.done()) {
      const ch = this.ch();
      let ch1;
      switch (ch) {
        case 92: // '\'
          this._pos++;
          if (this.done()) {
            throw new Error(
              `${decodeErrPrefix} unexpected string termination at position ${this._pos}`
            );
          }
          ch1 = this.ch();
          this._pos++;
          switch (ch1) {
            case 34: // '"'
            case 39: // '\''
            case 92: // '\'
            case 47: // '/'
              chars.push(ch1);
              break;
            case 98: // 'b'
              chars.push(8);
              break;
            case 116: // 't'
              chars.push(9);
              break;
            case 110: // 'n'
              chars.push(10);
              break;
            case 102: // 'f'
              chars.push(12);
              break;
            case 114: // 'r'
              chars.push(13);
              break;
            case 117: // 'u'
              chars.push(readu4());
              break;
            default:
              throw new Error(
                `${decodeErrPrefix} unexpected string escape character at position ${this._pos}`
              );
          }
          break;
        case 34: // '"'
          this._pos++;
          return new Token(
            Type.string,
            decodeCodePointsArray(chars),
            this._pos - startPos
          );
        default:
          if (ch < 32) {
            // ' '
            throw new Error(
              `${decodeErrPrefix} invalid control character at position ${this._pos}`
            );
          } else if (ch < 0x80) {
            chars.push(ch);
            this._pos++;
          } else {
            readUtf8Char();
          }
      }
    }

    throw new Error(
      `${decodeErrPrefix} unexpected end of string at position ${this._pos}`
    );
  }

  /**
   * @returns {Token}
   */
  parseValue() {
    switch (this.ch()) {
      case 123: // '{'
        this.modeStack.push("obj-start");
        this._pos++;
        return new Token(Type.map, Infinity, 1);
      case 91: // '['
        this.modeStack.push("array-start");
        this._pos++;
        return new Token(Type.array, Infinity, 1);
      case 34: {
        // '"'
        return this.parseString();
      }
      case 110: // 'n' / null
        this.expect([110, 117, 108, 108]); // 'null'
        return new Token(Type.null, null, 4);
      case 102: // 'f' / // false
        this.expect([102, 97, 108, 115, 101]); // 'false'
        return new Token(Type.false, false, 5);
      case 116: // 't' / // true
        this.expect([116, 114, 117, 101]); // 'true'
        return new Token(Type.true, true, 4);
      case 45: // '-'
      case 48: // '0'
      case 49: // '1'
      case 50: // '2'
      case 51: // '3'
      case 52: // '4'
      case 53: // '5'
      case 54: // '6'
      case 55: // '7'
      case 56: // '8'
      case 57: // '9'
        return this.parseNumber();
      default:
        throw new Error(
          `${decodeErrPrefix} unexpected character at position ${this._pos}`
        );
    }
  }

  /**
   * @returns {Token}
   */
  next() {
    this.skipWhitespace();
    switch (this.currentMode()) {
      case "value":
        this.modeStack.pop();
        return this.parseValue();
      case "array-value": {
        this.modeStack.pop();
        if (this.ch() === 93) {
          // ']'
          this._pos++;
          this.skipWhitespace();
          return new Token(Type.break, undefined, 1);
        }
        if (this.ch() !== 44) {
          // ','
          throw new Error(
            `${decodeErrPrefix} unexpected character at position ${
              this._pos
            }, was expecting array delimiter but found '${String.fromCharCode(
              this.ch()
            )}'`
          );
        }
        this._pos++;
        this.modeStack.push("array-value");
        this.skipWhitespace();
        return this.parseValue();
      }
      case "array-start": {
        this.modeStack.pop();
        if (this.ch() === 93) {
          // ']'
          this._pos++;
          this.skipWhitespace();
          return new Token(Type.break, undefined, 1);
        }
        this.modeStack.push("array-value");
        this.skipWhitespace();
        return this.parseValue();
      }
      // @ts-ignore
      case "obj-key":
        if (this.ch() === 125) {
          // '}'
          this.modeStack.pop();
          this._pos++;
          this.skipWhitespace();
          return new Token(Type.break, undefined, 1);
        }
        if (this.ch() !== 44) {
          // ','
          throw new Error(
            `${decodeErrPrefix} unexpected character at position ${
              this._pos
            }, was expecting object delimiter but found '${String.fromCharCode(
              this.ch()
            )}'`
          );
        }
        this._pos++;
        this.skipWhitespace();
      case "obj-start": {
        // eslint-disable-line no-fallthrough
        this.modeStack.pop();
        if (this.ch() === 125) {
          // '}'
          this._pos++;
          this.skipWhitespace();
          return new Token(Type.break, undefined, 1);
        }
        const token = this.parseString();
        this.skipWhitespace();
        if (this.ch() !== 58) {
          // ':'
          throw new Error(
            `${decodeErrPrefix} unexpected character at position ${
              this._pos
            }, was expecting key/value delimiter ':' but found '${String.fromCharCode(
              this.ch()
            )}'`
          );
        }
        this._pos++;
        this.modeStack.push("obj-value");
        return token;
      }
      case "obj-value": {
        this.modeStack.pop();
        this.modeStack.push("obj-key");
        this.skipWhitespace();
        return this.parseValue();
      }
      /* c8 ignore next 2 */
      default:
        throw new Error(
          `${decodeErrPrefix} unexpected parse state at position ${this._pos}; this shouldn't happen`
        );
    }
  }
}

/**
 * @param {Uint8Array} data
 * @param {DecodeOptions} [options]
 * @returns {any}
 */
function decode$u(data, options) {
  options = Object.assign({ tokenizer: new Tokenizer(data, options) }, options);
  return decode$D(data, options);
}

/* eslint max-depth: ["error", 7] */

/**
 * @template T
 * @typedef {import('multiformats/codecs/interface').ByteView<T>} ByteView
 */
/**
 * @template T
 * @typedef {import('multiformats/codecs/interface').ArrayBufferView<T>} ArrayBufferView
 */
/**
 * @template T
 * @typedef {import('multiformats').ToString<T>} ToString
 */
/**
 * @typedef {import('cborg/interface').DecodeTokenizer} DecodeTokenizer
 */

/**
 * @template T
 * @param {ByteView<T> | ArrayBufferView<T>} buf
 * @returns {ByteView<T>}
 */
function toByteView$1(buf) {
  if (buf instanceof ArrayBuffer) {
    return new Uint8Array(buf, 0, buf.byteLength);
  }

  return buf;
}

/**
 * cidEncoder will receive all Objects during encode, it needs to filter out
 * anything that's not a CID and return `null` for that so it's encoded as
 * normal. Encoding a CID means replacing it with a `{"/":"<CidString>}`
 * object as per the DAG-JSON spec.
 *
 * @param {any} obj
 * @returns {Token[]|null}
 */
function cidEncoder(obj) {
  if (obj.asCID !== obj && obj["/"] !== obj.bytes) {
    return null; // any other kind of object
  }
  const cid = CID$2.asCID(obj);
  /* c8 ignore next 4 */
  // very unlikely case, and it'll probably throw a recursion error in cborg
  if (!cid) {
    return null;
  }
  const cidString = cid.toString();

  return [
    new Token(Type.map, Infinity, 1),
    new Token(Type.string, "/", 1), // key
    new Token(Type.string, cidString, cidString.length), // value
    new Token(Type.break, undefined, 1),
  ];
}

/**
 * bytesEncoder will receive all Uint8Arrays (and friends) during encode, it
 * needs to replace it with a `{"/":{"bytes":"Base64ByteString"}}` object as
 * per the DAG-JSON spec.
 *
 * @param {Uint8Array} bytes
 * @returns {Token[]|null}
 */
function bytesEncoder(bytes) {
  const bytesString = base64$1.encode(bytes).slice(1); // no mbase prefix
  return [
    new Token(Type.map, Infinity, 1),
    new Token(Type.string, "/", 1), // key
    new Token(Type.map, Infinity, 1), // value
    new Token(Type.string, "bytes", 5), // inner key
    new Token(Type.string, bytesString, bytesString.length), // inner value
    new Token(Type.break, undefined, 1),
    new Token(Type.break, undefined, 1),
  ];
}

/**
 * taBytesEncoder wraps bytesEncoder() but for the more exotic typed arrays so
 * that we access the underlying ArrayBuffer data
 *
 * @param {Int8Array|Uint16Array|Int16Array|Uint32Array|Int32Array|Float32Array|Float64Array|Uint8ClampedArray|BigInt64Array|BigUint64Array} obj
 * @returns {Token[]|null}
 */
function taBytesEncoder(obj) {
  return bytesEncoder(
    new Uint8Array(obj.buffer, obj.byteOffset, obj.byteLength)
  );
}

/**
 * abBytesEncoder wraps bytesEncoder() but for plain ArrayBuffers
 *
 * @param {ArrayBuffer} ab
 * @returns {Token[]|null}
 */
function abBytesEncoder(ab) {
  return bytesEncoder(new Uint8Array(ab));
}

// eslint-disable-next-line jsdoc/require-returns-check
/**
 * Intercept all `undefined` values from an object walk and reject the entire
 * object if we find one.
 *
 * @returns {null}
 */
function undefinedEncoder() {
  throw new Error(
    "`undefined` is not supported by the IPLD Data Model and cannot be encoded"
  );
}

/**
 * Intercept all `number` values from an object walk and reject the entire
 * object if we find something that doesn't fit the IPLD data model (NaN &
 * Infinity).
 *
 * @param {number} num
 * @returns {null}
 */
function numberEncoder(num) {
  if (Number.isNaN(num)) {
    throw new Error(
      "`NaN` is not supported by the IPLD Data Model and cannot be encoded"
    );
  }
  if (num === Infinity || num === -Infinity) {
    throw new Error(
      "`Infinity` and `-Infinity` is not supported by the IPLD Data Model and cannot be encoded"
    );
  }
  return null; // process with standard number encoder
}

const encodeOptions = {
  typeEncoders: {
    Object: cidEncoder,
    Buffer: bytesEncoder,
    Uint8Array: bytesEncoder,
    Int8Array: taBytesEncoder,
    Uint16Array: taBytesEncoder,
    Int16Array: taBytesEncoder,
    Uint32Array: taBytesEncoder,
    Int32Array: taBytesEncoder,
    Float32Array: taBytesEncoder,
    Float64Array: taBytesEncoder,
    Uint8ClampedArray: taBytesEncoder,
    BigInt64Array: taBytesEncoder,
    BigUint64Array: taBytesEncoder,
    DataView: taBytesEncoder,
    ArrayBuffer: abBytesEncoder,
    undefined: undefinedEncoder,
    number: numberEncoder,
  },
};

/**
 * @implements {DecodeTokenizer}
 */
class DagJsonTokenizer extends Tokenizer {
  /**
   * @param {Uint8Array} data
   * @param {object} [options]
   */
  constructor(data, options) {
    super(data, options);
    /** @type {Token[]} */
    this.tokenBuffer = [];
  }

  /**
   * @returns {boolean}
   */
  done() {
    return this.tokenBuffer.length === 0 && super.done();
  }

  /**
   * @returns {Token}
   */
  _next() {
    if (this.tokenBuffer.length > 0) {
      // @ts-ignore https://github.com/Microsoft/TypeScript/issues/30406
      return this.tokenBuffer.pop();
    }
    return super.next();
  }

  /**
   * Implements rules outlined in https://github.com/ipld/specs/pull/356
   *
   * @returns {Token}
   */
  next() {
    const token = this._next();

    if (token.type === Type.map) {
      const keyToken = this._next();
      if (keyToken.type === Type.string && keyToken.value === "/") {
        const valueToken = this._next();
        if (valueToken.type === Type.string) {
          // *must* be a CID
          const breakToken = this._next(); // swallow the end-of-map token
          if (breakToken.type !== Type.break) {
            throw new Error("Invalid encoded CID form");
          }
          this.tokenBuffer.push(valueToken); // CID.parse will pick this up after our tag token
          return new Token(Type.tag, 42, 0);
        }
        if (valueToken.type === Type.map) {
          const innerKeyToken = this._next();
          if (
            innerKeyToken.type === Type.string &&
            innerKeyToken.value === "bytes"
          ) {
            const innerValueToken = this._next();
            if (innerValueToken.type === Type.string) {
              // *must* be Bytes
              for (let i = 0; i < 2; i++) {
                const breakToken = this._next(); // swallow two end-of-map tokens
                if (breakToken.type !== Type.break) {
                  throw new Error("Invalid encoded Bytes form");
                }
              }
              const bytes = base64$1.decode(`m${innerValueToken.value}`);
              return new Token(Type.bytes, bytes, innerValueToken.value.length);
            }
            this.tokenBuffer.push(innerValueToken); // bail
          }
          this.tokenBuffer.push(innerKeyToken); // bail
        }
        this.tokenBuffer.push(valueToken); // bail
      }
      this.tokenBuffer.push(keyToken); // bail
    }
    return token;
  }
}

const decodeOptions = {
  allowIndefinite: false,
  allowUndefined: false,
  allowNaN: false,
  allowInfinity: false,
  allowBigInt: true, // this will lead to BigInt for ints outside of
  // safe-integer range, which may surprise users
  strict: true,
  useMaps: false,
  rejectDuplicateMapKeys: true,
  /** @type {import('cborg').TagDecoder[]} */
  tags: [],
};

// we're going to get TAG(42)STRING("bafy...") from the tokenizer so we only need
// to deal with the STRING("bafy...") at this point
decodeOptions.tags[42] = CID$2.parse;

/**
 * @template T
 * @param {T} node
 * @returns {ByteView<T>}
 */
const encode$l = (node) => encode$m(node, encodeOptions);

/**
 * @template T
 * @param {ByteView<T> | ArrayBufferView<T>} data
 * @returns {T}
 */
const decode$t = (data) => {
  const buf = toByteView$1(data);
  // the tokenizer is stateful so we need a single instance of it
  const options = Object.assign(decodeOptions, {
    tokenizer: new DagJsonTokenizer(buf, decodeOptions),
  });
  return decode$u(buf, options);
};
new TextDecoder();
new TextEncoder();

/**
 * @template {UCAN.Capabilities} C
 * @param {UCAN.Model<C>} model
 * @returns {UCAN.JWT<C>}
 */
const format$4 = (model) => {
  const header = formatHeader(model.v, model.s.algorithm);
  const payload = formatPayload(model);
  const signature = formatSignature(model.s);
  return /** @type {UCAN.JWT<C>} */ (`${header}.${payload}.${signature}`);
};

/**
 * @template {UCAN.Capabilities} C
 * @param {UCAN.Payload<C>} payload
 * @param {UCAN.Version} version
 * @param {string} alg
 */
const formatSignPayload = (payload, version, alg) =>
  `${formatHeader(version, alg)}.${formatPayload(payload)}`;

/**
 * @param {UCAN.Version} version
 * @param {string} alg
 */
const formatHeader = (version, alg) =>
  base64url.baseEncode(encodeHeader$1(version, alg));

/**
 * @template {UCAN.Capabilities} C
 * @param {UCAN.Payload<C>} data
 */
const formatPayload = (data) => base64url.baseEncode(encodePayload(data));

/**
 * @param {UCAN.Signature<string>} signature
 */
const formatSignature = (signature) => base64url.baseEncode(signature.raw);

/**
 * @param {UCAN.Version} v
 * @param {string} alg
 * @returns {UCAN.ByteView<UCAN.JWTHeader>}
 */
const encodeHeader$1 = (v, alg) =>
  encode$l({
    alg,
    ucv: v,
    typ: "JWT",
  });

/**
 * @template {UCAN.Capabilities} C
 * @param {UCAN.Payload<C>} data
 * @returns {UCAN.ByteView<UCAN.JWTPayload<C>>}
 */
const encodePayload = (data) =>
  encode$l({
    iss: format$5(data.iss),
    aud: format$5(data.aud),
    att: data.att,
    exp: data.exp,
    prf: data.prf.map(encodeProof),
    // leave out optionals and empty fields
    ...(data.fct.length > 0 && { fct: data.fct }),
    ...(data.nnc && { nnc: data.nnc }),
    ...(data.nbf && { nbf: data.nbf }),
  });

/**
 * @param {UCAN.Link} proof
 * @returns {UCAN.ToString<UCAN.Link>}
 */
const encodeProof = (proof) =>
  /** @type {UCAN.ToString<UCAN.Link>} */ (proof.toString());

/**
 * @param {unknown} data
 */
const toJSON = (data) => JSON.parse(decode$x(encode$l(data)));

/**
 * @template {UCAN.Capabilities} C
 */
let View$1 = class View {
  /**
   * @param {UCAN.UCAN<C>} model
   */
  constructor(model) {
    /** @readonly */
    this.model = model;
  }

  get version() {
    return this.model.v;
  }

  get issuer() {
    return from$e(this.model.iss);
  }

  get audience() {
    return from$e(this.model.aud);
  }

  /**
   * @returns {C}
   */
  get capabilities() {
    return this.model.att;
  }

  /**
   * @returns {number}
   */
  get expiration() {
    const { exp } = this.model;
    return exp === null ? Infinity : exp;
  }

  /**
   * @returns {undefined|number}
   */
  get notBefore() {
    return this.model.nbf;
  }

  /**
   * @returns {undefined|string}
   */

  get nonce() {
    return this.model.nnc;
  }

  /**
   * @returns {UCAN.Fact[]}
   */
  get facts() {
    return this.model.fct;
  }

  /**
   * @returns {UCAN.Link[]}
   */

  get proofs() {
    return this.model.prf;
  }

  get signature() {
    return this.model.s;
  }

  // compatibility with UCAN.UCAN
  get jwt() {
    return this.model.jwt;
  }
  get s() {
    return this.model.s;
  }
  get v() {
    return this.model.v;
  }
  get iss() {
    return this.model.iss;
  }
  get aud() {
    return this.model.aud;
  }
  get att() {
    return this.model.att;
  }
  get exp() {
    return this.model.exp;
  }
  get nbf() {
    return this.model.nbf;
  }
  get nnc() {
    return this.model.nnc;
  }
  get fct() {
    return this.model.fct;
  }
  get prf() {
    return this.model.prf;
  }

  /**
   * @returns {UCAN.ToJSON<UCAN.UCAN<C>, UCAN.UCANJSON<this>>}
   */
  toJSON() {
    const { v, iss, aud, s, att, prf, exp, fct, nnc, nbf } = this.model;

    return {
      iss,
      aud,
      v,
      s,
      exp,
      ...toJSON({
        att,
        prf,
        ...(fct.length > 0 && { fct }),
      }),
      ...(nnc != null && { nnc }),
      ...(nbf && { nbf }),
    };
  }
};

const code$d = code$g;

/**
 * Creates a UCAN view from the underlying data model. Please note that this
 * function does no verification of the model and it is callers responsibility
 * to ensure that:
 *
 * 1. Data model is correct contains all the field etc...
 * 2. Payload of the signature will match paylodad when model is serialized
 *    with DAG-JSON.
 *
 * In other words you should never use this function unless you've parsed or
 * decoded a valid UCAN and want to wrap it into a view.
 *
 * @template {UCAN.Capabilities} C
 * @param {UCAN.FromModel<C>} model
 * @returns {UCAN.View<C>}
 */
const from$d = (model) => new CBORView(model);

/**
 * Encodes given UCAN (in either IPLD or JWT representation) and encodes it into
 * corresponding bytes representation. UCAN in IPLD representation is encoded as
 * DAG-CBOR which JWT representation is encoded as raw bytes of JWT string.
 *
 * @template {UCAN.Capabilities} C
 * @param {UCAN.Model<C>} model
 * @returns {UCAN.ByteView<UCAN.Model<C>>}
 */
const encode$k = (model) => {
  const { fct, nnc, nbf, ...payload } = readPayload(model);

  return /** @type {Uint8Array} */ (
    encode$q({
      // leave out optionals unless they are set
      ...(fct.length > 0 && { fct }),
      ...(nnc != null && { nnc }),
      ...(nbf && { nbf }),
      ...payload,
      // add version and signature
      v: readVersion(model.v, "v"),
      s: encodeSignature(model.s, "s"),
    })
  );
};

/**
 * @param {UCAN.Signature} signature
 * @param {string} context
 */
const encodeSignature = (signature, context) => {
  try {
    return encode$n(signature);
  } catch (cause) {
    throw new Error(
      `Expected signature ${context}, instead got ${JSON.stringify(signature)}`,
      // @ts-expect-error - types don't know about second arg
      { cause }
    );
  }
};

/**
 * Decodes UCAN in primary CBOR representation. It does not validate UCAN, it's
 * signature or proof chain. This is to say decoded UCAN may be invalid.
 *
 * @template {UCAN.Capabilities} C
 * @param {UCAN.ByteView<UCAN.Model<C>>} bytes
 * @returns {UCAN.View<C>}
 */
const decode$s = (bytes) => {
  const model = decode$y(bytes);
  return new CBORView({
    ...readPayload(model),
    v: readVersion(model.v, "v"),
    s: readSignature(model.s),
  });
};

/**
 * @template {UCAN.Capabilities} C
 * @extends {View<C>}
 */
class CBORView extends View$1 {
  /** @type {UCAN.MulticodecCode<typeof code, "CBOR">} */
  get code() {
    return code$d;
  }
  format() {
    return format$4(this.model);
  }
  encode() {
    return encode$k(this.model);
  }
}

/**
 * Parse JWT formatted UCAN. Note than no validation takes place here.
 *
 * @template {UCAN.Capabilities} C
 * @param {UCAN.JWT<C>|string} jwt
 * @returns {UCAN.Model<C>}
 */
const parse = (jwt) => {
  const segments = jwt.split(".");
  const [header, payload, signature] =
    segments.length === 3
      ? segments
      : fail$1(
          `Can't parse UCAN: ${jwt}: Expected JWT format: 3 dot-separated base64url-encoded values.`
        );

  const { ucv, alg } = parseHeader(header);

  return {
    ...parsePayload(payload),
    v: ucv,
    s: createNamed(alg, base64url.baseDecode(signature)),
  };
};

/**
 * @param {string} header
 */
const parseHeader = (header) => {
  const { ucv, alg, typ } = decode$t(base64url.baseDecode(header));

  return {
    typ: readLiteral(typ, "JWT", "typ"),
    ucv: readVersion(ucv, "ucv"),
    alg: readString(alg, "alg"),
  };
};

/**
 * @template {UCAN.Capabilities} C
 * @param {string} source
 * @returns {UCAN.Payload<C>}
 */
const parsePayload = (source) => {
  /** @type {Record<string, unknown>} */
  const payload = decode$t(base64url.baseDecode(source));
  return readJWTPayload(payload);
};

/**
 * @template {UCAN.Capabilities} C
 * @param {UCAN.ByteView<UCAN.FromJWT<C>>} bytes
 * @returns {UCAN.View<C>}
 */
const decode$r = (bytes) => {
  const jwt = /** @type {UCAN.JWT<C>} */ (decode$x(bytes));

  return new JWTView({ ...parse(jwt), jwt });
};

/**
 * @template {UCAN.Capabilities} C
 * @param {UCAN.FromJWT<C>} model
 * @returns {UCAN.ByteView<UCAN.UCAN<C>>}
 */
const encode$j = ({ jwt }) => encode$p(jwt);

/**
 * @template {UCAN.Capabilities} C
 * @param {UCAN.FromJWT<C>} model
 * @returns {UCAN.JWT<C>}
 */
const format$3 = ({ jwt }) => jwt;

/**
 * @template {UCAN.Capabilities} C
 * @extends {View<C>}
 */
class JWTView extends View$1 {
  /**
   * @param {UCAN.FromJWT<C>} model
   */
  constructor(model) {
    super(model);
    this.model = model;
  }
  /** @type {UCAN.MulticodecCode<typeof code, "Raw">} */
  get code() {
    return code$e;
  }
  format() {
    return format$3(this.model);
  }
  encode() {
    return encode$j(this.model);
  }
}

/* global crypto */
function sha$3(name) {
  return async (data) => new Uint8Array(await crypto.subtle.digest(name, data));
}
const sha256$4 = from$f({
  name: "sha2-256",
  code: 0x12,
  encode: sha$3("SHA-256"),
});

const VERSION = "0.9.1";

/**
 * We cast sha256 to workaround typescripts limited inference problem when using
 * sha256 as default. If hasher is omitted type `A` should match sha256.code
 * but TS fails to deduce that.
 * @type {UCAN.MultihashHasher<any>}
 */
const defaultHasher = sha256$4;

/**
 * Decodes binary encoded UCAN. It assumes UCAN is in primary IPLD
 * representation and attempts to decode it with DAG-CBOR, if that
 * fails it falls back to secondary representation and parses it as
 * a JWT.
 *
 * @template {UCAN.Capabilities} C
 * @param {UCAN.ByteView<UCAN.UCAN<C>>} bytes
 * @returns {UCAN.View<C>}
 */
const decode$q = (bytes) => {
  try {
    return decode$s(bytes);
  } catch (_) {
    return decode$r(/** @type {UCAN.ByteView<UCAN.FromJWT<C>>} */ (bytes));
  }
};

/**
 * @template {UCAN.Capabilities} C
 * @template {number} [A=typeof sha256.code] - Multihash code
 * @param {UCAN.UCAN<C>} ucan
 * @param {{hasher?: UCAN.MultihashHasher<A>}} options
 * @returns {Promise<UCAN.Block<C, UCAN.Code, A>>}
 */
const write$6 = async (ucan, { hasher = defaultHasher } = {}) => {
  const [code, bytes] = ucan.jwt
    ? [/** @type {UCAN.Code} */ (code$e), encode$j(ucan)]
    : [/** @type {UCAN.Code} */ (code$d), encode$k(ucan)];
  const digest = await hasher.digest(bytes);

  return {
    bytes,
    cid: create$g(code, digest),
    data: ucan,
  };
};

/**
 * Creates a new signed token with a given `options.issuer`. If expiration is
 * not set it defaults to 30 seconds from now. Returns UCAN in primary - IPLD
 * representation.
 *
 * @template {number} A
 * @template {UCAN.Capabilities} C
 * @param {UCAN.UCANOptions<C, A>} options
 * @returns {Promise<UCAN.View<C>>}
 */
const issue$2 = async ({
  issuer,
  audience,
  capabilities,
  lifetimeInSeconds = 30,
  expiration = now() + lifetimeInSeconds,
  notBefore,
  facts = [],
  proofs = [],
  nonce,
}) => {
  const v = VERSION;
  const data = readPayload({
    iss: parse$1(issuer.did()),
    aud: parse$1(audience.did()),
    att: capabilities,
    fct: facts,
    exp: expiration,
    nbf: notBefore,
    prf: proofs,
    nnc: nonce,
  });
  const payload = encodeSignaturePayload(data, v, issuer.signatureAlgorithm);

  return from$d({
    ...data,
    v,
    s: await issuer.sign(payload),
  });
};

/**
 *
 * @param {UCAN.Payload} payload
 * @param {UCAN.Version} version
 * @param {string} algorithm
 * @returns
 */
const encodeSignaturePayload = (payload, version, algorithm) =>
  encode$p(formatSignPayload(payload, version, algorithm));

/**
 * Returns UTC Unix timestamp for comparing it against time window of the UCAN.
 */
const now = () => Math.floor(Date.now() / 1000);

// @see https://www.iana.org/assignments/media-types/application/vnd.ipld.dag-cbor
const contentType$7 = "application/vnd.ipld.dag-cbor";

/**
 * @param {unknown} data
 * @param {Set<unknown>} seen
 * @returns {unknown}
 */
const prepare$1 = (data, seen) => {
  if (seen.has(data)) {
    throw new TypeError("Can not encode circular structure");
  }
  // top level undefined is ok
  if (data === undefined && seen.size === 0) {
    return null;
  }

  if (data === null) {
    return null;
  }

  if (typeof data === "symbol" && seen.size === 0) {
    return null;
  }

  if (isLink(data)) {
    return data;
  }

  if (ArrayBuffer.isView(data)) {
    return data;
  }

  if (Array.isArray(data)) {
    seen.add(data);
    const items = [];
    for (const item of data) {
      items.push(
        item === undefined || typeof item === "symbol"
          ? null
          : prepare$1(item, seen)
      );
    }
    return items;
  }

  if (typeof (/** @type {{toJSON?:unknown}} */ (data).toJSON) === "function") {
    seen.add(data);
    const json = /** @type {{toJSON():unknown}} */ (data).toJSON();
    return prepare$1(json, seen);
  }

  if (typeof data === "object") {
    seen.add(data);
    /** @type {Record<string, unknown>} */
    const object = {};
    for (const [key, value] of Object.entries(data)) {
      if (value !== undefined && typeof value !== "symbol") {
        object[key] = prepare$1(value, new Set(seen));
      }
    }
    return object;
  }

  return data;
};

/**
 * @template T
 * @param {T} data
 * @returns {CBOR.ByteView<T>}
 */
const encode$i = (data) =>
  /** @type {CBOR.ByteView<T>} */ (encode$q(prepare$1(data, new Set())));

/**
 * @template T
 * @param {API.ByteView<T>} bytes
 * @param {{hasher?: API.MultihashHasher }} options
 * @returns {Promise<API.Link<T, typeof CBOR.code>>}
 *
 */
const link$3 = async (bytes, { hasher = sha256$4 } = {}) => {
  return /** @type {API.Link<T, typeof CBOR.code>} */ (
    create$g(code$g, await hasher.digest(bytes))
  );
};

/**
 * @template T
 * @param {T} data
 * @param {{hasher?: API.MultihashHasher }} [options]
 * @returns {Promise<API.Block<T, typeof CBOR.code>>}
 */
const write$5 = async (data, options) => {
  const bytes = encode$i(data);
  const cid = await link$3(bytes, options);

  return { cid, bytes };
};

var CBOR = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  code: code$g,
  contentType: contentType$7,
  decode: decode$y,
  encode: encode$i,
  link: link$3,
  name: name$9,
  write: write$5,
});

/**
 * Function takes arbitrary value and if it happens to be an `IPLDView`
 * it will iterate over it's blocks. It is just a convenience for traversing
 * arbitrary structures that may contain `IPLDView`s in them.
 * Note if you pass anything other than `IPLDView` it will not attempt
 * to find views nested inside them, instead it will just emit no blocks.
 *
 * @param {unknown} value
 * @returns {IterableIterator<API.Block>}
 */
const iterate$2 = function* (value) {
  if (
    value &&
    typeof value === "object" &&
    "iterateIPLDBlocks" in value &&
    typeof value.iterateIPLDBlocks === "function"
  ) {
    yield* value.iterateIPLDBlocks();
  }
};

/**
 * @template [T=unknown]
 * @typedef {Map<API.ToString<API.Link>, API.Block<T, number, number, 0>|API.Block<T, number, number, 1>>} BlockStore
 */

/**
 * @template [T=unknown]
 * @param {API.Block<T>[]} blocks
 * @returns {API.BlockStore<T>}
 */
const createStore = (blocks = []) => {
  const store = new Map();
  addEveryInto(blocks, store);
  return store;
};

/** @type {API.MulticodecCode<typeof identity.code, typeof identity.name>} */
const EMBED_CODE = identity.code;

/**
 * Gets block corresponding to the given CID from the store. If store does not
 * contain the block, `fallback` is returned. If `fallback` is not provided, it
 * will throw an error.
 *
 * @template {0|1} V
 * @template {T} U
 * @template T
 * @template {API.MulticodecCode} Format
 * @template {API.MulticodecCode} Alg
 * @template [E=never]
 * @param {API.Link<U, Format, Alg, V>} cid
 * @param {BlockStore<T>} store
 * @param {E} [fallback]
 * @returns {API.Block<U, Format, Alg, V>|E}
 */
const get$i = (cid, store, fallback) => {
  // If CID uses identity hash, we can return the block data directly
  if (cid.multihash.code === EMBED_CODE) {
    return { cid, bytes: cid.multihash.digest };
  }

  const block = /** @type {API.Block<U, Format, Alg, V>|undefined} */ (
    store.get(`${cid}`)
  );
  return block ? block : fallback;
};

/**
 * @param {API.Link<*, *, *, *>} link
 * @returns {never}
 */
const notFound = (link) => {
  throw new Error(`Block for the ${link} is not found`);
};

/**
 * @template T
 * @template {T} U
 * @template {API.MulticodecCode} C
 * @template {API.MulticodecCode} A
 * @param {U} source
 * @param {BlockStore<T>} store
 * @param {object} options
 * @param {MF.BlockEncoder<C, unknown>} [options.codec]
 * @param {MF.MultihashHasher<A>} [options.hasher]
 * @returns {Promise<API.Block<U, C, A> & { data: U }>}
 */
const writeInto = async (source, store, options = {}) => {
  const codec = /** @type {MF.BlockEncoder<C, U>} */ (options.codec || CBOR);
  const hasher = /** @type {MF.MultihashHasher<A>} */ (
    options.hasher || sha256$4
  );

  const bytes = codec.encode(source);
  const digest = await hasher.digest(bytes);
  /** @type {API.Link<U, typeof codec.code, typeof hasher.code>} */
  const link = create$g(codec.code, digest);
  store.set(/** @type {API.ToString<typeof link>} */ (link.toString()), {
    bytes,
    cid: link,
  });

  return { bytes, cid: link, data: source };
};

/**
 * @template T
 * @template {T} U
 * @param {API.Block<U>} block
 * @param {BlockStore<T>} store
 * @returns {API.Block<U>}
 */
const addInto = ({ cid, bytes }, store) => {
  store.set(/** @type {API.ToString<typeof cid>} */ (cid.toString()), {
    bytes,
    cid,
  });

  return { bytes, cid };
};

/**
 * @template T
 * @template {T} U
 * @param {Iterable<API.Block<U>>} source
 * @param {BlockStore<T>} store
 */
const addEveryInto = (source, store) => {
  for (const block of source) {
    addInto(block, store);
  }
};

var commonjsGlobal =
  typeof globalThis !== "undefined"
    ? globalThis
    : typeof window !== "undefined"
    ? window
    : typeof global !== "undefined"
    ? global
    : typeof self !== "undefined"
    ? self
    : {};

function getDefaultExportFromCjs(x) {
  return x && x.__esModule && Object.prototype.hasOwnProperty.call(x, "default")
    ? x["default"]
    : x;
}

var encode_1$3;
var hasRequiredEncode;

function requireEncode() {
  if (hasRequiredEncode) return encode_1$3;
  hasRequiredEncode = 1;
  encode_1$3 = encode;

  var MSB = 0x80,
    MSBALL = -128,
    INT = Math.pow(2, 31);

  function encode(num, out, offset) {
    if (Number.MAX_SAFE_INTEGER && num > Number.MAX_SAFE_INTEGER) {
      encode.bytes = 0;
      throw new RangeError("Could not encode varint");
    }
    out = out || [];
    offset = offset || 0;
    var oldOffset = offset;

    while (num >= INT) {
      out[offset++] = (num & 0xff) | MSB;
      num /= 128;
    }
    while (num & MSBALL) {
      out[offset++] = (num & 0xff) | MSB;
      num >>>= 7;
    }
    out[offset] = num | 0;

    encode.bytes = offset - oldOffset + 1;

    return out;
  }
  return encode_1$3;
}

var decode$p;
var hasRequiredDecode;

function requireDecode() {
  if (hasRequiredDecode) return decode$p;
  hasRequiredDecode = 1;
  decode$p = read;

  var MSB = 0x80,
    REST = 0x7f;

  function read(buf, offset) {
    var res = 0,
      offset = offset || 0,
      shift = 0,
      counter = offset,
      b,
      l = buf.length;

    do {
      if (counter >= l || shift > 49) {
        read.bytes = 0;
        throw new RangeError("Could not decode varint");
      }
      b = buf[counter++];
      res += shift < 28 ? (b & REST) << shift : (b & REST) * Math.pow(2, shift);
      shift += 7;
    } while (b >= MSB);

    read.bytes = counter - offset;

    return res;
  }
  return decode$p;
}

var length$3;
var hasRequiredLength;

function requireLength() {
  if (hasRequiredLength) return length$3;
  hasRequiredLength = 1;
  var N1 = Math.pow(2, 7);
  var N2 = Math.pow(2, 14);
  var N3 = Math.pow(2, 21);
  var N4 = Math.pow(2, 28);
  var N5 = Math.pow(2, 35);
  var N6 = Math.pow(2, 42);
  var N7 = Math.pow(2, 49);
  var N8 = Math.pow(2, 56);
  var N9 = Math.pow(2, 63);

  length$3 = function (value) {
    return value < N1
      ? 1
      : value < N2
      ? 2
      : value < N3
      ? 3
      : value < N4
      ? 4
      : value < N5
      ? 5
      : value < N6
      ? 6
      : value < N7
      ? 7
      : value < N8
      ? 8
      : value < N9
      ? 9
      : 10;
  };
  return length$3;
}

var varint$4;
var hasRequiredVarint;

function requireVarint() {
  if (hasRequiredVarint) return varint$4;
  hasRequiredVarint = 1;
  varint$4 = {
    encode: requireEncode(),
    decode: requireDecode(),
    encodingLength: requireLength(),
  };
  return varint$4;
}

var varintExports = requireVarint();
var varint$3 = /*@__PURE__*/ getDefaultExportFromCjs(varintExports);

const CIDV0_BYTES$1 = {
  SHA2_256: 0x12,
  LENGTH: 0x20,
  DAG_PB: 0x70,
};

const V2_HEADER_LENGTH =
  /* characteristics */ 16 /* v1 offset */ +
  8 /* v1 size */ +
  8 /* index offset */ +
  8;

/**
 * Decodes varint and seeks the buffer
 *
 * ```js
 * // needs bytes to be read first
 * const bytes = reader.upTo(8) // maybe async
 * ```
 *
 * @param {Uint8Array} bytes
 * @param {import('./coding').Seekable} seeker
 * @returns {number}
 */
function decodeVarint$1(bytes, seeker) {
  if (!bytes.length) {
    throw new Error("Unexpected end of data");
  }
  const i = varint$3.decode(bytes);
  seeker.seek(/** @type {number} */ (varint$3.decode.bytes));
  return i;
}

/**
 * Decode v2 header
 *
 * ```js
 * // needs bytes to be read first
 * const bytes = reader.exactly(V2_HEADER_LENGTH, true) // maybe async
 * ```
 *
 * @param {Uint8Array} bytes
 * @returns {import('./coding').CarV2FixedHeader}
 */
function decodeV2Header(bytes) {
  const dv = new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength);
  let offset = 0;
  const header = {
    version: 2,
    /** @type {[bigint, bigint]} */
    characteristics: [
      dv.getBigUint64(offset, true),
      dv.getBigUint64((offset += 8), true),
    ],
    dataOffset: Number(dv.getBigUint64((offset += 8), true)),
    dataSize: Number(dv.getBigUint64((offset += 8), true)),
    indexOffset: Number(dv.getBigUint64((offset += 8), true)),
  };
  return header;
}

/**
 * Checks the length of the multihash to be read afterwards
 *
 * ```js
 * // needs bytes to be read first
 * const bytes = reader.upTo(8) // maybe async
 * ```
 *
 * @param {Uint8Array} bytes
 */
function getMultihashLength$1(bytes) {
  // | code | length | .... |
  // where both code and length are varints, so we have to decode
  // them first before we can know total length

  varint$3.decode(bytes); // code
  const codeLength = /** @type {number} */ (varint$3.decode.bytes);
  const length = varint$3.decode(bytes.subarray(varint$3.decode.bytes));
  const lengthLength = /** @type {number} */ (varint$3.decode.bytes);
  const mhLength = codeLength + lengthLength + length;

  return mhLength;
}

/** Auto-generated with @ipld/schema@v4.2.0 at Thu Sep 14 2023 from IPLD Schema:
 *
 * # CarV1HeaderOrV2Pragma is a more relaxed form, and can parse {version:x} where
 * # roots are optional. This is typically useful for the {verison:2} CARv2
 * # pragma.
 *
 * type CarV1HeaderOrV2Pragma struct {
 * 	roots optional [&Any]
 * 	# roots is _not_ optional for CarV1 but we defer that check within code to
 * 	# gracefully handle the V2 case where it's just {version:X}
 * 	version Int
 * }
 *
 * # CarV1Header is the strict form of the header, and requires roots to be
 * # present. This is compatible with the CARv1 specification.
 *
 * # type CarV1Header struct {
 * # 	roots [&Any]
 * # 	version Int
 * # }
 *
 */

const Kinds = {
  Null: /** @returns {undefined|null} */ (/** @type {any} */ obj) =>
    obj === null ? obj : undefined,
  Int: /** @returns {undefined|number} */ (/** @type {any} */ obj) =>
    Number.isInteger(obj) ? obj : undefined,
  Float: /** @returns {undefined|number} */ (/** @type {any} */ obj) =>
    typeof obj === "number" && Number.isFinite(obj) ? obj : undefined,
  String: /** @returns {undefined|string} */ (/** @type {any} */ obj) =>
    typeof obj === "string" ? obj : undefined,
  Bool: /** @returns {undefined|boolean} */ (/** @type {any} */ obj) =>
    typeof obj === "boolean" ? obj : undefined,
  Bytes: /** @returns {undefined|Uint8Array} */ (/** @type {any} */ obj) =>
    obj instanceof Uint8Array ? obj : undefined,
  Link: /** @returns {undefined|object} */ (/** @type {any} */ obj) =>
    obj !== null && typeof obj === "object" && obj.asCID === obj
      ? obj
      : undefined,
  List: /** @returns {undefined|Array<any>} */ (/** @type {any} */ obj) =>
    Array.isArray(obj) ? obj : undefined,
  Map: /** @returns {undefined|object} */ (/** @type {any} */ obj) =>
    obj !== null &&
    typeof obj === "object" &&
    obj.asCID !== obj &&
    !Array.isArray(obj) &&
    !(obj instanceof Uint8Array)
      ? obj
      : undefined,
};
/** @type {{ [k in string]: (obj:any)=>undefined|any}} */
const Types = {
  "CarV1HeaderOrV2Pragma > roots (anon) > valueType (anon)": Kinds.Link,
  "CarV1HeaderOrV2Pragma > roots (anon)": /** @returns {undefined|any} */ (
    /** @type {any} */ obj
  ) => {
    if (Kinds.List(obj) === undefined) {
      return undefined;
    }
    for (let i = 0; i < obj.length; i++) {
      let v = obj[i];
      v = Types["CarV1HeaderOrV2Pragma > roots (anon) > valueType (anon)"](v);
      if (v === undefined) {
        return undefined;
      }
      if (v !== obj[i]) {
        const ret = obj.slice(0, i);
        for (let j = i; j < obj.length; j++) {
          let v = obj[j];
          v =
            Types["CarV1HeaderOrV2Pragma > roots (anon) > valueType (anon)"](v);
          if (v === undefined) {
            return undefined;
          }
          ret.push(v);
        }
        return ret;
      }
    }
    return obj;
  },
  Int: Kinds.Int,
  CarV1HeaderOrV2Pragma: /** @returns {undefined|any} */ (
    /** @type {any} */ obj
  ) => {
    if (Kinds.Map(obj) === undefined) {
      return undefined;
    }
    const entries = Object.entries(obj);
    /** @type {{[k in string]: any}} */
    let ret = obj;
    let requiredCount = 1;
    for (let i = 0; i < entries.length; i++) {
      const [key, value] = entries[i];
      switch (key) {
        case "roots":
          {
            const v = Types["CarV1HeaderOrV2Pragma > roots (anon)"](obj[key]);
            if (v === undefined) {
              return undefined;
            }
            if (v !== value || ret !== obj) {
              if (ret === obj) {
                /** @type {{[k in string]: any}} */
                ret = {};
                for (let j = 0; j < i; j++) {
                  ret[entries[j][0]] = entries[j][1];
                }
              }
              ret.roots = v;
            }
          }
          break;
        case "version":
          {
            requiredCount--;
            const v = Types.Int(obj[key]);
            if (v === undefined) {
              return undefined;
            }
            if (v !== value || ret !== obj) {
              if (ret === obj) {
                /** @type {{[k in string]: any}} */
                ret = {};
                for (let j = 0; j < i; j++) {
                  ret[entries[j][0]] = entries[j][1];
                }
              }
              ret.version = v;
            }
          }
          break;
        default:
          return undefined;
      }
    }

    if (requiredCount > 0) {
      return undefined;
    }
    return ret;
  },
};
/** @type {{ [k in string]: (obj:any)=>undefined|any}} */
const Reprs = {
  "CarV1HeaderOrV2Pragma > roots (anon) > valueType (anon)": Kinds.Link,
  "CarV1HeaderOrV2Pragma > roots (anon)": /** @returns {undefined|any} */ (
    /** @type {any} */ obj
  ) => {
    if (Kinds.List(obj) === undefined) {
      return undefined;
    }
    for (let i = 0; i < obj.length; i++) {
      let v = obj[i];
      v = Reprs["CarV1HeaderOrV2Pragma > roots (anon) > valueType (anon)"](v);
      if (v === undefined) {
        return undefined;
      }
      if (v !== obj[i]) {
        const ret = obj.slice(0, i);
        for (let j = i; j < obj.length; j++) {
          let v = obj[j];
          v =
            Reprs["CarV1HeaderOrV2Pragma > roots (anon) > valueType (anon)"](v);
          if (v === undefined) {
            return undefined;
          }
          ret.push(v);
        }
        return ret;
      }
    }
    return obj;
  },
  Int: Kinds.Int,
  CarV1HeaderOrV2Pragma: /** @returns {undefined|any} */ (
    /** @type {any} */ obj
  ) => {
    if (Kinds.Map(obj) === undefined) {
      return undefined;
    }
    const entries = Object.entries(obj);
    /** @type {{[k in string]: any}} */
    let ret = obj;
    let requiredCount = 1;
    for (let i = 0; i < entries.length; i++) {
      const [key, value] = entries[i];
      switch (key) {
        case "roots":
          {
            const v = Reprs["CarV1HeaderOrV2Pragma > roots (anon)"](value);
            if (v === undefined) {
              return undefined;
            }
            if (v !== value || ret !== obj) {
              if (ret === obj) {
                /** @type {{[k in string]: any}} */
                ret = {};
                for (let j = 0; j < i; j++) {
                  ret[entries[j][0]] = entries[j][1];
                }
              }
              ret.roots = v;
            }
          }
          break;
        case "version":
          {
            requiredCount--;
            const v = Reprs.Int(value);
            if (v === undefined) {
              return undefined;
            }
            if (v !== value || ret !== obj) {
              if (ret === obj) {
                /** @type {{[k in string]: any}} */
                ret = {};
                for (let j = 0; j < i; j++) {
                  ret[entries[j][0]] = entries[j][1];
                }
              }
              ret.version = v;
            }
          }
          break;
        default:
          return undefined;
      }
    }
    if (requiredCount > 0) {
      return undefined;
    }
    return ret;
  },
};

const CarV1HeaderOrV2Pragma = {
  toTyped: Types.CarV1HeaderOrV2Pragma,
  toRepresentation: Reprs.CarV1HeaderOrV2Pragma,
};

/**
 * @typedef {import('./api').Block} Block
 * @typedef {import('./api').BlockHeader} BlockHeader
 * @typedef {import('./api').BlockIndex} BlockIndex
 * @typedef {import('./coding').BytesBufferReader} BytesBufferReader
 * @typedef {import('./coding').CarHeader} CarHeader
 * @typedef {import('./coding').CarV2Header} CarV2Header
 * @typedef {import('./coding').CarV2FixedHeader} CarV2FixedHeader
 */

/**
 * Reads header data from a `BytesReader`. The header may either be in the form
 * of a `CarHeader` or `CarV2Header` depending on the CAR being read.
 *
 * @name decoder.readHeader(reader)
 * @param {BytesBufferReader} reader
 * @param {number} [strictVersion]
 * @returns {CarHeader | CarV2Header}
 */
function readHeader$1(reader, strictVersion) {
  const length = decodeVarint$1(reader.upTo(8), reader);
  if (length === 0) {
    throw new Error("Invalid CAR header (zero length)");
  }
  const header = reader.exactly(length, true);
  const block = decode$y(header);
  if (CarV1HeaderOrV2Pragma.toTyped(block) === undefined) {
    throw new Error("Invalid CAR header format");
  }
  if (
    (block.version !== 1 && block.version !== 2) ||
    (strictVersion !== undefined && block.version !== strictVersion)
  ) {
    throw new Error(
      `Invalid CAR version: ${block.version}${
        strictVersion !== undefined ? ` (expected ${strictVersion})` : ""
      }`
    );
  }
  if (block.version === 1) {
    // CarV1HeaderOrV2Pragma makes roots optional, let's make it mandatory
    if (!Array.isArray(block.roots)) {
      throw new Error("Invalid CAR header format");
    }
    return block;
  }
  // version 2
  if (block.roots !== undefined) {
    throw new Error("Invalid CAR header format");
  }
  const v2Header = decodeV2Header(reader.exactly(V2_HEADER_LENGTH, true));
  reader.seek(v2Header.dataOffset - reader.pos);
  const v1Header = readHeader$1(reader, 1);
  return Object.assign(v1Header, v2Header);
}

/**
 * Reads CID sync
 *
 * @param {BytesBufferReader} reader
 * @returns {CID}
 */
function readCid$1(reader) {
  const first = reader.exactly(2, false);
  if (
    first[0] === CIDV0_BYTES$1.SHA2_256 &&
    first[1] === CIDV0_BYTES$1.LENGTH
  ) {
    // cidv0 32-byte sha2-256
    const bytes = reader.exactly(34, true);
    const multihash = decode$z(bytes);
    return CID$2.create(0, CIDV0_BYTES$1.DAG_PB, multihash);
  }

  const version = decodeVarint$1(reader.upTo(8), reader);
  if (version !== 1) {
    throw new Error(`Unexpected CID version (${version})`);
  }
  const codec = decodeVarint$1(reader.upTo(8), reader);
  const bytes = reader.exactly(getMultihashLength$1(reader.upTo(8)), true);
  const multihash = decode$z(bytes);
  return CID$2.create(version, codec, multihash);
}

/**
 * Reads the leading data of an individual block from CAR data from a
 * `BytesBufferReader`. Returns a `BlockHeader` object which contains
 * `{ cid, length, blockLength }` which can be used to either index the block
 * or read the block binary data.
 *
 * @name async decoder.readBlockHead(reader)
 * @param {BytesBufferReader} reader
 * @returns {BlockHeader}
 */
function readBlockHead$1(reader) {
  // length includes a CID + Binary, where CID has a variable length
  // we have to deal with
  const start = reader.pos;
  let length = decodeVarint$1(reader.upTo(8), reader);
  if (length === 0) {
    throw new Error("Invalid CAR section (zero length)");
  }
  length += reader.pos - start;
  const cid = readCid$1(reader);
  const blockLength = length - Number(reader.pos - start); // subtract CID length

  return { cid, length, blockLength };
}

/**
 * Returns Car header and blocks from a Uint8Array
 *
 * @param {Uint8Array} bytes
 * @returns {{ header : CarHeader | CarV2Header , blocks: Block[]}}
 */
function fromBytes$5(bytes) {
  let reader = bytesReader$1(bytes);
  const header = readHeader$1(reader);
  if (header.version === 2) {
    const v1length = reader.pos - header.dataOffset;
    reader = limitReader$1(reader, header.dataSize - v1length);
  }

  const blocks = [];
  while (reader.upTo(8).length > 0) {
    const { cid, blockLength } = readBlockHead$1(reader);

    blocks.push({ cid, bytes: reader.exactly(blockLength, true) });
  }

  return {
    header,
    blocks,
  };
}

/**
 * Creates a `BytesBufferReader` from a `Uint8Array`.
 *
 * @name decoder.bytesReader(bytes)
 * @param {Uint8Array} bytes
 * @returns {BytesBufferReader}
 */
function bytesReader$1(bytes) {
  let pos = 0;

  /** @type {BytesBufferReader} */
  return {
    upTo(length) {
      return bytes.subarray(pos, pos + Math.min(length, bytes.length - pos));
    },

    exactly(length, seek = false) {
      if (length > bytes.length - pos) {
        throw new Error("Unexpected end of data");
      }

      const out = bytes.subarray(pos, pos + length);
      if (seek) {
        pos += length;
      }
      return out;
    },

    seek(length) {
      pos += length;
    },

    get pos() {
      return pos;
    },
  };
}

/**
 * Wraps a `BytesBufferReader` in a limiting `BytesBufferReader` which limits maximum read
 * to `byteLimit` bytes. It _does not_ update `pos` of the original
 * `BytesBufferReader`.
 *
 * @name decoder.limitReader(reader, byteLimit)
 * @param {BytesBufferReader} reader
 * @param {number} byteLimit
 * @returns {BytesBufferReader}
 */
function limitReader$1(reader, byteLimit) {
  let bytesRead = 0;

  /** @type {BytesBufferReader} */
  return {
    upTo(length) {
      let bytes = reader.upTo(length);
      if (bytes.length + bytesRead > byteLimit) {
        bytes = bytes.subarray(0, byteLimit - bytesRead);
      }
      return bytes;
    },

    exactly(length, seek = false) {
      const bytes = reader.exactly(length, seek);
      if (bytes.length + bytesRead > byteLimit) {
        throw new Error("Unexpected end of data");
      }
      if (seek) {
        bytesRead += length;
      }
      return bytes;
    },

    seek(length) {
      bytesRead += length;
      reader.seek(length);
    },

    get pos() {
      return reader.pos;
    },
  };
}

/**
 * @typedef {import('multiformats').CID} CID
 * @typedef {import('./api').Block} Block
 * @typedef {import('./api').CarBufferReader} ICarBufferReader
 * @typedef {import('./coding').CarHeader} CarHeader
 * @typedef {import('./coding').CarV2Header} CarV2Header
 */

/**
 * Provides blockstore-like access to a CAR.
 *
 * Implements the `RootsBufferReader` interface:
 * {@link ICarBufferReader.getRoots `getRoots()`}. And the `BlockBufferReader` interface:
 * {@link ICarBufferReader.get `get()`}, {@link ICarBufferReader.has `has()`},
 * {@link ICarBufferReader.blocks `blocks()`} and
 * {@link ICarBufferReader.cids `cids()`}.
 *
 * Load this class with either `import { CarBufferReader } from '@ipld/car/buffer-reader'`
 * (`const { CarBufferReader } = require('@ipld/car/buffer-reader')`). Or
 * `import { CarBufferReader } from '@ipld/car'` (`const { CarBufferReader } = require('@ipld/car')`).
 * The former will likely result in smaller bundle sizes where this is
 * important.
 *
 * @name CarBufferReader
 * @class
 * @implements {ICarBufferReader}
 * @property {number} version The version number of the CAR referenced by this
 * reader (should be `1` or `2`).
 */
class CarBufferReader {
  /**
   * @constructs CarBufferReader
   * @param {CarHeader|CarV2Header} header
   * @param {Block[]} blocks
   */
  constructor(header, blocks) {
    this._header = header;
    this._blocks = blocks;
    this._cids = undefined;
  }

  /**
   * @property version
   * @memberof CarBufferReader
   * @instance
   */
  get version() {
    return this._header.version;
  }

  /**
   * Get the list of roots defined by the CAR referenced by this reader. May be
   * zero or more `CID`s.
   *
   * @function
   * @memberof CarBufferReader
   * @instance
   * @returns {CID[]}
   */
  getRoots() {
    return this._header.roots;
  }

  /**
   * Check whether a given `CID` exists within the CAR referenced by this
   * reader.
   *
   * @function
   * @memberof CarBufferReader
   * @instance
   * @param {CID} key
   * @returns {boolean}
   */
  has(key) {
    return this._blocks.some((b) => b.cid.equals(key));
  }

  /**
   * Fetch a `Block` (a `{ cid:CID, bytes:Uint8Array }` pair) from the CAR
   * referenced by this reader matching the provided `CID`. In the case where
   * the provided `CID` doesn't exist within the CAR, `undefined` will be
   * returned.
   *
   * @function
   * @memberof CarBufferReader
   * @instance
   * @param {CID} key
   * @returns {Block | undefined}
   */
  get(key) {
    return this._blocks.find((b) => b.cid.equals(key));
  }

  /**
   * Returns a `Block[]` of the `Block`s (`{ cid:CID, bytes:Uint8Array }` pairs) contained within
   * the CAR referenced by this reader.
   *
   * @function
   * @memberof CarBufferReader
   * @instance
   * @returns {Block[]}
   */
  blocks() {
    return this._blocks;
  }

  /**
   * Returns a `CID[]` of the `CID`s contained within the CAR referenced by this reader.
   *
   * @function
   * @memberof CarBufferReader
   * @instance
   * @returns {CID[]}
   */
  cids() {
    if (!this._cids) {
      this._cids = this._blocks.map((b) => b.cid);
    }
    return this._cids;
  }

  /**
   * Instantiate a {@link CarBufferReader} from a `Uint8Array` blob. This performs a
   * decode fully in memory and maintains the decoded state in memory for full
   * access to the data via the `CarReader` API.
   *
   * @static
   * @memberof CarBufferReader
   * @param {Uint8Array} bytes
   * @returns {CarBufferReader}
   */
  static fromBytes(bytes) {
    if (!(bytes instanceof Uint8Array)) {
      throw new TypeError("fromBytes() requires a Uint8Array");
    }

    const { header, blocks } = fromBytes$5(bytes);
    return new CarBufferReader(header, blocks);
  }
}

/**
 * @typedef {import('../interface').EncodeOptions} EncodeOptions
 * @typedef {import('../interface').TokenTypeEncoder} TokenTypeEncoder
 * @typedef {import('../interface').TokenOrNestedTokens} TokenOrNestedTokens
 */

const cborEncoders = makeCborEncoders();

/** @type {EncodeOptions} */
const defaultEncodeOptions = {
  float64: false,
  quickEncodeToken,
};

/**
 * Calculate the byte length of the data as represented by the given tokens when
 * encoded as CBOR with the options provided.
 * This function is for advanced users and would not normally be called
 * directly. See `encodedLength()` for appropriate use.
 *
 * @param {TokenOrNestedTokens} tokens
 * @param {TokenTypeEncoder[]} [encoders]
 * @param {EncodeOptions} [options]
 */
function tokensToLength(
  tokens,
  encoders = cborEncoders,
  options = defaultEncodeOptions
) {
  if (Array.isArray(tokens)) {
    let len = 0;
    for (const token of tokens) {
      len += tokensToLength(token, encoders, options);
    }
    return len;
  } else {
    const encoder = encoders[tokens.type.major];
    /* c8 ignore next 3 */
    if (
      encoder.encodedSize === undefined ||
      typeof encoder.encodedSize !== "function"
    ) {
      throw new Error(
        `Encoder for ${tokens.type.name} does not have an encodedSize()`
      );
    }
    return encoder.encodedSize(tokens, options);
  }
}

/**
 * @typedef {import('./api').CID} CID
 * @typedef {import('./api').Block} Block
 * @typedef {import('./api').CarBufferWriter} Writer
 * @typedef {import('./api').CarBufferWriterOptions} Options
 * @typedef {import('./coding').CarEncoder} CarEncoder
 */

/**
 * A simple CAR writer that writes to a pre-allocated buffer.
 *
 * @class
 * @name CarBufferWriter
 * @implements {Writer}
 */
class CarBufferWriter {
  /**
   * @param {Uint8Array} bytes
   * @param {number} headerSize
   */
  constructor(bytes, headerSize) {
    /** @readonly */
    this.bytes = bytes;
    this.byteOffset = headerSize;

    /**
     * @readonly
     * @type {CID[]}
     */
    this.roots = [];
    this.headerSize = headerSize;
  }

  /**
   * Add a root to this writer, to be used to create a header when the CAR is
   * finalized with {@link CarBufferWriter.close `close()`}
   *
   * @param {CID} root
   * @param {{resize?:boolean}} [options]
   * @returns {CarBufferWriter}
   */
  addRoot(root, options) {
    addRoot(this, root, options);
    return this;
  }

  /**
   * Write a `Block` (a `{ cid:CID, bytes:Uint8Array }` pair) to the archive.
   * Throws if there is not enough capacity.
   *
   * @param {Block} block - A `{ cid:CID, bytes:Uint8Array }` pair.
   * @returns {CarBufferWriter}
   */
  write(block) {
    addBlock(this, block);
    return this;
  }

  /**
   * Finalize the CAR and return it as a `Uint8Array`.
   *
   * @param {object} [options]
   * @param {boolean} [options.resize]
   * @returns {Uint8Array}
   */
  close(options) {
    return close$7(this, options);
  }
}

/**
 * @param {CarBufferWriter} writer
 * @param {CID} root
 * @param {{resize?:boolean}} [options]
 */
const addRoot = (writer, root, options = {}) => {
  const { resize = false } = options;
  const { bytes, headerSize, byteOffset, roots } = writer;
  writer.roots.push(root);
  const size = headerLength(writer);
  // If there is not enough space for the new root
  if (size > headerSize) {
    // Check if we root would fit if we were to resize the head.
    if (size - headerSize + byteOffset < bytes.byteLength) {
      // If resize is enabled resize head
      if (resize) {
        resizeHeader(writer, size);
        // otherwise remove head and throw an error suggesting to resize
      } else {
        roots.pop();
        throw new RangeError(`Header of size ${headerSize} has no capacity for new root ${root}.
  However there is a space in the buffer and you could call addRoot(root, { resize: root }) to resize header to make a space for this root.`);
      }
      // If head would not fit even with resize pop new root and throw error
    } else {
      roots.pop();
      throw new RangeError(`Buffer has no capacity for a new root ${root}`);
    }
  }
};

/**
 * Calculates number of bytes required for storing given block in CAR. Useful in
 * estimating size of an `ArrayBuffer` for the `CarBufferWriter`.
 *
 * @name CarBufferWriter.blockLength(Block)
 * @param {Block} block
 * @returns {number}
 */
const blockLength = ({ cid, bytes }) => {
  const size = cid.bytes.byteLength + bytes.byteLength;
  return varint$3.encodingLength(size) + size;
};

/**
 * @param {CarBufferWriter} writer
 * @param {Block} block
 */
const addBlock = (writer, { cid, bytes }) => {
  const byteLength = cid.bytes.byteLength + bytes.byteLength;
  const size = varint$3.encode(byteLength);
  if (writer.byteOffset + size.length + byteLength > writer.bytes.byteLength) {
    throw new RangeError("Buffer has no capacity for this block");
  } else {
    writeBytes(writer, size);
    writeBytes(writer, cid.bytes);
    writeBytes(writer, bytes);
  }
};

/**
 * @param {CarBufferWriter} writer
 * @param {object} [options]
 * @param {boolean} [options.resize]
 */
const close$7 = (writer, options = {}) => {
  const { resize = false } = options;
  const { roots, bytes, byteOffset, headerSize } = writer;

  const headerBytes = encode$q({ version: 1, roots });
  const varintBytes = varint$3.encode(headerBytes.length);

  const size = varintBytes.length + headerBytes.byteLength;
  const offset = headerSize - size;

  // If header size estimate was accurate we just write header and return
  // view into buffer.
  if (offset === 0) {
    writeHeader(writer, varintBytes, headerBytes);
    return bytes.subarray(0, byteOffset);
    // If header was overestimated and `{resize: true}` is passed resize header
  } else if (resize) {
    resizeHeader(writer, size);
    writeHeader(writer, varintBytes, headerBytes);
    return bytes.subarray(0, writer.byteOffset);
  } else {
    throw new RangeError(`Header size was overestimated.
You can use close({ resize: true }) to resize header`);
  }
};

/**
 * @param {CarBufferWriter} writer
 * @param {number} byteLength
 */
const resizeHeader = (writer, byteLength) => {
  const { bytes, headerSize } = writer;
  // Move data section to a new offset
  bytes.set(bytes.subarray(headerSize, writer.byteOffset), byteLength);
  // Update header size & byteOffset
  writer.byteOffset += byteLength - headerSize;
  writer.headerSize = byteLength;
};

/**
 * @param {CarBufferWriter} writer
 * @param {number[]|Uint8Array} bytes
 */

const writeBytes = (writer, bytes) => {
  writer.bytes.set(bytes, writer.byteOffset);
  writer.byteOffset += bytes.length;
};
/**
 * @param {{bytes:Uint8Array}} writer
 * @param {number[]} varint
 * @param {Uint8Array} header
 */
const writeHeader = ({ bytes }, varint, header) => {
  bytes.set(varint);
  bytes.set(header, varint.length);
};

const headerPreludeTokens = [
  new Token(Type.map, 2),
  new Token(Type.string, "version"),
  new Token(Type.uint, 1),
  new Token(Type.string, "roots"),
];

const CID_TAG = new Token(Type.tag, 42);

/**
 * Calculates header size given the array of byteLength for roots.
 *
 * @name CarBufferWriter.calculateHeaderLength(rootLengths)
 * @param {number[]} rootLengths
 * @returns {number}
 */
const calculateHeaderLength = (rootLengths) => {
  const tokens = [...headerPreludeTokens];
  tokens.push(new Token(Type.array, rootLengths.length));
  for (const rootLength of rootLengths) {
    tokens.push(CID_TAG);
    tokens.push(new Token(Type.bytes, { length: rootLength + 1 }));
  }
  const length = tokensToLength(tokens); // no options needed here because we have simple tokens
  return varint$3.encodingLength(length) + length;
};

/**
 * Calculates header size given the array of roots.
 *
 * @name CarBufferWriter.headerLength({ roots })
 * @param {object} options
 * @param {CID[]} options.roots
 * @returns {number}
 */
const headerLength = ({ roots }) =>
  calculateHeaderLength(roots.map((cid) => cid.bytes.byteLength));

/**
 * Creates synchronous CAR writer that can be used to encode blocks into a given
 * buffer. Optionally you could pass `byteOffset` and `byteLength` to specify a
 * range inside buffer to write into. If car file is going to have `roots` you
 * need to either pass them under `options.roots` (from which header size will
 * be calculated) or provide `options.headerSize` to allocate required space
 * in the buffer. You may also provide known `roots` and `headerSize` to
 * allocate space for the roots that may not be known ahead of time.
 *
 * Note: Incorrect `headerSize` may lead to copying bytes inside a buffer
 * which will have a negative impact on performance.
 *
 * @name CarBufferWriter.createWriter(buffer[, options])
 * @param {ArrayBuffer} buffer
 * @param {object} [options]
 * @param {CID[]} [options.roots]
 * @param {number} [options.byteOffset]
 * @param {number} [options.byteLength]
 * @param {number} [options.headerSize]
 * @returns {CarBufferWriter}
 */
const createWriter$2 = (buffer, options = {}) => {
  const {
    roots = [],
    byteOffset = 0,
    byteLength = buffer.byteLength,
    headerSize = headerLength({ roots }),
  } = options;
  const bytes = new Uint8Array(buffer, byteOffset, byteLength);

  const writer = new CarBufferWriter(bytes, headerSize);
  for (const root of roots) {
    writer.addRoot(root);
  }

  return writer;
};

// @see https://www.iana.org/assignments/media-types/application/vnd.ipld.car
const contentType$6 = "application/vnd.ipld.car";
const name$7 = "CAR";

/** @type {API.MulticodecCode<0x0202, 'CAR'>} */
const code$c = 0x0202;

/**
 * @typedef {{
 * roots: API.IPLDBlock[]
 * blocks: Map<string, API.IPLDBlock>
 * }} Model
 */

class Writer {
  /**
   * @param {API.IPLDBlock[]} blocks
   * @param {number} byteLength
   */
  constructor(blocks = [], byteLength = 0) {
    this.written = new Set();
    this.blocks = blocks;
    this.byteLength = byteLength;
  }
  /**
   * @param {API.IPLDBlock[]} blocks
   */
  write(...blocks) {
    for (const block of blocks) {
      const id = block.cid.toString(base32$2);
      if (!this.written.has(id)) {
        this.blocks.push(block);
        this.byteLength += blockLength(/** @type {any} */ (block));
        this.written.add(id);
      }
    }
    return this;
  }
  /**
   * @param {API.IPLDBlock[]} rootBlocks
   */
  flush(...rootBlocks) {
    const roots = [];
    // We reverse the roots so that the first root is the last block in the CAR
    for (const block of rootBlocks.reverse()) {
      const id = block.cid.toString(base32$2);
      if (!this.written.has(id)) {
        this.blocks.unshift(block);
        this.byteLength += blockLength({
          cid: /** @type {CarBufferWriter.CID} */ (block.cid),
          bytes: block.bytes,
        });
        this.written.add(id);
      }

      // We unshift here because we want to preserve the order of the roots
      roots.unshift(/** @type {CarBufferWriter.CID} */ (block.cid));
    }

    this.byteLength += headerLength({ roots });

    const buffer = new ArrayBuffer(this.byteLength);
    const writer = createWriter$2(buffer, { roots });

    for (const block of /** @type {CarBufferWriter.Block[]} */ (this.blocks)) {
      writer.write(block);
    }

    return writer.close();
  }
}

const createWriter$1 = () => new Writer();

/**
 * @template {Partial<Model>} T
 * @param {T} input
 * @returns {API.ByteView<T>}
 */
const encode$h = ({ roots = [], blocks }) => {
  const writer = new Writer();
  if (blocks) {
    writer.write(...blocks.values());
  }
  return writer.flush(...roots);
};

/**
 * @param {API.ByteView<Partial<Model>>} bytes
 * @returns {Model}
 */
const decode$o = (bytes) => {
  const reader = CarBufferReader.fromBytes(bytes);
  /** @type {API.IPLDBlock[]} */
  const roots = [];
  const blocks = new Map();

  for (const root of reader.getRoots()) {
    const block = /** @type {API.IPLDBlock} */ (reader.get(root));
    if (block) {
      roots.push(block);
    }
  }

  for (const block of reader.blocks()) {
    blocks.set(block.cid.toString(), block);
  }

  return { roots, blocks };
};

/**
 * @template {Partial<Model>} T
 * @param {API.ByteView<T>} bytes
 * @param {{hasher?: API.MultihashHasher }} options
 */
const link$2 = async (bytes, { hasher = sha256$4 } = {}) => {
  return /** @type {API.Link<T, typeof code, typeof hasher.code>} */ (
    create$g(code$c, await hasher.digest(bytes))
  );
};

/**
 * @template {Partial<Model>} T
 * @param {T} data
 * @param {{hasher?: API.MultihashHasher }} [options]
 * @returns {Promise<API.Block<T, typeof code>>}
 */
const write$4 = async (data, options) => {
  const bytes = encode$h(data);
  const cid = await link$2(bytes, options);

  return { bytes, cid };
};

var car = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  code: code$c,
  contentType: contentType$6,
  createWriter: createWriter$1,
  decode: decode$o,
  encode: encode$h,
  link: link$2,
  name: name$7,
  write: write$4,
});

/**
 * Creates the success result containing given `value`. Throws if
 * `null` or `undefined` passed to encourage use of units instead.
 *
 * @template {{}|string|boolean|number} T
 * @param {T} value
 * @returns {{ok: T, error?:undefined}}
 */
const ok = (value) => {
  if (value == null) {
    throw new TypeError(`ok(${value}) is not allowed, consider ok({}) instead`);
  } else {
    return { ok: value };
  }
};

/**
 * Creates the failing result containing given `cause` of error.
 * Throws if `cause` is `null` or `undefined` to encourage
 * passing descriptive errors instead.
 *
 * @template {{}|string|boolean|number} X
 * @param {X} cause
 * @returns {{ok?:undefined, error:X}}
 */
const error$1 = (cause) => {
  if (cause == null) {
    throw new TypeError(
      `error(${cause}) is not allowed, consider passing an error instead`
    );
  } else {
    return { error: cause };
  }
};

/**
 * Crash the program with a given `message`. This function is
 * intended to be used in places where it is impossible to
 * recover from an error. It is similar to `panic` function in
 * Rust.
 *
 * @param {string} message
 */
const panic$1 = (message) => {
  throw new Failure(message);
};
/**
 * Creates the failing result containing an error with a given
 * `message`. Unlike `error` function it creates a very generic
 *  error with `message` & `stack` fields. The `error` function
 * is recommended over `fail` for all but the most basic use cases.
 *
 * @param {string} message
 * @returns {{error:API.Failure, ok?:undefined}}
 */
const fail = (message) => ({ error: new Failure(message) });

/**
 * @implements {API.Failure}
 */
class Failure extends Error {
  describe() {
    return this.toString();
  }
  get message() {
    return this.describe();
  }
  toJSON() {
    const { name, message, stack } = this;
    return { name, message, stack };
  }
}

/**
 * @abstract
 * @template [T=unknown]
 * @template [I=unknown]
 * @template [Settings=void]
 * @extends {Schema.Base<T, I, Settings>}
 * @implements {Schema.Schema<T, I>}
 */
class API {
  /**
   * @param {Settings} settings
   */
  constructor(settings) {
    /** @protected */
    this.settings = settings;
  }

  toString() {
    return `new ${this.constructor.name}()`;
  }
  /**
   * @abstract
   * @param {I} input
   * @param {Settings} settings
   * @returns {Schema.ReadResult<T>}
   */
  /* c8 ignore next 3 */
  readWith(input, settings) {
    throw new Error(`Abstract method readWith must be implemented by subclass`);
  }
  /**
   * @param {I} input
   * @returns {Schema.ReadResult<T>}
   */
  read(input) {
    return this.readWith(input, this.settings);
  }

  /**
   * @param {unknown} value
   * @returns {value is T}
   */
  is(value) {
    return !this.read(/** @type {I} */ (value))?.error;
  }

  /**
   * @param {unknown} value
   * @return {T}
   */
  from(value) {
    const result = this.read(/** @type {I} */ (value));
    if (result.error) {
      throw result.error;
    } else {
      return result.ok;
    }
  }

  /**
   * @returns {Schema.Schema<T|undefined, I>}
   */
  optional() {
    return optional$1(this);
  }

  /**
   * @returns {Schema.Schema<T|null, I>}
   */
  nullable() {
    return nullable(this);
  }

  /**
   * @returns {Schema.Schema<T[], I>}
   */
  array() {
    return array(this);
  }
  /**
   * @template U
   * @param {Schema.Reader<U, I>} schema
   * @returns {Schema.Schema<T | U, I>}
   */

  or(schema) {
    return or$9(this, schema);
  }

  /**
   * @template U
   * @param {Schema.Reader<U, I>} schema
   * @returns {Schema.Schema<T & U, I>}
   */
  and(schema) {
    return and$4(this, schema);
  }

  /**
   * @template {T} U
   * @param {Schema.Reader<U, T>} schema
   * @returns {Schema.Schema<U, I>}
   */
  refine(schema) {
    return refine(this, schema);
  }

  /**
   * @template {string} Kind
   * @param {Kind} [kind]
   * @returns {Schema.Schema<Schema.Branded<T, Kind>, I>}
   */
  brand(kind) {
    return /** @type {Schema.Schema<Schema.Branded<T, Kind>, I>} */ (this);
  }

  /**
   * @param {Schema.NotUndefined<T>} value
   * @returns {Schema.DefaultSchema<Schema.NotUndefined<T>, I>}
   */
  default(value) {
    // ⚠️ this.from will throw if wrong default is provided
    const fallback = this.from(value);
    // we also check that fallback is not undefined because that is the point
    // of having a fallback
    if (fallback === undefined) {
      throw new Error(`Value of type undefined is not a valid default`);
    }

    const schema = new Default({
      reader: /** @type {Schema.Reader<T, I>} */ (this),
      value: /** @type {Schema.NotUndefined<T>} */ (fallback),
    });

    return /** @type {Schema.DefaultSchema<Schema.NotUndefined<T>, I>} */ (
      schema
    );
  }
}

/**
 * @template [I=unknown]
 * @extends API<unknown, I, void>
 * @implements {Schema.Schema<unknown, I>}
 */
class Unknown extends API {
  /**
   * @param {I} input
   */
  read(input) {
    return /** @type {Schema.ReadResult<unknown>}*/ ({ ok: input });
  }
  toString() {
    return "unknown()";
  }
}

/**
 * @template [I=unknown]
 * @returns {Schema.Schema<unknown, I>}
 */
const unknown = () => new Unknown();

/**
 * @template O
 * @template [I=unknown]
 * @extends {API<null|O, I, Schema.Reader<O, I>>}
 * @implements {Schema.Schema<null|O, I>}
 */
class Nullable extends API {
  /**
   * @param {I} input
   * @param {Schema.Reader<O, I>} reader
   */
  readWith(input, reader) {
    const result = reader.read(input);
    if (result.error) {
      return input === null
        ? { ok: null }
        : {
            error: new UnionError({
              causes: [
                result.error,
                typeError({ expect: "null", actual: input }).error,
              ],
            }),
          };
    } else {
      return result;
    }
  }
  toString() {
    return `${this.settings}.nullable()`;
  }
}

/**
 * @template O
 * @template [I=unknown]
 * @param {Schema.Reader<O, I>} schema
 * @returns {Schema.Schema<O|null, I>}
 */
const nullable = (schema) => new Nullable(schema);

/**
 * @template O
 * @template [I=unknown]
 * @extends {API<O|undefined, I, Schema.Reader<O, I>>}
 * @implements {Schema.Schema<O|undefined, I>}
 */
class Optional extends API {
  optional() {
    return this;
  }
  /**
   * @param {I} input
   * @param {Schema.Reader<O, I>} reader
   * @returns {Schema.ReadResult<O|undefined>}
   */
  readWith(input, reader) {
    const result = reader.read(input);
    return result.error && input === undefined ? { ok: undefined } : result;
  }
  toString() {
    return `${this.settings}.optional()`;
  }
}

/**
 * @template {unknown} O
 * @template [I=unknown]
 * @extends {API<O, I, {reader:Schema.Reader<O, I>, value:O & Schema.NotUndefined<O>}>}
 * @implements {Schema.DefaultSchema<O, I>}
 */
class Default extends API {
  /**
   * @returns {Schema.DefaultSchema<O & Schema.NotUndefined<O>, I>}
   */
  optional() {
    // Short circuit here as we there is no point in wrapping this in optional.
    return /** @type {Schema.DefaultSchema<O & Schema.NotUndefined<O>, I>} */ (
      this
    );
  }
  /**
   * @param {I} input
   * @param {object} options
   * @param {Schema.Reader<O|undefined, I>} options.reader
   * @param {O} options.value
   * @returns {Schema.ReadResult<O>}
   */
  readWith(input, { reader, value }) {
    if (input === undefined) {
      return /** @type {Schema.ReadResult<O>} */ ({ ok: value });
    } else {
      const result = reader.read(input);

      return result.error
        ? result
        : result.ok !== undefined
        ? // We just checked that result.ok is not undefined but still needs
          // reassurance
          /** @type {Schema.ReadResult<O>} */ (result)
        : { ok: value };
    }
  }
  toString() {
    return `${this.settings.reader}.default(${JSON.stringify(
      this.settings.value
    )})`;
  }

  get value() {
    return this.settings.value;
  }
}

/**
 * @template O
 * @template [I=unknown]
 * @param {Schema.Reader<O, I>} schema
 * @returns {Schema.Schema<O|undefined, I>}
 */
const optional$1 = (schema) => new Optional(schema);

/**
 * @template O
 * @template [I=unknown]
 * @extends {API<O[], I, Schema.Reader<O, I>>}
 * @implements {Schema.ArraySchema<O, I>}
 */
class ArrayOf extends API {
  /**
   * @param {I} input
   * @param {Schema.Reader<O, I>} schema
   */
  readWith(input, schema) {
    if (!Array.isArray(input)) {
      return typeError({ expect: "array", actual: input });
    }
    /** @type {O[]} */
    const results = [];
    for (const [index, value] of input.entries()) {
      const result = schema.read(value);
      if (result.error) {
        return memberError({ at: index, cause: result.error });
      } else {
        results.push(result.ok);
      }
    }
    return { ok: results };
  }
  get element() {
    return this.settings;
  }
  toString() {
    return `array(${this.element})`;
  }
}

/**
 * @template O
 * @template [I=unknown]
 * @param {Schema.Reader<O, I>} schema
 * @returns {Schema.ArraySchema<O, I>}
 */
const array = (schema) => new ArrayOf(schema);

/**
 * @template {Schema.Reader<unknown, I>} T
 * @template {[T, ...T[]]} U
 * @template [I=unknown]
 * @extends {API<Schema.InferTuple<U>, I, U>}
 * @implements {Schema.Schema<Schema.InferTuple<U>, I>}
 */
class Tuple extends API {
  /**
   * @param {I} input
   * @param {U} shape
   * @returns {Schema.ReadResult<Schema.InferTuple<U>>}
   */
  readWith(input, shape) {
    if (!Array.isArray(input)) {
      return typeError({ expect: "array", actual: input });
    }
    if (input.length !== this.shape.length) {
      return error(`Array must contain exactly ${this.shape.length} elements`);
    }

    const results = [];
    for (const [index, reader] of shape.entries()) {
      const result = reader.read(input[index]);
      if (result.error) {
        return memberError({ at: index, cause: result.error });
      } else {
        results[index] = result.ok;
      }
    }

    return { ok: /** @type {Schema.InferTuple<U>} */ (results) };
  }

  /** @type {U} */
  get shape() {
    return this.settings;
  }

  toString() {
    return `tuple([${this.shape
      .map((reader) => reader.toString())
      .join(", ")}])`;
  }
}

/**
 * @template {Schema.Reader<unknown, I>} T
 * @template {[T, ...T[]]} U
 * @template [I=unknown]
 * @param {U} shape
 * @returns {Schema.Schema<Schema.InferTuple<U>, I>}
 */
const tuple = (shape) => new Tuple(shape);

/**
 * @template V
 * @template {string} K
 * @template [I=unknown]
 * @extends {API<Schema.Dictionary<K, V>, I, { key: Schema.Reader<K, string>, value: Schema.Reader<V, I> }>}
 * @implements {Schema.DictionarySchema<V, K, I>}
 */
class Dictionary extends API {
  /**
   * @param {I} input
   * @param {object} schema
   * @param {Schema.Reader<K, string>} schema.key
   * @param {Schema.Reader<V, I>} schema.value
   */
  readWith(input, { key, value }) {
    if (typeof input != "object" || input === null || Array.isArray(input)) {
      return typeError({
        expect: "dictionary",
        actual: input,
      });
    }

    const dict = /** @type {Schema.Dictionary<K, V>} */ ({});

    for (const [k, v] of Object.entries(input)) {
      const keyResult = key.read(k);
      if (keyResult.error) {
        return memberError({ at: k, cause: keyResult.error });
      }

      const valueResult = value.read(v);
      if (valueResult.error) {
        return memberError({ at: k, cause: valueResult.error });
      }

      // skip undefined because they mess up CBOR and are generally useless.
      if (valueResult.ok !== undefined) {
        dict[keyResult.ok] = valueResult.ok;
      }
    }

    return { ok: dict };
  }
  get key() {
    return this.settings.key;
  }
  get value() {
    return this.settings.value;
  }

  partial() {
    const { key, value } = this.settings;
    return new Dictionary({
      key,
      value: optional$1(value),
    });
  }
  toString() {
    return `dictionary(${this.settings})`;
  }
}

/**
 * @template {string} K
 * @template {unknown} V
 * @template [I=unknown]
 * @param {object} shape
 * @param {Schema.Reader<V, I>} shape.value
 * @param {Schema.Reader<K, string>} [shape.key]
 * @returns {Schema.DictionarySchema<V, K, I>}
 */
const dictionary = ({ value, key }) =>
  new Dictionary({
    value,
    key: key || /** @type {Schema.Reader<K, string>} */ (string()),
  });

/**
 * @template {Schema.Reader<unknown, I>} T
 * @template {[T, ...T[]]} U
 * @template [I=unknown]
 * @extends {API<Schema.InferUnion<U>, I, U>}
 * @implements {Schema.Schema<Schema.InferUnion<U>, I>}
 */
class Union extends API {
  /**
   * @param {I} input
   * @param {U} variants
   */
  readWith(input, variants) {
    const causes = [];
    for (const reader of variants) {
      const result = reader.read(input);
      if (result.error) {
        causes.push(result.error);
      } else {
        return /** @type {Schema.ReadResult<Schema.InferUnion<U>>} */ (result);
      }
    }
    return { error: new UnionError({ causes }) };
  }

  get variants() {
    return this.settings;
  }
  toString() {
    return `union([${this.variants
      .map((type) => type.toString())
      .join(", ")}])`;
  }
}

/**
 * @template {Schema.Reader<unknown, I>} T
 * @template {[T, ...T[]]} U
 * @template [I=unknown]
 * @param {U} variants
 * @returns {Schema.Schema<Schema.InferUnion<U>, I>}
 */
const union = (variants) => new Union(variants);

/**
 * @template T, U
 * @template [I=unknown]
 * @param {Schema.Reader<T, I>} left
 * @param {Schema.Reader<U, I>} right
 * @returns {Schema.Schema<T|U, I>}
 */
const or$9 = (left, right) => union([left, right]);

/**
 * @template {Schema.Reader<unknown, I>} T
 * @template {[T, ...T[]]} U
 * @template [I=unknown]
 * @extends {API<Schema.InferIntersection<U>, I, U>}
 * @implements {Schema.Schema<Schema.InferIntersection<U>, I>}
 */
class Intersection extends API {
  /**
   * @param {I} input
   * @param {U} schemas
   * @returns {Schema.ReadResult<Schema.InferIntersection<U>>}
   */
  readWith(input, schemas) {
    const causes = [];
    for (const schema of schemas) {
      const result = schema.read(input);
      if (result.error) {
        causes.push(result.error);
      }
    }

    return causes.length > 0
      ? { error: new IntersectionError({ causes }) }
      : /** @type {Schema.ReadResult<Schema.InferIntersection<U>>} */ ({
          ok: input,
        });
  }
  toString() {
    return `intersection([${this.settings
      .map((type) => type.toString())
      .join(",")}])`;
  }
}

/**
 * @template {Schema.Reader<unknown, I>} T
 * @template {[T, ...T[]]} U
 * @template [I=unknown]
 * @param {U} variants
 * @returns {Schema.Schema<Schema.InferIntersection<U>, I>}
 */
const intersection$1 = (variants) => new Intersection(variants);

/**
 * @template T, U
 * @template [I=unknown]
 * @param {Schema.Reader<T, I>} left
 * @param {Schema.Reader<U, I>} right
 * @returns {Schema.Schema<T & U, I>}
 */
const and$4 = (left, right) => intersection$1([left, right]);

/**
 * @template [I=unknown]
 * @extends {API<boolean, I>}
 */
let Boolean$1 = class Boolean extends API {
  /**
   * @param {I} input
   */
  readWith(input) {
    switch (input) {
      case true:
      case false:
        return { ok: /** @type {boolean} */ (input) };
      default:
        return typeError({
          expect: "boolean",
          actual: input,
        });
    }
  }

  toString() {
    return `boolean()`;
  }
};

/** @type {Schema.Schema<boolean, unknown>} */
const anyBoolean = new Boolean$1();

const boolean = () => anyBoolean;

/**
 * @template {number} [O=number]
 * @template [I=unknown]
 * @template [Settings=void]
 * @extends {API<O, I, Settings>}
 * @implements {Schema.NumberSchema<O, I>}
 */
class UnknownNumber extends API {
  /**
   * @param {number} n
   */
  greaterThan(n) {
    return this.refine(greaterThan(n));
  }
  /**
   * @param {number} n
   */
  lessThan(n) {
    return this.refine(lessThan(n));
  }

  /**
   * @template {O} U
   * @param {Schema.Reader<U, O>} schema
   * @returns {Schema.NumberSchema<U, I>}
   */
  refine(schema) {
    return new RefinedNumber({ base: this, schema });
  }
}

/**
 * @template [I=unknown]
 * @extends {UnknownNumber<number, I>}
 * @implements {Schema.NumberSchema<number, I>}
 */
class AnyNumber extends UnknownNumber {
  /**
   * @param {I} input
   * @returns {Schema.ReadResult<number>}
   */
  readWith(input) {
    return typeof input === "number"
      ? { ok: input }
      : typeError({ expect: "number", actual: input });
  }
  toString() {
    return `number()`;
  }
}

/** @type {Schema.NumberSchema<number, unknown>} */
const anyNumber = new AnyNumber();
const number = () => anyNumber;

/**
 * @template {number} [T=number]
 * @template {T} [O=T]
 * @template [I=unknown]
 * @extends {UnknownNumber<O, I, {base:Schema.Reader<T, I>, schema:Schema.Reader<O, T>}>}
 * @implements {Schema.NumberSchema<O, I>}
 */
class RefinedNumber extends UnknownNumber {
  /**
   * @param {I} input
   * @param {{base:Schema.Reader<T, I>, schema:Schema.Reader<O, T>}} settings
   * @returns {Schema.ReadResult<O>}
   */
  readWith(input, { base, schema }) {
    const result = base.read(input);
    return result.error ? result : schema.read(result.ok);
  }
  toString() {
    return `${this.settings.base}.refine(${this.settings.schema})`;
  }
}

/**
 * @template {number} T
 * @extends {API<T, T, number>}
 */
class LessThan extends API {
  /**
   * @param {T} input
   * @param {number} number
   * @returns {Schema.ReadResult<T>}
   */
  readWith(input, number) {
    if (input < number) {
      return { ok: input };
    } else {
      return error(`Expected ${input} < ${number}`);
    }
  }
  toString() {
    return `lessThan(${this.settings})`;
  }
}

/**
 * @template {number} T
 * @param {number} n
 * @returns {Schema.Schema<T, T>}
 */
const lessThan = (n) => new LessThan(n);

/**
 * @template {number} T
 * @extends {API<T, T, number>}
 */
class GreaterThan extends API {
  /**
   * @param {T} input
   * @param {number} number
   * @returns {Schema.ReadResult<T>}
   */
  readWith(input, number) {
    if (input > number) {
      return { ok: input };
    } else {
      return error(`Expected ${input} > ${number}`);
    }
  }
  toString() {
    return `greaterThan(${this.settings})`;
  }
}

/**
 * @template {number} T
 * @param {number} n
 * @returns {Schema.Schema<T, T>}
 */
const greaterThan = (n) => new GreaterThan(n);

const Integer = {
  /**
   * @param {number} input
   * @returns {Schema.ReadResult<Schema.Integer>}
   */
  read(input) {
    return Number.isInteger(input)
      ? { ok: /** @type {Schema.Integer} */ (input) }
      : typeError({
          expect: "integer",
          actual: input,
        });
  },
  toString() {
    return `Integer`;
  },
};

const anyInteger = anyNumber.refine(Integer);
const integer = () => anyInteger;

const Float = {
  /**
   * @param {number} number
   * @returns {Schema.ReadResult<Schema.Float>}
   */
  read(number) {
    return Number.isFinite(number)
      ? { ok: /** @type {Schema.Float} */ (number) }
      : typeError({
          expect: "Float",
          actual: number,
        });
  },
  toString() {
    return "Float";
  },
};

anyNumber.refine(Float);

/**
 * @template {string} [O=string]
 * @template [I=unknown]
 * @template [Settings=void]
 * @extends {API<O, I, Settings>}
 */
class UnknownString extends API {
  /**
   * @template {O|unknown} U
   * @param {Schema.Reader<U, O>} schema
   * @returns {Schema.StringSchema<O & U, I>}
   */
  refine(schema) {
    const other = /** @type {Schema.Reader<U, O>} */ (schema);
    const rest = new RefinedString({
      base: this,
      schema: other,
    });

    return /** @type {Schema.StringSchema<O & U, I>} */ (rest);
  }
  /**
   * @template {string} Prefix
   * @param {Prefix} prefix
   */
  startsWith(prefix) {
    return this.refine(startsWith(prefix));
  }
  /**
   * @template {string} Suffix
   * @param {Suffix} suffix
   */
  endsWith(suffix) {
    return this.refine(endsWith(suffix));
  }
  toString() {
    return `string()`;
  }
}

/**
 * @template O
 * @template {string} [T=string]
 * @template [I=unknown]
 * @extends {UnknownString<T & O, I, {base:Schema.Reader<T, I>, schema:Schema.Reader<O, T>}>}
 * @implements {Schema.StringSchema<O & T, I>}
 */
class RefinedString extends UnknownString {
  /**
   * @param {I} input
   * @param {{base:Schema.Reader<T, I>, schema:Schema.Reader<O, T>}} settings
   * @returns {Schema.ReadResult<T & O>}
   */
  readWith(input, { base, schema }) {
    const result = base.read(input);
    return result.error
      ? result
      : /** @type {Schema.ReadResult<T & O>} */ (schema.read(result.ok));
  }
  toString() {
    return `${this.settings.base}.refine(${this.settings.schema})`;
  }
}

/**
 * @template [I=unknown]
 * @extends {UnknownString<string, I>}
 * @implements {Schema.StringSchema<string, I>}
 */
class AnyString extends UnknownString {
  /**
   * @param {I} input
   * @returns {Schema.ReadResult<string>}
   */
  readWith(input) {
    return typeof input === "string"
      ? { ok: input }
      : typeError({ expect: "string", actual: input });
  }
}

/** @type {Schema.StringSchema<string, unknown>} */
const anyString = new AnyString();
const string = () => anyString;

/**
 * @template [I=unknown]
 * @extends {API<Uint8Array, I, void>}
 */
class BytesSchema extends API {
  /**
   * @param {I} input
   * @returns {Schema.ReadResult<Uint8Array>}
   */
  readWith(input) {
    if (input instanceof Uint8Array) {
      return { ok: input };
    } else {
      return typeError({ expect: "Uint8Array", actual: input });
    }
  }
}

/** @type {Schema.Schema<Uint8Array, unknown>} */
const Bytes = new BytesSchema();
const bytes = () => Bytes;

/**
 * @template {string} Prefix
 * @template {string} Body
 * @extends {API<Body & `${Prefix}${Body}`, Body, Prefix>}
 * @implements {Schema.Schema<Body & `${Prefix}${Body}`, Body>}
 */
class StartsWith extends API {
  /**
   * @param {Body} input
   * @param {Prefix} prefix
   */
  readWith(input, prefix) {
    const result = input.startsWith(prefix)
      ? /** @type {Schema.ReadResult<Body & `${Prefix}${Body}`>} */ ({
          ok: input,
        })
      : error(`Expect string to start with "${prefix}" instead got "${input}"`);

    return result;
  }
  get prefix() {
    return this.settings;
  }
  toString() {
    return `startsWith("${this.prefix}")`;
  }
}

/**
 * @template {string} Prefix
 * @template {string} Body
 * @param {Prefix} prefix
 * @returns {Schema.Schema<`${Prefix}${string}`, string>}
 */
const startsWith = (prefix) => new StartsWith(prefix);

/**
 * @template {string} Suffix
 * @template {string} Body
 * @extends {API<Body & `${Body}${Suffix}`, Body, Suffix>}
 */
class EndsWith extends API {
  /**
   * @param {Body} input
   * @param {Suffix} suffix
   */
  readWith(input, suffix) {
    return input.endsWith(suffix)
      ? /** @type {Schema.ReadResult<Body & `${Body}${Suffix}`>} */ ({
          ok: input,
        })
      : error(`Expect string to end with "${suffix}" instead got "${input}"`);
  }
  get suffix() {
    return this.settings;
  }
  toString() {
    return `endsWith("${this.suffix}")`;
  }
}

/**
 * @template {string} Suffix
 * @param {Suffix} suffix
 * @returns {Schema.Schema<`${string}${Suffix}`, string>}
 */
const endsWith = (suffix) => new EndsWith(suffix);

/**
 * @template T
 * @template {T} U
 * @template [I=unknown]
 * @extends {API<U, I, { base: Schema.Reader<T, I>, schema: Schema.Reader<U, T> }>}
 * @implements {Schema.Schema<U, I>}
 */

class Refine extends API {
  /**
   * @param {I} input
   * @param {{ base: Schema.Reader<T, I>, schema: Schema.Reader<U, T> }} settings
   */
  readWith(input, { base, schema }) {
    const result = base.read(input);
    return result.error ? result : schema.read(result.ok);
  }
  toString() {
    return `${this.settings.base}.refine(${this.settings.schema})`;
  }
}

/**
 * @template T
 * @template {T} U
 * @template [I=unknown]
 * @param {Schema.Reader<T, I>} base
 * @param {Schema.Reader<U, T>} schema
 * @returns {Schema.Schema<U, I>}
 */
const refine = (base, schema) => new Refine({ base, schema });

/**
 * @template {null|boolean|string|number} T
 * @template [I=unknown]
 * @extends {API<T, I, T>}
 * @implements {Schema.LiteralSchema<T, I>}
 */
class Literal extends API {
  /**
   * @param {I} input
   * @param {T} expect
   * @returns {Schema.ReadResult<T>}
   */
  readWith(input, expect) {
    return input !== /** @type {unknown} */ (expect)
      ? { error: new LiteralError({ expect, actual: input }) }
      : { ok: expect };
  }
  get value() {
    return /** @type {Exclude<T, undefined>} */ (this.settings);
  }
  /**
   * @template {Schema.NotUndefined<T>} U
   * @param {U} value
   */
  default(value = /** @type {U} */ (this.value)) {
    return super.default(value);
  }
  toString() {
    return `literal(${toString(this.value)})`;
  }
}

/**
 * @template {null|boolean|string|number} T
 * @template [I=unknown]
 * @param {T} value
 * @returns {Schema.LiteralSchema<T, I>}
 */
const literal = (value) => new Literal(value);

/**
 * @template {{[key:string]: Schema.Reader}} U
 * @template [I=unknown]
 * @extends {API<Schema.InferStruct<U>, I, U>}
 */
class Struct extends API {
  /**
   * @param {I} input
   * @param {U} shape
   * @returns {Schema.ReadResult<Schema.InferStruct<U>>}
   */
  readWith(input, shape) {
    if (typeof input != "object" || input === null || Array.isArray(input)) {
      return typeError({
        expect: "object",
        actual: input,
      });
    }

    const source = /** @type {{[K in keyof U]: unknown}} */ (input);

    const struct = /** @type {{[K in keyof U]: Schema.Infer<U[K]>}} */ ({});
    const entries =
      /** @type {{[K in keyof U]: [K & string, U[K]]}[keyof U][]} */ (
        Object.entries(shape)
      );

    for (const [at, reader] of entries) {
      const result = reader.read(source[at]);
      if (result.error) {
        return memberError({ at, cause: result.error });
      }
      // skip undefined because they mess up CBOR and are generally useless.
      else if (result.ok !== undefined) {
        struct[at] = /** @type {Schema.Infer<U[typeof at]>} */ (result.ok);
      }
    }

    return { ok: struct };
  }

  /**
   * @returns {Schema.MapRepresentation<Partial<Schema.InferStruct<U>>> & Schema.StructSchema}
   */
  partial() {
    return new Struct(
      Object.fromEntries(
        Object.entries(this.shape).map(([key, value]) => [
          key,
          optional$1(value),
        ])
      )
    );
  }

  /** @type {U} */
  get shape() {
    // @ts-ignore - We declared `settings` private but we access it here
    return this.settings;
  }

  toString() {
    return [
      `struct({ `,
      ...Object.entries(this.shape)
        .map(([key, schema]) => `${key}: ${schema}`)
        .join(", "),
      ` })`,
    ].join("");
  }

  /**
   * @param {Schema.InferStructSource<U>} data
   */
  create(data) {
    return this.from(data || {});
  }

  /**
   * @template {{[key:string]: Schema.Reader}} E
   * @param {E} extension
   * @returns {Schema.StructSchema<U & E, I>}
   */
  extend(extension) {
    return new Struct({ ...this.shape, ...extension });
  }
}

/**
 * @template {null|boolean|string|number} T
 * @template {{[key:string]: T|Schema.Reader}} U
 * @template {{[K in keyof U]: U[K] extends Schema.Reader ? U[K] : Schema.LiteralSchema<U[K] & T>}} V
 * @template [I=unknown]
 * @param {U} fields
 * @returns {Schema.StructSchema<V, I>}
 */
const struct = (fields) => {
  const shape =
    /** @type {{[K in keyof U]: Schema.Reader<unknown, unknown>}} */ ({});
  /** @type {[keyof U & string, T|Schema.Reader][]} */
  const entries = Object.entries(fields);

  for (const [key, field] of entries) {
    switch (typeof field) {
      case "number":
      case "string":
      case "boolean":
        shape[key] = literal(field);
        break;
      case "object":
        shape[key] = field === null ? literal(null) : field;
        break;
      default:
        throw new Error(
          `Invalid struct field "${key}", expected schema or literal, instead got ${typeof field}`
        );
    }
  }

  return new Struct(/** @type {V} */ (shape));
};

/**
 * @template {Schema.VariantChoices} U
 * @template [I=unknown]
 * @extends {API<Schema.InferVariant<U>, I, U>}
 * @implements {Schema.VariantSchema<U, I>}
 */
class Variant extends API {
  /**
   * @param {I} input
   * @param {U} variants
   * @returns {Schema.ReadResult<Schema.InferVariant<U>>}
   */
  readWith(input, variants) {
    if (typeof input != "object" || input === null || Array.isArray(input)) {
      return typeError({
        expect: "object",
        actual: input,
      });
    }

    const keys = /** @type {Array<keyof input & keyof variants & string>} */ (
      Object.keys(input)
    );

    const [key] = keys.length === 1 ? keys : [];
    const reader = key ? variants[key] : undefined;

    if (reader) {
      const result = reader.read(input[key]);
      return result.error
        ? memberError({ at: key, cause: result.error })
        : { ok: /** @type {Schema.InferVariant<U>} */ ({ [key]: result.ok }) };
    } else if (variants._) {
      const result = variants._.read(input);
      return result.error
        ? result
        : { ok: /** @type {Schema.InferVariant<U>} */ ({ _: result.ok }) };
    } else if (key) {
      return error(
        `Expected an object with one of the these keys: ${Object.keys(variants)
          .sort()
          .join(", ")} instead got object with key ${key}`
      );
    } else {
      return error(
        "Expected an object with a single key instead got object with keys " +
          keys.sort().join(", ")
      );
    }
  }

  /**
   * @template [E=never]
   * @param {I} input
   * @param {E} [fallback]
   */
  match(input, fallback) {
    const result = this.read(input);
    if (result.error) {
      if (fallback !== undefined) {
        return [null, fallback];
      } else {
        throw result.error;
      }
    } else {
      const [key] = Object.keys(result.ok);
      const value = result.ok[key];
      return /** @type {any} */ ([key, value]);
    }
  }

  /**
   * @template {Schema.InferVariant<U>} O
   * @param {O} source
   * @returns {O}
   */
  create(source) {
    return /** @type {O} */ (this.from(source));
  }
}

/**
 * Defines a schema for the `Variant` type. It takes an object where
 * keys denote branches of the variant and values are schemas for the values of
 * those branches. The schema will only match objects with a single key and
 * value that matches the schema for that key. If the object has more than one
 * key or the key does not match any of the keys in the schema then the schema
 * will fail.
 *
 * The `_` branch is a special case. If such branch is present then it will be
 * used as a fallback for any object that does not match any of the variant
 * branches. The `_` branch will be used even if the object has more than one
 * key. Unlike other branches the `_` branch will receive the entire object as
 * input and not just the value of the key. Usually the `_` branch can be set
 * to `Schema.unknown` or `Schema.dictionary` to facilitate exhaustive matching.
 *
 * @example
 * ```ts
 * const Shape = Variant({
 *    circle: Schema.struct({ radius: Schema.integer() }),
 *    rectangle: Schema.struct({ width: Schema.integer(), height: Schema.integer() })
 * })
 *
 * const demo = (input:unknown) => {
 *   const [kind, value] = Schema.match(input)
 *   switch (kind) {
 *     case "circle":
 *       return `Circle with radius ${shape.radius}`
 *     case "rectangle":
 *       return `Rectangle with width ${shape.width} and height ${shape.height}`
 *    }
 * }
 *
 * const ExhaustiveShape = Variant({
 *   circle: Schema.struct({ radius: Schema.integer() }),
 *   rectangle: Schema.struct({ width: Schema.integer(), height: Schema.integer() }),
 *  _: Schema.dictionary({ value: Schema.unknown() })
 * })
 *
 * const exhastiveDemo = (input:unknown) => {
 *   const [kind, value] = Schema.match(input)
 *   switch (kind) {
 *     case "circle":
 *       return `Circle with radius ${shape.radius}`
 *     case "rectangle":
 *       return `Rectangle with width ${shape.width} and height ${shape.height}`
 *     case: "_":
 *       return `Unknown shape ${JSON.stringify(value)}`
 *    }
 * }
 * ```
 *
 * @template {Schema.VariantChoices} Choices
 * @template [In=unknown]
 * @param {Choices} variants
 * @returns {Schema.VariantSchema<Choices, In>}
 */
const variant = (variants) => new Variant(variants);

/**
 * @param {string} message
 * @returns {{error: Schema.Error, ok?: undefined}}
 */
const error = (message) => ({ error: new SchemaError(message) });

class SchemaError extends Failure {
  get name() {
    return "SchemaError";
  }
  /* c8 ignore next 3 */
  describe() {
    return this.name;
  }
}

let TypeError$1 = class TypeError extends SchemaError {
  /**
   * @param {{expect:string, actual:unknown}} data
   */
  constructor({ expect, actual }) {
    super();
    this.expect = expect;
    this.actual = actual;
  }
  get name() {
    return "TypeError";
  }
  describe() {
    return `Expected value of type ${this.expect} instead got ${toString(
      this.actual
    )}`;
  }
};

/**
 * @param {object} data
 * @param {string} data.expect
 * @param {unknown} data.actual
 * @returns {{ error: Schema.Error }}
 */
const typeError = (data) => ({ error: new TypeError$1(data) });

/**
 *
 * @param {unknown} value
 */
const toString = (value) => {
  const type = typeof value;
  switch (type) {
    case "boolean":
    case "string":
      return JSON.stringify(value);
    // if these types we do not want JSON.stringify as it may mess things up
    // eg turn NaN and Infinity to null
    case "bigint":
      return `${value}n`;
    case "number":
    case "symbol":
    case "undefined":
      return String(value);
    case "object":
      return value === null
        ? "null"
        : Array.isArray(value)
        ? "array"
        : Symbol.toStringTag in /** @type {object} */ (value)
        ? value[Symbol.toStringTag]
        : "object";
    default:
      return type;
  }
};

class LiteralError extends SchemaError {
  /**
   * @param {{
   * expect:string|number|boolean|null
   * actual:unknown
   * }} data
   */
  constructor({ expect, actual }) {
    super();
    this.expect = expect;
    this.actual = actual;
  }
  get name() {
    return "LiteralError";
  }
  describe() {
    return `Expected literal ${toString(this.expect)} instead got ${toString(
      this.actual
    )}`;
  }
}

class ElementError extends SchemaError {
  /**
   * @param {{at:number, cause:Schema.Error}} data
   */
  constructor({ at, cause }) {
    super();
    this.at = at;
    this.cause = cause;
  }
  get name() {
    return "ElementError";
  }
  describe() {
    return [
      `Array contains invalid element at ${this.at}:`,
      li$1(this.cause.message),
    ].join("\n");
  }
}

class FieldError extends SchemaError {
  /**
   * @param {{at:string, cause:Schema.Error}} data
   */
  constructor({ at, cause }) {
    super();
    this.at = at;
    this.cause = cause;
  }
  get name() {
    return "FieldError";
  }
  describe() {
    return [
      `Object contains invalid field "${this.at}":`,
      li$1(this.cause.message),
    ].join("\n");
  }
}

/**
 * @param {object} options
 * @param {string|number} options.at
 * @param {Schema.Error} options.cause
 * @returns {{error: Schema.Error}}
 */
const memberError = ({ at, cause }) =>
  typeof at === "string"
    ? { error: new FieldError({ at, cause }) }
    : { error: new ElementError({ at, cause }) };

class UnionError extends SchemaError {
  /**
   * @param {{causes: Schema.Error[]}} data
   */
  constructor({ causes }) {
    super();
    this.causes = causes;
  }
  get name() {
    return "UnionError";
  }
  describe() {
    const { causes } = this;
    return [
      `Value does not match any type of the union:`,
      ...causes.map((cause) => li$1(cause.message)),
    ].join("\n");
  }
}

class IntersectionError extends SchemaError {
  /**
   * @param {{causes: Schema.Error[]}} data
   */
  constructor({ causes }) {
    super();
    this.causes = causes;
  }
  get name() {
    return "IntersectionError";
  }
  describe() {
    const { causes } = this;
    return [
      `Value does not match following types of the intersection:`,
      ...causes.map((cause) => li$1(cause.message)),
    ].join("\n");
  }
}

/**
 * @param {string} message
 */
const indent$1 = (message, indent = "  ") =>
  `${indent}${message.split("\n").join(`\n${indent}`)}`;

/**
 * @param {string} message
 */
const li$1 = (message) => indent$1(`- ${message}`);

/**
 * @template {API.Protocol} [P=API.Protocol]
 * @typedef {{protocol: P}} Options
 */

/**
 * @template {Options} O
 * @extends {Schema.API<API.URI<O['protocol']>, unknown, Partial<O>>}
 */
class URISchema extends API {
  /**
   * @param {unknown} input
   * @param {Partial<O>} options
   * @returns {Schema.ReadResult<API.URI<O['protocol']>>}
   */
  readWith(input, { protocol } = {}) {
    if (typeof input !== "string" && !(input instanceof URL)) {
      return error(
        `Expected URI but got ${input === null ? "null" : typeof input}`
      );
    }

    try {
      const url = new URL(String(input));
      if (protocol != null && url.protocol !== protocol) {
        return error(`Expected ${protocol} URI instead got ${url.href}`);
      } else {
        return { ok: /** @type {API.URI<O['protocol']>} */ (url.href) };
      }
    } catch (_) {
      return error(`Invalid URI`);
    }
  }
}

/**
 * @template {API.Protocol} P
 * @template {Options<P>} O
 * @param {O} options
 * @returns {Schema.Schema<API.URI<O['protocol']>, unknown>}
 */
const match$3 = (options) => new URISchema(options);

/**
 * @template {number} [Code=number]
 * @template {number} [Alg=number]
 * @template {1|0} [Version=0|1]
 * @typedef {{
 * code?:Code,
 * version?:Version
 * multihash?: {code?: Alg, digest?: Uint8Array}
 * }} Settings
 */

/**
 * @template {number} Code
 * @template {number} Alg
 * @template {1|0} Version
 * @extends {Schema.API<API.Link<unknown, Code, Alg, Version>, unknown, Settings<Code, Alg, Version>>}
 */
class LinkSchema extends API {
  /**
   *
   * @param {unknown} cid
   * @param {Settings<Code, Alg, Version>} settings
   * @returns {Schema.ReadResult<API.Link<unknown, Code, Alg, Version>>}
   */
  readWith(cid, { code, multihash = {}, version }) {
    if (cid == null) {
      return error(`Expected link but got ${cid} instead`);
    } else {
      if (!isLink(cid)) {
        return error(`Expected link to be a CID instead of ${cid}`);
      } else {
        if (code != null && cid.code !== code) {
          return error(
            `Expected link to be CID with 0x${code.toString(16)} codec`
          );
        }

        if (multihash.code != null && cid.multihash.code !== multihash.code)
          return error(
            `Expected link to be CID with 0x${multihash.code.toString(
              16
            )} hashing algorithm`
          );

        if (version != null && cid.version !== version) {
          return error(
            `Expected link to be CID version ${version} instead of ${cid.version}`
          );
        }

        const [expectDigest, actualDigest] =
          multihash.digest != null
            ? [
                base32$2.baseEncode(multihash.digest),
                base32$2.baseEncode(cid.multihash.digest),
              ]
            : ["", ""];

        if (expectDigest !== actualDigest) {
          return error(
            `Expected link with "${expectDigest}" hash digest instead of "${actualDigest}"`
          );
        }

        return {
          ok: /** @type {API.Link<unknown, any, any, any>} */ (cid),
        };
      }
    }
  }
}

/** @type {Schema.Schema<API.Link<unknown, number, number, 0|1>, unknown>}  */
const schema$2 = new LinkSchema({});

const link$1 = () => schema$2;

/**
 * @template {number} Code
 * @template {number} Alg
 * @template {1|0} Version
 * @param {Settings<Code, Alg, Version>} options
 * @returns {Schema.Schema<API.Link<unknown, Code, Alg, Version>>}
 */
const match$2 = (options = {}) => new LinkSchema(options);

/**
 * @param {unknown} input
 */
const read$5 = (input) => schema$2.read(input);

const optional = () => schema$2.optional();

var Link = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  create: create$g,
  createLegacy: createLegacy,
  isLink: isLink,
  link: link$1,
  match: match$2,
  optional: optional,
  parse: parse$2,
  read: read$5,
  schema: schema$2,
});

/**
 * @template {string} Method
 * @extends {Schema.API<API.DID<Method> & API.URI<"did:">, string, void|Method>}
 */
class DIDSchema extends API {
  /**
   * @param {string} source
   * @param {void|Method} method
   */
  readWith(source, method) {
    const prefix = method ? `did:${method}:` : `did:`;
    if (!source.startsWith(prefix)) {
      return error(`Expected a ${prefix} but got "${source}" instead`);
    } else {
      return { ok: /** @type {API.DID<Method>} */ (source) };
    }
  }
}

const schema$1 = string().refine(new DIDSchema());

const did = () => schema$1;
/**
 *
 * @param {unknown} input
 */
const read$4 = (input) => schema$1.read(input);

/**
 * @template {string} Method
 * @param {{method?: Method}} options
 */
const match$1 = (options = {}) =>
  /** @type {Schema.Schema<API.DID<Method> & API.URI<"did:">>} */ (
    string().refine(new DIDSchema(options.method))
  );

/**
 * Create a DID string from any input (or throw)
 * @param {unknown} input
 */
const from$c = (input) => match$1({}).from(input);

/**
 * @template {string} Method
 * @extends {Schema.API<API.DID<Method> & API.URI<"did:">, unknown, void|Method>}
 */
class DIDBytesSchema extends API {
  /**
   * @param {unknown} source
   * @param {void|Method} method
   */
  readWith(source, method) {
    if (!(source instanceof Uint8Array)) {
      return typeError({ expect: "Uint8Array", actual: source });
    }
    let did;
    try {
      did = decode$w(source).did();
    } catch (err) {
      return error(`Unable to parse bytes as did: ${err}`);
    }
    const prefix = method ? `did:${method}:` : `did:`;
    if (!did.startsWith(prefix)) {
      return error(`Expected a ${prefix} but got "${did}" instead`);
    } else {
      return { ok: /** @type {API.DID<Method>} */ (did) };
    }
  }
}

const schemaBytes = new DIDBytesSchema();

const didBytes = () => schemaBytes;
/**
 *
 * @param {unknown} input
 */
const readBytes = (input) => schemaBytes.read(input);

/**
 * @template {string} Method
 * @param {{method?: Method}} options
 */
const matchBytes = (options = {}) =>
  /** @type {Schema.Schema<API.DID<Method> & API.URI<"did:">>} */ (
    new DIDBytesSchema(options.method)
  );

/**
 * Create a DID string from any input (or throw)
 * @param {unknown} input
 */
const fromBytes$4 = (input) => matchBytes({}).from(input);

var DID = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  did: did,
  didBytes: didBytes,
  from: from$c,
  fromBytes: fromBytes$4,
  match: match$1,
  matchBytes: matchBytes,
  read: read$4,
  readBytes: readBytes,
});

const schema = string();

/**
 * @param {{pattern: RegExp}} [options]
 */
const match = (options) =>
  options ? schema.refine(new Match$1(options.pattern)) : schema;

/**
 * @extends {Schema.API<string, string, RegExp>}
 */
let Match$1 = class Match extends API {
  /**
   * @param {string} source
   * @param {RegExp} pattern
   */
  readWith(source, pattern) {
    if (!pattern.test(source)) {
      return error(`Expected to match ${pattern} but got "${source}" instead`);
    } else {
      return { ok: source };
    }
  }
};

/**
 *
 * @param {API.Proof} proof
 * @return {proof is API.Delegation}
 */
const isDelegation = (proof) => !isLink(proof);

/**
 * Takes one or more delegations and returns all delegated capabilities in
 * UCAN 0.10 format, expanding all the special forms like `with: ucan:*` and
 * `can: *` to explicit forms.
 *
 * Note that this function only considers included proofs and ignores linked
 * proofs. It is up to the user of this function to resolve whatever proofs it
 * needs and build delegation with them before calling this function.
 *
 * Also note that this function does not validate the delegations and may
 * produce result containing capabilities that escalate, which for the validator
 * perspective is no different from not including such capabilities.
 *
 * @template {[API.Delegation, ...API.Delegation[]]} T
 * @param {T} delegations
 * @returns {API.InferAllowedFromDelegations<T>}
 */
const allows = (...delegations) => {
  /** @type {API.Allows} */
  let allow = {};
  for (const delegation of delegations) {
    for (const { with: uri, can, nb } of iterateCapabilities(delegation)) {
      const resource = allow[uri] || (allow[uri] = {});
      const abilities = resource[can] || (resource[can] = []);
      abilities.push({ ...nb });
    }
  }

  return /** @type {API.InferAllowedFromDelegations<T>} */ (allow);
};

/**
 * Function takes a delegation and iterates over all the capabilities expanding
 * all the special forms like `with: ucan:*` and `can: *`.
 *
 * Note that this function only considers proofs that are included in the
 * delegation, linked proofs will not be resolved nor considered. It is up to
 * the user of this function to resolve whatever proofs it needs to consider
 * before calling this function.
 *
 * @param {API.Delegation} delegation
 * @returns {Iterable<API.Capability>}
 */
const iterateCapabilities = function* ({ issuer, capabilities, proofs }) {
  for (const own of capabilities) {
    // If `with` field is set to  `ucan:*` it implies re-delegation of all own
    // and delegated capabilities.
    if (own.with === "ucan:*") {
      // Fist we include own capabilities. Note that we can not expand `can`
      // because it implicitly covers all possible options in the universe.
      yield {
        ...own,
        with: issuer.did(),
      };

      // Next we iterate over all delegated capabilities including ones that
      // match ability in the `own.can` field.
      for (const proof of proofs) {
        // We only consider proofs that are included and ignore linked proofs.
        if (isDelegation(proof)) {
          for (const capability of iterateCapabilities(proof)) {
            // We attempt to match `capability.can` against `own.can` field
            // if there is a match we include the capability otherwise we skip
            const can = matchAbility(capability.can, own.can);
            if (can) {
              yield {
                ...capability,
                can,
                // We do not know capability semantics so it is impossible
                // for us to eliminate capabilities that do not satisfy imposed
                // caveats (`own.nb`). Therefore we optimistically assume that
                // `own.nb` further constraints `capability.nb` and do a shallow
                // merge of the two. As a result we may include capabilities
                // that during validation will be considered invalid due to
                // constraint violations. While that is not ideal validator
                // will treat them as if they were omitted and therefore it
                // is a reasonable compromise.
                nb: { ...capability.nb, ...Object(own.nb) },
              };
            }
          }
        }
      }
    } else {
      yield own;
    }
  }
};

/**
 * Function takes `can` field from the delegated capability and attempts to
 * match it against `can` field of the claimed capability. If there is a match
 * the function returns more specific `can` field of two, otherwise it returns
 * `null`.
 *
 * @param {API.Ability} provided
 * @param {API.Ability} claimed
 */
const matchAbility = (provided, claimed) => {
  // If provided capability delegates all abilities we can derive any `can`
  // from it so we return `claimed` as is.
  if (provided === "*") {
    return claimed;
  }
  // If claimed capability delegates all abilities that includes any `can`
  // so we return `provided` as is.
  if (claimed === "*") {
    return provided;
  }
  // If claimed `can` is a pattern that includes `provided` `can` we return
  // `provided` as is.
  if (claimed.endsWith("/*") && provided.startsWith(claimed.slice(0, -1))) {
    return provided;
  }
  // If provided `can` is a pattern that includes `claimed` `can` we can derive
  // `claimed` from it so we return `claimed` as is.
  if (provided.endsWith("/*") && claimed.startsWith(provided.slice(0, -1))) {
    return claimed;
  }
  // If `can` fields are concrete and the same we have a match and can return it.
  if (provided === claimed) {
    return provided;
  }
  // otherwise two are incompatible and we return null.
  return null;
};

/**
 * Represents UCAN chain view over the set of DAG UCAN nodes. You can think of
 * this as UCAN interface of the CAR.
 *
 * @template {API.Capabilities} C
 * @implements {API.Delegation<C>}
 * @extends {DelegationView<C>}
 */
class Delegation {
  /**
   * @param {API.UCANBlock<C>} root
   * @param {DAG.BlockStore} [blocks]
   */
  constructor(root, blocks = new Map()) {
    this.root = root;
    this.blocks = blocks;

    Object.defineProperties(this, {
      blocks: {
        enumerable: false,
      },
    });
  }

  /**
   * @returns {API.AttachedLinkSet}
   */
  get attachedLinks() {
    const _attachedLinks = new Set();
    const ucanView = this.data;

    // Get links from capabilities nb
    for (const capability of ucanView.capabilities) {
      /** @type {Link[]} */
      const links = getLinksFromObject(capability);

      for (const link of links) {
        _attachedLinks.add(`${link}`);
      }
    }

    // Get links from facts values
    for (const fact of ucanView.facts) {
      if (isLink(fact)) {
        _attachedLinks.add(`${fact}`);
      } else {
        /** @type {Link[]} */
        // @ts-expect-error isLink does not infer value type
        const links = Object.values(fact).filter((e) => isLink(e));

        for (const link of links) {
          _attachedLinks.add(`${link}`);
        }
      }
    }

    return _attachedLinks;
  }

  get version() {
    return this.data.version;
  }
  get signature() {
    return this.data.signature;
  }
  get cid() {
    return this.root.cid;
  }
  link() {
    return this.root.cid;
  }
  get asCID() {
    return this.cid;
  }
  get bytes() {
    return this.root.bytes;
  }
  get data() {
    const data = decode$n(this.root);
    Object.defineProperties(this, { data: { value: data, enumerable: false } });
    return data;
  }
  /**
   * Attach a block to the delegation DAG so it would be included in the
   * block iterator.
   * ⚠️ You can only attach blocks that are referenced from the `capabilities`
   * or `facts`.
   *
   * @param {API.Block} block
   */
  attach(block) {
    if (!this.attachedLinks.has(`${block.cid.link()}`)) {
      throw new Error(`given block with ${block.cid} is not an attached link`);
    }
    this.blocks.set(`${block.cid}`, block);
  }
  export() {
    return exportDAG(this.root, this.blocks, this.attachedLinks);
  }

  /**
   * @returns {API.Await<API.Result<Uint8Array, Error>>}
   */
  archive() {
    return archive$2(this);
  }

  iterateIPLDBlocks() {
    return exportDAG(this.root, this.blocks, this.attachedLinks);
  }

  /**
   * @type {API.Proof[]}
   */
  get proofs() {
    return proofs(this);
  }

  /**
   * @type {API.Principal}
   */
  get issuer() {
    return this.data.issuer;
  }

  /**
   * @type {API.Principal}
   */
  get audience() {
    return this.data.audience;
  }

  /**
   * @returns {C}
   */
  get capabilities() {
    return /** @type {C} */ (this.data.capabilities);
  }

  /**
   * @returns {number}
   */
  get expiration() {
    return this.data.expiration;
  }

  /**
   * @returns {undefined|number}
   */
  get notBefore() {
    return this.data.notBefore;
  }

  /**
   * @returns {undefined|string}
   */

  get nonce() {
    return this.data.nonce;
  }

  /**
   * @returns {API.Fact[]}
   */
  get facts() {
    return this.data.facts;
  }

  /**
   * Iterate over the proofs
   *
   * @returns {IterableIterator<API.Delegation>}
   */
  iterate() {
    return it(this);
  }

  delegate() {
    return this;
  }

  buildIPLDView() {
    return this;
  }

  /**
   * @returns {API.DelegationJSON<this>}
   */
  toJSON() {
    return /** @type {any} */ ({
      ...this.data.toJSON(),
      "/": this.cid.toString(),
      prf: this.proofs.map((proof) =>
        isDelegation(proof) ? proof : { "/": proof.toString() }
      ),
    });
  }
}

/**
 * Writes given `Delegation` chain into a content addressed archive (CAR)
 * buffer and returns it.
 *
 * @param {API.Delegation} delegation}
 * @returns {Promise<API.Result<Uint8Array, Error>>}
 */
const archive$2 = async (delegation) => {
  try {
    // Iterate over all of the blocks in the DAG and add them to the
    // block store.
    const store = new Map();
    for (const block of delegation.iterateIPLDBlocks()) {
      store.set(`${block.cid}`, block);
    }

    // Then we we create a descriptor block to describe what this DAG represents
    // and it to the block store as well.
    const variant = await write$5({
      [`ucan@${delegation.version}`]: delegation.root.cid,
    });
    store.set(`${variant.cid}`, variant);

    // And finally we encode the whole thing into a CAR.
    const bytes = encode$h({
      roots: [variant],
      blocks: store,
    });

    return ok(bytes);
  } catch (cause) {
    return error$1(/** @type {Error} */ (cause));
  }
};

const ArchiveSchema = variant({
  "ucan@0.9.1": /** @type {Schema.Schema<API.UCANLink>} */ (
    match$2({ version: 1 })
  ),
});

/**
 * Extracts a `Delegation` chain from a given content addressed archive (CAR)
 * buffer. Assumes that the CAR contains a single root block corresponding to
 * the delegation variant.
 *
 * @param {Uint8Array} archive
 */
const extract$1 = async (archive) => {
  try {
    const { roots, blocks } = decode$o(archive);
    const [root] = roots;
    if (root == null) {
      return error("CAR archive does not contain a root block");
    }
    const { bytes } = root;
    const variant = decode$y(bytes);
    const [, link] = ArchiveSchema.match(variant);
    return ok(view$3({ root: link, blocks }));
  } catch (cause) {
    return error$1(/** @type {Error} */ (cause));
  }
};

/**
 * @param {API.Delegation} delegation
 * @returns {IterableIterator<API.Delegation>}
 */
const it = function* (delegation) {
  for (const proof of delegation.proofs) {
    if (isDelegation(proof)) {
      yield* it(proof);
      yield proof;
    }
  }
};

const decodeCache = new WeakMap();
/**
 * @template {API.Capabilities} C
 * @param {API.UCANBlock<C>} block
 * @returns {UCAN.View<C>}
 */
const decode$n = ({ bytes }) => {
  const data = decodeCache.get(bytes);
  if (!data) {
    const data = decode$q(bytes);
    decodeCache.set(bytes, data);
    return data;
  }
  return data;
};

/**
 * Creates a new signed token with a given `options.issuer`. If expiration is
 * not set it defaults to 30 seconds from now. Returns UCAN in primary - IPLD
 * representation.
 *
 * @template {API.Capabilities} C
 * @param {API.DelegationOptions<C>} data
 * @param {API.EncodeOptions} [options]
 * @returns {Promise<API.Delegation<C>>}
 */

const delegate$3 = async (
  { issuer, audience, proofs = [], attachedBlocks = new Map(), ...input },
  options
) => {
  const links = [];
  const blocks = new Map();
  for (const proof of proofs) {
    if (!isDelegation(proof)) {
      links.push(proof);
    } else {
      links.push(proof.cid);
      for (const block of proof.export()) {
        blocks.set(block.cid.toString(), block);
      }
    }
  }

  const data = await issue$2({
    ...input,
    issuer,
    audience,
    proofs: links,
  });
  const { cid, bytes } = await write$6(data, options);
  decodeCache.set(cid, data);

  /** @type {API.Delegation<C>} */
  const delegation = new Delegation({ cid, bytes }, blocks);
  Object.defineProperties(delegation, { proofs: { value: proofs } });

  for (const block of attachedBlocks.values()) {
    delegation.attach(block);
  }

  return delegation;
};

/**
 * @template {API.Capabilities} C
 * @param {API.UCANBlock<C>} root
 * @param {DAG.BlockStore} blocks
 * @param {API.AttachedLinkSet} attachedLinks
 * @returns {IterableIterator<API.Block>}
 */

const exportDAG = function* (root, blocks, attachedLinks) {
  for (const link of decode$n(root).proofs) {
    // Check if block is included in this delegation
    const root = /** @type {UCAN.Block} */ (blocks.get(`${link}`));
    if (root) {
      yield* exportSubDAG(root, blocks);
    }
  }

  for (const link of attachedLinks.values()) {
    const block = blocks.get(link);

    if (block) {
      // @ts-expect-error can get blocks with v0 and v1
      yield block;
    }
  }

  yield root;
};

/**
 * @template {API.Capabilities} C
 * @param {API.UCANBlock<C>} root
 * @param {DAG.BlockStore} blocks
 * @returns {IterableIterator<API.Block>}
 */
const exportSubDAG = function* (root, blocks) {
  for (const link of decode$n(root).proofs) {
    // Check if block is included in this delegation
    const root = /** @type {UCAN.Block} */ (blocks.get(`${link}`));
    if (root) {
      yield* exportSubDAG(root, blocks);
    }
  }

  yield root;
};

/**
 * @template {API.Capabilities} C
 * @param {Iterable<API.Block>} dag
 * @returns {API.Delegation<C>}
 */
const importDAG = (dag) => {
  /** @type {Array<[string, API.Block]>} */
  let entries = [];
  for (const block of dag) {
    entries.push([block.cid.toString(), block]);
  }

  const last = entries.pop();
  if (!last) {
    throw new RangeError("Empty DAG can not be turned into a delegation");
  } else {
    const [, root] = last;

    return new Delegation(
      /** @type {API.UCANBlock<C>} */ (root),
      new Map(entries)
    );
  }
};

/**
 * @template {API.Capabilities} C
 * @param {object} dag
 * @param {API.UCANBlock<C>} dag.root
 * @param {DAG.BlockStore} [dag.blocks]
 * @returns {API.Delegation<C>}
 */
const create$e = ({ root, blocks }) => new Delegation(root, blocks);

/**
 * @template {API.Capabilities} C
 * @template [E=never]
 * @param {object} dag
 * @param {API.UCANLink<C>} dag.root
 * @param {DAG.BlockStore} dag.blocks
 * @param {E} [fallback]
 * @returns {API.Delegation<C>|E}
 */
const view$3 = ({ root, blocks }, fallback) => {
  const block = get$i(root, blocks, null);
  if (block == null) {
    return fallback !== undefined ? fallback : notFound(root);
  }
  return create$e({ root: block, blocks });
};

/**
 * @param {API.Delegation} delegation
 */
const proofs = (delegation) => {
  /** @type {API.Proof[]} */
  const proofs = [];
  const { root, blocks } = delegation;
  // Iterate over proof links and materialize Delegation views.
  for (const link of decode$n(root).proofs) {
    // Check if linked proof is included in our blocks if so create delegation
    // view otherwise use a link
    const root = /** @type {UCAN.Block} */ (blocks.get(link.toString()));
    proofs.push(root ? create$e({ root, blocks }) : link);
  }

  // we cache result of this computation as this property may get accessed
  // more than once.
  Object.defineProperty(delegation, "proofs", { value: proofs });
  return proofs;
};

/**
 * @param {API.Capability<API.Ability, `${string}:${string}`, unknown>} obj
 */
function getLinksFromObject(obj) {
  /** @type {Link[]} */
  const links = [];

  /**
   * @param {object} obj
   */
  function recurse(obj) {
    for (const key in obj) {
      // @ts-expect-error record type not inferred
      const value = obj[key];
      if (isLink(value)) {
        // @ts-expect-error isLink does not infer value type
        links.push(value);
      } else if (value && typeof value === "object") {
        recurse(value);
      }
    }
  }

  recurse(obj);

  return links;
}

/**
 * @template {API.Capability} Capability
 * @param {API.InvocationOptions<Capability>} options
 * @return {API.IssuedInvocationView<Capability>}
 */
const invoke = (options) => new IssuedInvocation(options);

/**
 * @template {API.Capability} C
 * @param {object} dag
 * @param {API.UCANBlock<[C]>} dag.root
 * @param {DAG.BlockStore} [dag.blocks]
 * @returns {API.Invocation<C>}
 */
const create$d = ({ root, blocks }) => new Invocation(root, blocks);

/**
 * Takes a link of the `root` block and a map of blocks and constructs an
 * `Invocation` from it. If `root` is not included in the provided blocks
 * provided fallback is returned and if not provided than throws an error.
 * If root points to wrong block (that is not an invocation) it will misbehave
 * and likely throw some errors on field access.
 *
 * @template {API.Capability} C
 * @template {API.Invocation} Invocation
 * @template [T=never]
 * @param {object} dag
 * @param {API.UCANLink<[C]>} dag.root
 * @param {DAG.BlockStore} dag.blocks
 * @param {T} [fallback]
 * @returns {API.Invocation<C>|T}
 */
const view$2 = ({ root, blocks }, fallback) => {
  const block = get$i(root, blocks, null);
  if (block == null) {
    return fallback !== undefined ? fallback : notFound(root);
  }

  return /** @type {API.Invocation<C>} */ (create$d({ root: block, blocks }));
};

/**
 * @template {API.Capability} Capability
 * @implements {API.IssuedInvocationView<Capability>}
 * @implements {API.IssuedInvocation<Capability>}
 */
class IssuedInvocation {
  /**
   * @param {API.InvocationOptions<Capability>} data
   */
  constructor({
    issuer,
    audience,
    capability,
    proofs = [],
    expiration,
    lifetimeInSeconds,
    notBefore,
    nonce,
    facts = [],
  }) {
    /** @readonly */
    this.issuer = issuer;
    /** @readonly */
    this.audience = audience;
    /** @readonly */
    this.proofs = proofs;

    /**
     * @readonly
     * @type {[Capability]}
     */
    this.capabilities = [capability];

    this.expiration = expiration;
    this.lifetimeInSeconds = lifetimeInSeconds;
    this.notBefore = notBefore;
    this.nonce = nonce;
    this.facts = facts;

    /** @type {API.BlockStore<unknown>} */
    this.attachedBlocks = new Map();
  }

  /**
   * @param {API.Block} block
   */
  attach(block) {
    this.attachedBlocks.set(`${block.cid}`, block);
  }

  delegate() {
    return delegate$3(this);
  }

  buildIPLDView() {
    return delegate$3(this);
  }

  /**
   * @template {API.InvocationService<Capability>} Service
   * @param {API.ConnectionView<Service>} connection
   * @returns {Promise<API.InferReceipt<Capability, Service>>}
   */
  async execute(connection) {
    /** @type {API.ServiceInvocation<Capability, Service>} */
    // @ts-expect-error - Our `API.InvocationService<Capability>` constraint
    // does not seem to be enough to convince TS that `this` is valid
    // `ServiceInvocations<Service>`.
    const invocation = this;
    const [result] = await connection.execute(invocation);
    return result;
  }
}

/**
 * @template {API.Capability} Capability
 * @implements {API.Invocation<Capability>}
 * @extends {Delegation<[Capability]>}
 */
class Invocation extends Delegation {}

/**
 * @template {{}} Ok
 * @template {{}} Error
 * @template {API.Invocation} Ran
 * @template [E=never]
 * @param {object} input
 * @param {API.Link<API.ReceiptModel<Ok, Error, Ran>>} input.root
 * @param {DAG.BlockStore} input.blocks
 * @param {E} [fallback]
 */
const view$1 = ({ root, blocks }, fallback) => {
  const block = get$i(root, blocks, null);
  if (block == null) {
    return fallback !== undefined ? fallback : notFound(root);
  }
  const data = decode$y(block.bytes);

  return new Receipt({ root: { ...block, data }, store: blocks });
};

/**
 * Represents a UCAN invocation receipt view over some block store e.g. in
 * memory CAR. It incrementally decodes proofs, ran invocation etc. on access
 * which reduces overhead but potentially defers errors if references blocks
 * do not conform to the expected IPLD schema.
 *
 * @template {{}} Ok
 * @template {{}} Error
 * @template {API.Invocation} Ran
 * @template {API.SigAlg} [SigAlg=API.SigAlg]
 * @implements {API.Receipt<Ok, Error, Ran, SigAlg>}
 */
class Receipt {
  /**
   * @param {object} input
   * @param {Required<API.Block<API.ReceiptModel<Ok, Error, Ran>>>} input.root
   * @param {DAG.BlockStore} input.store
   * @param {API.Meta} [input.meta]
   * @param {Ran|ReturnType<Ran['link']>} [input.ran]
   * @param {API.EffectsModel} [input.fx]
   * @param {API.SignatureView<API.OutcomeModel<Ok, Error, Ran>, SigAlg>} [input.signature]
   * @param {API.UCAN.Principal} [input.issuer]
   * @param {API.Proof[]} [input.proofs]
   */
  constructor({ root, store, ran, issuer, signature, proofs }) {
    this.store = store;

    this.root = root;
    this._ran = ran;

    // Field is materialized on demand when `fx` getter is first accessed.
    /** @type {API.Effects|undefined} */
    this._fx = undefined;
    this._signature = signature;
    this._proofs = proofs;
    this._issuer = issuer;
  }

  /**
   * @returns {Ran|ReturnType<Ran['link']>}
   */
  get ran() {
    const ran = this._ran;
    if (!ran) {
      const ran = /** @type {Ran} */ (
        view$2(
          {
            root: this.root.data.ocm.ran,
            blocks: this.store,
          },
          this.root.data.ocm.ran
        )
      );
      this._ran = ran;
      return ran;
    } else {
      return ran;
    }
  }
  get proofs() {
    const proofs = this._proofs;
    if (proofs) {
      return proofs;
    } else {
      const { store, root } = this;
      const { prf } = root.data.ocm;
      const proofs = [];
      if (prf) {
        for (const link of prf) {
          const proof = view$3({ root: link, blocks: store }, link);
          proofs.push(proof);
        }
      }

      this._proofs = proofs;
      return proofs;
    }
  }
  link() {
    return this.root.cid;
  }
  get meta() {
    return this.root.data.ocm.meta;
  }
  get issuer() {
    const issuer = this._issuer;
    if (issuer) {
      return issuer;
    } else {
      const { iss } = this.root.data.ocm;
      if (iss) {
        const issuer = parse$1(iss);
        this._issuer = issuer;
        return issuer;
      }
    }
  }

  get out() {
    return this.root.data.ocm.out;
  }

  get fx() {
    let fx = this._fx;
    if (!fx) {
      const { store: blocks } = this;
      const { fork, join } = this.root.data.ocm.fx;

      fx = {
        fork: fork.map((root) => view$2({ root, blocks }, root)),
      };

      if (join) {
        fx.join = view$2({ root: join, blocks }, join);
      }

      this._fx = fx;
    }
    return fx;
  }

  get signature() {
    const signature = this._signature;
    if (signature) {
      return signature;
    } else {
      const signature =
        /** @type {API.SignatureView<API.OutcomeModel<Ok, Error, Ran>, SigAlg>} */ (
          view$4(this.root.data.sig)
        );
      this._signature = signature;
      return signature;
    }
  }

  /**
   * @param {API.Crypto.Verifier} signingPrincipal
   */
  verifySignature(signingPrincipal) {
    return this.signature.verify(
      signingPrincipal,
      encode$i(this.root.data.ocm)
    );
  }

  buildIPLDView() {
    return this;
  }

  *iterateIPLDBlocks() {
    const { ran, fx, proofs, root } = this;

    yield* iterate$2(ran);

    for (const fork of fx.fork) {
      yield* iterate$2(fork);
    }

    if (fx.join) {
      yield* iterate$2(fx.join);
    }

    for (const proof of proofs) {
      yield* iterate$2(proof);
    }

    yield root;
  }
}

/**
 * Represents a receipt builder that can be used to create a receipt that later
 * can be encoded into desired IPLD codec and hasher. In the future we may make
 * this an incremental builder so you could set some fields later on.
 *
 * @template {{}} Ok
 * @template {{}} Error
 * @template {API.Invocation} Ran
 * @template {API.SigAlg} SigAlg
 * @implements {API.IPLDViewBuilder<API.Receipt<Ok, Error, Ran, SigAlg>>}
 */
class ReceptBuilder {
  /**
   * @param {object} options
   * @param {API.Signer<API.DID, SigAlg>} options.issuer
   * @param {Ran|ReturnType<Ran['link']>} options.ran
   * @param {API.Result<Ok, Error>} options.result
   * @param {API.Effects} [options.fx]
   * @param {API.Proof[]} [options.proofs]
   * @param {Record<string, unknown>} [options.meta]
   */
  constructor({ issuer, result, ran, fx = NOFX, proofs = [], meta = {} }) {
    this.issuer = issuer;
    this.result = result;
    this.ran = ran;
    this.fx = fx;
    this.proofs = proofs;
    this.meta = meta;
  }
  async buildIPLDView({ hasher = sha256$4, codec = CBOR } = {}) {
    const store = createStore();

    // copy invocation blocks int
    addEveryInto(iterate$2(this.ran), store);

    // copy proof blocks into store
    const prf = [];
    for (const proof of this.proofs) {
      addEveryInto(iterate$2(proof), store);
      prf.push(proof.link());
    }

    // copy blocks from the embedded fx
    /** @type {{fork: API.Run[], join?:API.Run}}  */
    const fx = { fork: [] };
    for (const fork of this.fx.fork) {
      addEveryInto(iterate$2(fork), store);
      fx.fork.push(fork.link());
    }

    if (this.fx.join) {
      addEveryInto(iterate$2(this.fx.join), store);
      fx.join = this.fx.join.link();
    }

    /** @type {API.OutcomeModel<Ok, Error, Ran>} */
    const outcome = {
      ran: /** @type {ReturnType<Ran['link']>} */ (this.ran.link()),
      out: this.result,
      fx,
      meta: this.meta,
      iss: this.issuer.did(),
      prf,
    };

    const signature = await this.issuer.sign(encode$i(outcome));

    /** @type {API.ReceiptModel<Ok, Error, Ran>} */
    const model = {
      ocm: outcome,
      sig: signature,
    };
    const root = await writeInto(model, store, {
      hasher,
      codec,
    });

    return new Receipt({
      root,
      store,
      signature,
      proofs: this.proofs,
      ran: this.ran,
    });
  }
}

const NOFX = Object.freeze({ fork: Object.freeze([]) });

/**
 * Creates a receipt in CBOR with sha256 hashed links.
 *
 * @template {{}} Ok
 * @template {{}} Error
 * @template {API.Invocation} Ran
 * @template {API.SigAlg} SigAlg
 * @param {object} options
 * @param {API.Signer<API.DID, SigAlg>} options.issuer
 * @param {Ran|ReturnType<Ran['link']>} options.ran
 * @param {API.Result<Ok, Error>} options.result
 * @param {API.Effects} [options.fx]
 * @param {API.Proof[]} [options.proofs]
 * @param {Record<string, unknown>} [options.meta]
 * @returns {Promise<API.Receipt<Ok, Error, Ran, SigAlg>>}
 */
const issue$1 = (options) => new ReceptBuilder(options).buildIPLDView();

const MessageSchema = variant({
  "ucanto/message@7.0.0": struct({
    execute: match$2().array().optional(),
    delegate: dictionary({
      key: string(),
      value: /** @type {API.Reader<API.Link<API.ReceiptModel>>} */ (match$2()),
    })
      .array()
      .optional(),
  }),
});

/**
 * @template {API.Tuple<API.IssuedInvocation>} I
 * @template {API.Tuple<API.Receipt>} R
 * @param {object} source
 * @param {I} [source.invocations]
 * @param {R} [source.receipts]
 * @returns {Promise<API.AgentMessage<{ In: API.InferInvocations<I>, Out: R }>>}
 */
const build$1 = ({ invocations, receipts }) =>
  new MessageBuilder({ invocations, receipts }).buildIPLDView();

/**
 * @template [E=never]
 * @param {object} source
 * @param {API.Link} source.root
 * @param {DAG.BlockStore} source.store
 * @param {E} [fallback]
 * @returns {API.AgentMessage|E}
 */
const view = ({ root, store }, fallback) => {
  const block = get$i(root, store, null);
  if (block === null) {
    return fallback !== undefined ? fallback : notFound(root);
  }
  const data = decode$y(block.bytes);
  const [branch, value] = MessageSchema.match(data, fallback);
  switch (branch) {
    case "ucanto/message@7.0.0":
      return new Message({ root: { ...block, data }, store });
    default:
      return value;
  }
};

/**
 * @template {API.Tuple<API.IssuedInvocation>} I
 * @template {API.Tuple<API.Receipt>} R
 * @implements {API.AgentMessageBuilder<{In: API.InferInvocations<I>, Out: R }>}
 *
 */
class MessageBuilder {
  /**
   * @param {object} source
   * @param {I} [source.invocations]
   * @param {R} [source.receipts]
   */
  constructor({ invocations, receipts }) {
    this.invocations = invocations;
    this.receipts = receipts;
  }
  /**
   *
   * @param {API.BuildOptions} [options]
   * @returns {Promise<Message<{ In: API.InferInvocations<I>, Out: R }>>}
   */
  async buildIPLDView(options) {
    const store = new Map();

    const { invocations, ...executeField } = await writeInvocations(
      this.invocations || [],
      store
    );

    const { receipts, ...receiptsField } = await writeReceipts(
      this.receipts || [],
      store
    );

    const root = await writeInto(
      /** @type {API.AgentMessageModel<{ In: API.InferInvocations<I>, Out: R }>} */
      ({
        "ucanto/message@7.0.0": {
          ...executeField,
          ...receiptsField,
        },
      }),
      store,
      options
    );

    return new Message({ root, store }, { receipts, invocations });
  }
}

/**
 *
 * @param {API.IssuedInvocation[]} run
 * @param {Map<string, API.Block>} store
 */
const writeInvocations = async (run, store) => {
  const invocations = [];
  const execute = [];
  for (const invocation of run) {
    const view = await invocation.buildIPLDView();
    execute.push(view.link());
    invocations.push(view);
    for (const block of view.iterateIPLDBlocks()) {
      store.set(`${block.cid}`, block);
    }
  }

  return { invocations, ...(execute.length > 0 ? { execute } : {}) };
};

/**
 * @param {API.Receipt[]} source
 * @param {Map<string, API.Block>} store
 */
const writeReceipts = async (source, store) => {
  if (source.length === 0) {
    return {};
  }

  const receipts = new Map();
  /** @type {Record<API.ToString<API.ReceiptModel['ocm']['ran']>, API.Link<API.ReceiptModel>>} */
  const report = {};

  for (const [n, receipt] of source.entries()) {
    const view = await receipt.buildIPLDView();
    for (const block of view.iterateIPLDBlocks()) {
      store.set(`${block.cid}`, block);
    }

    const key = `${view.ran.link()}`;
    if (!(key in report)) {
      report[key] = view.root.cid;
      receipts.set(key, view);
    } else {
      // In theory we could have gotten the same invocation twice and both
      // should get same receipt. In legacy code we send tuple of results
      // as opposed to a map keyed by invocation to keep old clients working
      // we just stick the receipt in the map with a unique key so that when
      // legacy encoder maps entries to array it will get both receipts in
      // the right order.
      receipts.set(`${key}@${n}`, view);
    }
  }

  return { receipts, report };
};

/**
 * @template {{ In: API.Invocation[], Out: API.Receipt[] }} T
 * @implements {API.AgentMessage<T>}
 */
class Message {
  /**
   * @param {object} source
   * @param {Required<API.Block<API.AgentMessageModel<T>>>} source.root
   * @param {DAG.BlockStore} source.store
   * @param {object} build
   * @param {API.Invocation[]} [build.invocations]
   * @param {Map<string, API.Receipt>} [build.receipts]
   */
  constructor({ root, store }, { invocations, receipts } = {}) {
    this.root = root;
    this.store = store;
    this._invocations = invocations;
    this._receipts = receipts;
  }
  *iterateIPLDBlocks() {
    for (const invocation of this.invocations) {
      yield* invocation.iterateIPLDBlocks();
    }

    for (const receipt of this.receipts.values()) {
      yield* receipt.iterateIPLDBlocks();
    }

    yield this.root;
  }
  /**
   * @template [E=never]
   * @param {API.Link} link
   * @param {E} [fallback]
   * @returns {API.Receipt|E}
   */
  get(link, fallback) {
    const receipts = this.root.data["ucanto/message@7.0.0"].report || {};
    const receipt = receipts[`${link}`];
    if (receipt) {
      return view$1({ root: receipt, blocks: this.store });
    } else {
      return fallback !== undefined
        ? fallback
        : panic$1(`Message does not include receipt for ${link}`);
    }
  }

  get invocationLinks() {
    return this.root.data["ucanto/message@7.0.0"].execute || [];
  }

  get invocations() {
    let invocations = this._invocations;
    if (!invocations) {
      invocations = this.invocationLinks.map((link) => {
        return view$2({ root: link, blocks: this.store });
      });
    }

    return invocations;
  }

  get receipts() {
    let receipts = this._receipts;
    if (!receipts) {
      receipts = new Map();
      const report = this.root.data["ucanto/message@7.0.0"].report || {};
      for (const [key, link] of Object.entries(report)) {
        const receipt = view$1({ root: link, blocks: this.store });
        receipts.set(`${receipt.ran.link()}`, receipt);
      }
    }

    return receipts;
  }
}

/**
 * Creates a connection to a service.
 *
 * @template {Record<string, any>} T
 * @param {API.ConnectionOptions<T>} options
 * @returns {API.ConnectionView<T>}
 */
const connect = (options) => new Connection(options);

/**
 * @template {Record<string, any>} T
 * @implements {API.ConnectionView<T>}
 */
class Connection {
  /**
   * @param {API.ConnectionOptions<T>} options
   */
  constructor(options) {
    this.id = options.id;
    this.options = options;
    this.codec = options.codec;
    this.channel = options.channel;
    this.hasher = options.hasher || sha256$4;
  }
  /**
   * Execute invocations.
   *
   * @template {API.Capability} C
   * @template {API.Tuple<API.ServiceInvocation<C, T>>} I
   * @param {I} invocations
   * @returns {Promise<API.InferReceipts<I, T>>}
   */
  async execute(...invocations) {
    return execute(invocations, this);
  }
}

/**
 * @template {API.Capability} C
 * @template {Record<string, any>} T
 * @template {API.Tuple<API.ServiceInvocation<C, T>>} I
 * @param {API.Connection<T>} connection
 * @param {I} invocations
 * @returns {Promise<API.InferReceipts<I, T>>}
 */
const execute = async (invocations, connection) => {
  const input = await build$1({ invocations });
  const request = await connection.codec.encode(input, connection);
  const response = await connection.channel.request(request);
  // We may fail to decode the response if content type is not supported
  // or if data was corrupted. We do not want to throw in such case however,
  // because client will get an Error object as opposed to a receipt, to retain
  // consistent client API with two kinds of errors we encode caught error as
  // a receipts per workflow invocation.
  try {
    const output = await connection.codec.decode(response);
    const receipts = input.invocationLinks.map((link) => output.get(link));
    return /** @type {API.InferReceipts<I, T>} */ (receipts);
  } catch (error) {
    // No third party code is run during decode and we know
    // we only throw an Error
    const { message, name = "Error", ...cause } = /** @type {Error} */ (error);
    const receipts = [];
    for await (const ran of input.invocationLinks) {
      const receipt = await issue$1({
        ran,
        result: { error: { ...cause, name, message } },
        // @ts-expect-error - we can not really sign a receipt without having
        // an access to a signer which client does not have. In the future
        // we will change client API requiring a signer to be passed in but
        // for now we just use a dummy signer.
        issuer: {
          did() {
            return connection.id.did();
          },
          sign() {
            return createNonStandard("", new Uint8Array());
          },
        },
      });

      receipts.push(receipt);
    }

    return /** @type {API.InferReceipts<I, T>} */ (receipts);
  }
};

const contentType$5 = contentType$6;

const HEADERS$2 = Object.freeze({
  "content-type": contentType$5,
  // We will signal that we want to receive a CAR file in the response
  accept: contentType$5,
});

/**
 * Encodes `AgentMessage` into an `HTTPRequest`.
 *
 * @template {API.AgentMessage} Message
 * @param {Message} message
 * @param {API.EncodeOptions & { headers?: Record<string, string> }} [options]
 * @returns {API.HTTPRequest<Message>}
 */
const encode$g = (message, options) => {
  const blocks = new Map();
  for (const block of message.iterateIPLDBlocks()) {
    blocks.set(`${block.cid}`, block);
  }

  /**
   * Cast to Uint8Array to remove phantom type set by the
   * CAR encoder which is too specific.
   *
   * @type {Uint8Array}
   */
  const body = encode$h({
    roots: [message.root],
    blocks,
  });

  return {
    headers: options?.headers || { ...HEADERS$2 },
    body,
  };
};

/**
 * Decodes `AgentMessage` from the received `HTTPRequest`.
 *
 * @template {API.AgentMessage} Message
 * @param {API.HTTPRequest<Message>} request
 * @returns {Promise<Message>}
 */
const decode$m = async ({ headers, body }) => {
  const { roots, blocks } = decode$o(/** @type {Uint8Array} */ (body));
  const message = view({ root: roots[0].cid, store: blocks });
  return /** @type {Message} */ (message);
};

var request$3 = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  codec: car,
  contentType: contentType$5,
  decode: decode$m,
  encode: encode$g,
});

const contentType$4 = contentType$6;

const HEADERS$1 = Object.freeze({
  "content-type": contentType$4,
});

/**
 * Encodes `AgentMessage` into an `HTTPRequest`.
 *
 * @template {API.AgentMessage} Message
 * @param {Message} message
 * @param {API.EncodeOptions} [options]
 * @returns {API.HTTPResponse<Message>}
 */
const encode$f = (message, options) => {
  const blocks = new Map();
  for (const block of message.iterateIPLDBlocks()) {
    blocks.set(`${block.cid}`, block);
  }

  /**
   * Cast to Uint8Array to remove phantom type set by the
   * CAR encoder which is too specific.
   *
   * @type {Uint8Array}
   */
  const body = encode$h({
    roots: [message.root],
    blocks,
  });

  return {
    headers: { ...HEADERS$1 },
    body,
  };
};

/**
 * Decodes `AgentMessage` from the received `HTTPResponse`.
 *
 * @template {API.AgentMessage} Message
 * @param {API.HTTPResponse<Message>} response
 * @returns {Promise<Message>}
 */
const decode$l = async ({ headers, body }) => {
  const { roots, blocks } = decode$o(/** @type {Uint8Array} */ (body));
  const message = view({ root: roots[0].cid, store: blocks });
  return /** @type {Message} */ (message);
};

var response$1 = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  codec: car,
  contentType: contentType$4,
  decode: decode$l,
  encode: encode$f,
});

/**
 * @typedef {`${Lowercase<string>}/${Lowercase<string>}`|`${Lowercase<string>}/${Lowercase<string>}+${Lowercase<string>}`} ContentType
 * @typedef {`${Lowercase<string>}/${Lowercase<string>}`|`${Lowercase<string>}/${Lowercase<string>};q=${number}.${number}`} MediaType
 * @param {object} source
 * @param {Record<ContentType, API.Transport.RequestDecoder>} source.decoders
 * @param {Record<MediaType, API.Transport.ResponseEncoder>} source.encoders
 * @returns {API.InboundCodec}
 */
const inbound = (source) => new Inbound(source);

/**
 * @implements {API.InboundCodec}
 */
class Inbound {
  /**
   * @param {API.HTTPRequest} request
   * @returns {API.Result<API.InboundAcceptCodec, API.HTTPError>} transport
   */
  accept({ headers }) {
    const contentType = headers["content-type"] || headers["Content-Type"];
    const decoder = this.decoders[contentType];
    if (!decoder) {
      return {
        error: {
          status: 415,
          message: `The server cannot process the request because the payload format is not supported. Please check the content-type header and try again with a supported media type.`,
          headers: {
            accept: Object.keys(this.decoders).join(", "),
          },
        },
      };
    }

    const accept = parseAcceptHeader(headers.accept || headers.Accept || "*/*");
    for (const { category, type } of accept) {
      for (const encoder of this.encoders) {
        const select =
          (category === "*" || category === encoder.category) &&
          (type === "*" || type === encoder.type);

        if (select) {
          return { ok: { ...encoder, decoder } };
        }
      }
    }

    return {
      error: {
        status: 406,
        message: `The requested resource cannot be served in the requested content type. Please specify a supported content type using the Accept header.`,
        headers: {
          accept: formatAcceptHeader(Object.values(this.encoders)),
        },
      },
    };
  }

  /**
   * @param {object} source
   * @param {Record<string, API.Transport.RequestDecoder>} source.decoders
   * @param {Record<string, API.Transport.ResponseEncoder>} source.encoders
   */
  constructor({ decoders = {}, encoders = {} }) {
    this.decoders = decoders;

    if (Object.keys(decoders).length === 0) {
      throw new Error("At least one decoder MUST be provided");
    }

    // We sort the encoders by preference, so that we can pick the most
    // preferred one when client accepts multiple content types.
    this.encoders = Object.entries(encoders)
      .map(([mediaType, encoder]) => {
        return { ...parseMediaType(mediaType), encoder };
      })
      .sort((a, b) => b.preference - a.preference);

    if (this.encoders.length === 0) {
      throw new Error("At least one encoder MUST be provided");
    }
  }
}

/**
 * @param {object} source
 * @param {Record<MediaType, API.Transport.RequestEncoder>} source.encoders
 * @param {Record<ContentType, API.Transport.ResponseDecoder>} source.decoders
 * @returns {API.OutboundCodec}
 */
const outbound$1 = (source) => new Outbound(source);

/**
 * @implements {API.OutboundCodec}
 */
class Outbound {
  /**
   * @param {object} source
   * @param {Record<string, API.Transport.RequestEncoder>} source.encoders
   * @param {Record<string, API.Transport.ResponseDecoder>} source.decoders
   */
  constructor({ decoders = {}, encoders = {} }) {
    this.decoders = decoders;

    if (Object.keys(decoders).length === 0) {
      throw new Error("At least one decoder MUST be provided");
    }

    // We sort the encoders by preference, so that we can pick the most
    // preferred one when client accepts multiple content types.
    this.encoders = Object.entries(encoders)
      .map(([mediaType, encoder]) => {
        return { ...parseMediaType(mediaType), encoder };
      })
      .sort((a, b) => b.preference - a.preference);

    this.acceptType = formatAcceptHeader(this.encoders);

    if (this.encoders.length === 0) {
      throw new Error("At least one encoder MUST be provided");
    }

    this.encoder = this.encoders[0].encoder;
  }

  /**
   * @template {API.AgentMessage} Message
   * @param {Message} message
   */
  encode(message) {
    return this.encoder.encode(message, {
      accept: this.acceptType,
    });
  }
  /**
   * @template {API.AgentMessage} Message
   * @param {API.HTTPResponse<Message>} response
   * @returns {API.Await<Message>}
   */
  decode(response) {
    const { headers } = response;
    const contentType = headers["content-type"] || headers["Content-Type"];
    const decoder = this.decoders[contentType] || this.decoders["*/*"];
    switch (response.status) {
      case 415:
      case 406:
        throw Object.assign(
          new RangeError(new TextDecoder().decode(response.body)),
          {
            status: response.status,
            headers: response.headers,
          }
        );
    }
    if (!decoder) {
      throw Object.assign(
        TypeError(
          `Can not decode response with content-type '${contentType}' because no matching transport decoder is configured.`
        ),
        {
          error: true,
        }
      );
    }

    return decoder.decode(response);
  }
}

/**
 * @typedef {{ category: string, type: string, preference: number }} Media
 * @param {string} source
 * @returns {Media}
 */
const parseMediaType = (source) => {
  const [mediaType = "*/*", mediaRange = ""] = source.trim().split(";");
  const [category = "*", type = "*"] = mediaType.split("/");
  const params = new URLSearchParams(mediaRange);
  const preference = parseFloat(params.get("q") || "0");
  return {
    category,
    type,
    /* c8 ignore next */
    preference: isNaN(preference) ? 0 : preference,
  };
};

/**
 * @param {Media} media
 */
const formatMediaType = ({ category, type, preference }) =>
  /** @type {MediaType}  */ (
    `${category}/${type}${preference ? `;q=${preference}` : ""}`
  );

/**
 * @param {string} source
 */
const parseAcceptHeader = (source) =>
  source
    .split(",")
    .map(parseMediaType)
    .sort((a, b) => b.preference - a.preference);

/**
 * @param {Media[]} source
 */
const formatAcceptHeader = (source) => source.map(formatMediaType).join(", ");

const contentType$3 = contentType$6;

inbound({
  decoders: {
    [contentType$5]: request$3,
  },
  encoders: {
    [contentType$4]: response$1,
  },
});

const outbound = outbound$1({
  encoders: {
    [contentType$5]: request$3,
  },
  decoders: {
    [contentType$4]: response$1,
  },
});

/**
 * @typedef {{
 * ok: boolean
 * arrayBuffer():API.Await<ArrayBuffer>
 * headers: {
 *  entries?: () => Iterable<[string, string]>
 * } | Headers
 * status?: number
 * statusText?: string
 * url?: string
 * }} FetchResponse
 * @typedef {(url:string, init:API.HTTPRequest) => API.Await<FetchResponse>} Fetcher
 */
/**
 * @template S
 * @param {object} options
 * @param {URL} options.url
 * @param {(url:string, init:API.HTTPRequest) => API.Await<FetchResponse>} [options.fetch]
 * @param {string} [options.method]
 * @returns {API.Channel<S>}
 */
const open$2 = ({ url, method = "POST", fetch }) => {
  /* c8 ignore next 9 */
  if (!fetch) {
    if (typeof globalThis.fetch !== "undefined") {
      fetch = globalThis.fetch.bind(globalThis);
    } else {
      throw new TypeError(
        `ucanto HTTP transport got undefined \`fetch\`. Try passing in a \`fetch\` implementation explicitly.`
      );
    }
  }
  return new Channel({ url, method, fetch });
};

/**
 * @template {Record<string, any>} S
 * @implements {API.Channel<S>}
 */
class Channel {
  /**
   * @param {object} options
   * @param {URL} options.url
   * @param {Fetcher} options.fetch
   * @param {string} [options.method]
   */
  constructor({ url, fetch, method }) {
    this.fetch = fetch;
    this.method = method;
    this.url = url;
  }
  /**
   * @template {API.Tuple<API.ServiceInvocation<API.Capability, S>>} I
   * @param {API.HTTPRequest<API.AgentMessage<{ In: API.InferInvocations<I>, Out: API.Tuple<API.Receipt> }>>} request
   * @returns {Promise<API.HTTPResponse<API.AgentMessage<{ Out: API.InferReceipts<I, S>, In: API.Tuple<API.Invocation> }>>>}
   */
  async request({ headers, body }) {
    const response = await this.fetch(this.url.href, {
      headers,
      body,
      method: this.method,
    });

    const buffer = response.ok
      ? await response.arrayBuffer()
      : HTTPError.throw(
          `HTTP Request failed. ${this.method} ${this.url.href} → ${response.status}`,
          response
        );

    return {
      headers: response.headers.entries
        ? Object.fromEntries(response.headers.entries())
        : /* c8 ignore next */
          {},
      body: new Uint8Array(buffer),
    };
  }
}

/**
 * @typedef {{
 * status?: number
 * statusText?: string
 * url?: string
 * }} Options
 */
class HTTPError extends Error {
  /**
   * @param {string} message
   * @param {Options} options
   * @returns {never}
   */
  static throw(message, options) {
    throw new this(message, options);
  }
  /**
   * @param {string} message
   * @param {Options} options
   */
  constructor(message, { url, status = 500, statusText = "Server error" }) {
    super(message);
    /** @type {'HTTPError'} */
    this.name = "HTTPError";
    this.url = url;
    this.status = status;
    this.statusText = statusText;
  }
}

/**
 * @template {string|boolean|number|[unknown, ...unknown[]]} T
 * @param {T} value
 * @returns {T}
 */
const the = (value) => value;

/**
 * @template {{}} O
 * @param {O} object
 * @returns {({ [K in keyof O]: [K, O[K]][] }[keyof O])|[[never, never]]}
 */

const entries$1 = (object) => /** @type {any} */ (Object.entries(object));

/**
 * @template T
 * @param {T[][]} dataset
 * @returns {T[][]}
 */
const combine = ([first, ...rest]) => {
  const results = first.map((value) => [value]);
  for (const values of rest) {
    const tuples = results.splice(0);
    for (const value of values) {
      for (const tuple of tuples) {
        results.push([...tuple, value]);
      }
    }
  }
  return results;
};

/**
 * @template T
 * @param {T[]} left
 * @param {T[]} right
 * @returns {T[]}
 */
const intersection = (left, right) => {
  const [result, other] =
    left.length < right.length
      ? [new Set(left), new Set(right)]
      : [new Set(right), new Set(left)];

  for (const item of result) {
    if (!other.has(item)) {
      result.delete(item);
    }
  }

  return [...result];
};

class EscalatedCapability extends Failure {
  /**
   * @param {API.ParsedCapability} claimed
   * @param {object} delegated
   * @param {API.Failure} cause
   */
  constructor(claimed, delegated, cause) {
    super();
    this.claimed = claimed;
    this.delegated = delegated;
    this.cause = cause;
    this.name = the("EscalatedCapability");
  }
  describe() {
    return `Constraint violation: ${this.cause.message}`;
  }
}

/**
 * @implements {API.DelegationError}
 */
class DelegationError extends Failure {
  /**
   * @param {(API.InvalidCapability | API.EscalatedDelegation | API.DelegationError)[]} causes
   * @param {object} context
   */
  constructor(causes, context) {
    super();
    this.name = the("InvalidClaim");
    this.causes = causes;
    this.context = context;
  }
  describe() {
    return [
      `Can not derive ${this.context} from delegated capabilities:`,
      ...this.causes.map((cause) => li(cause.message)),
    ].join("\n");
  }

  /**
   * @type {API.InvalidCapability | API.EscalatedDelegation | API.DelegationError}
   */
  get cause() {
    /* c8 ignore next 9 */
    if (this.causes.length !== 1) {
      return this;
    } else {
      const [cause] = this.causes;
      const value = cause.name === "InvalidClaim" ? cause.cause : cause;
      Object.defineProperties(this, { cause: { value } });
      return value;
    }
  }
}

/**
 * @implements {API.MalformedCapability}
 */
class MalformedCapability extends Failure {
  /**
   * @param {API.Capability} capability
   * @param {API.Failure} cause
   */
  constructor(capability, cause) {
    super();
    this.name = the("MalformedCapability");
    this.capability = capability;
    this.cause = cause;
  }
  describe() {
    return [
      `Encountered malformed '${this.capability.can}' capability: ${format$2(
        this.capability
      )}`,
      li(this.cause.message),
    ].join("\n");
  }
}

class UnknownCapability extends Failure {
  /**
   * @param {API.Capability} capability
   */
  constructor(capability) {
    super();
    this.name = the("UnknownCapability");
    this.capability = capability;
  }
  /* c8 ignore next 3 */
  describe() {
    return `Encountered unknown capability: ${format$2(this.capability)}`;
  }
}

/**
 * @param {unknown} capability
 * @param {string|number} [space]
 */

const format$2 = (capability, space) =>
  JSON.stringify(
    capability,
    (_key, value) => {
      /* c8 ignore next 2 */
      if (isLink(value)) {
        return value.toString();
      } else {
        return value;
      }
    },
    space
  );

/**
 * @param {string} message
 */
const indent = (message, indent = "  ") =>
  `${indent}${message.split("\n").join(`\n${indent}`)}`;

/**
 * @param {string} message
 */
const li = (message) => indent(`- ${message}`);

/**
 * @template {API.Ability} A
 * @template {API.URI} R
 * @template {API.Caveats} C
 * @typedef {{
 * can: A
 * with: API.Reader<R, API.Resource, API.Failure>
 * nb?: Schema.MapRepresentation<C, unknown>
 * derives?: (claim: {can:A, with: R, nb: C}, proof:{can:A, with:R, nb:C}) => API.Result<{}, API.Failure>
 * }} Descriptor
 */

/**
 * @template {API.Ability} A
 * @template {API.URI} R
 * @template {API.Caveats} [C={}]
 * @param {Descriptor<A, R, C>} descriptor
 
 * @returns {API.TheCapabilityParser<API.CapabilityMatch<A, R, C>>}
 */
const capability = ({
  derives = defaultDerives,
  nb = defaultNBSchema,
  ...etc
}) => new Capability({ derives, nb, ...etc });

const defaultNBSchema =
  /** @type {Schema.MapRepresentation<any>} */
  (struct({}));

/**
 * @template {API.Match} M
 * @template {API.Match} W
 * @param {API.Matcher<M>} left
 * @param {API.Matcher<W>} right
 * @returns {API.CapabilityParser<M|W>}
 */
const or$8 = (left, right) => new Or(left, right);

/**
 * @template {API.MatchSelector<API.Match>[]} Selectors
 * @param {Selectors} selectors
 * @returns {API.CapabilitiesParser<API.InferMembers<Selectors>>}
 */
const and$3 = (...selectors) => new And(selectors);

/**
 * @template {API.Match} M
 * @template {API.ParsedCapability} T
 * @param {object} source
 * @param {API.MatchSelector<M>} source.from
 * @param {API.TheCapabilityParser<API.DirectMatch<T>>} source.to
 * @param {API.Derives<T, API.InferDeriveProof<M['value']>>} source.derives
 
 * @returns {API.TheCapabilityParser<API.DerivedMatch<T, M>>}
 */
const derive$1 = ({ from, to, derives }) => new Derive(from, to, derives);

/**
 * @template {API.Match} M
 * @implements {API.View<M>}
 */
class View {
  /**
   * @param {API.Source} source
   * @returns {API.MatchResult<M>}
   */
  /* c8 ignore next 3 */
  match(source) {
    return { error: new UnknownCapability(source.capability) };
  }

  /**
   * @param {API.Source[]} capabilities
   * @returns {API.Select<M>}
   */
  select(capabilities) {
    return select(this, capabilities);
  }

  /**
   * @template {API.ParsedCapability} U
   * @param {object} source
   * @param {API.TheCapabilityParser<API.DirectMatch<U>>} source.to
   * @param {API.Derives<U, API.InferDeriveProof<M['value']>>} source.derives
   * @returns {API.TheCapabilityParser<API.DerivedMatch<U, M>>}
   */
  derive({ derives, to }) {
    return derive$1({ derives, to, from: this });
  }
}

/**
 * @template {API.Match} M
 * @implements {API.CapabilityParser<M>}
 * @extends {View<M>}
 */
class Unit extends View {
  /**
   * @template {API.Match} W
   * @param {API.MatchSelector<W>} other
   * @returns {API.CapabilityParser<M | W>}
   */
  or(other) {
    return or$8(this, other);
  }

  /**
   * @template {API.Match} W
   * @param {API.CapabilityParser<W>} other
   * @returns {API.CapabilitiesParser<[M, W]>}
   */
  and(other) {
    return and$3(/** @type {API.CapabilityParser<M>} */ (this), other);
  }
}

/**
 * @template {API.Ability} A
 * @template {API.URI} R
 * @template {API.Caveats} C
 * @implements {API.TheCapabilityParser<API.DirectMatch<API.ParsedCapability<A, R, C>>>}
 * @extends {Unit<API.DirectMatch<API.ParsedCapability<A, R, C>>>}
 */
class Capability extends Unit {
  /**
   * @param {Required<Descriptor<A, R, C>>} descriptor
   */
  constructor(descriptor) {
    super();
    this.descriptor = descriptor;
    this.schema = struct({
      can: literal(descriptor.can),
      with: descriptor.with,
      nb: descriptor.nb,
    });
  }

  /**
   * @param {API.InferCreateOptions<R, C>} options
   */
  create(options) {
    const { descriptor, can } = this;
    descriptor.nb;
    const data = /** @type {C} */ (options.nb || {});

    const resource = descriptor.with.read(options.with);
    if (resource.error) {
      throw Object.assign(
        new Error(`Invalid 'with' - ${resource.error.message}`),
        {
          cause: resource,
        }
      );
    }

    const nb = descriptor.nb.read(data);
    if (nb.error) {
      throw Object.assign(new Error(`Invalid 'nb' - ${nb.error.message}`), {
        cause: nb,
      });
    }

    return createCapability({ can, with: resource.ok, nb: nb.ok });
  }

  /**
   * @param {API.InferInvokeOptions<R, C>} options
   */
  invoke({ with: with_, nb, ...options }) {
    return invoke({
      ...options,
      capability: this.create(
        /** @type {API.InferCreateOptions<R, C>} */
        ({ with: with_, nb })
      ),
    });
  }

  /**
   * @param {API.InferDelegationOptions<R, C>} options
   * @returns {Promise<API.Delegation<[API.InferDelegatedCapability<API.ParsedCapability<A, R, C>>]>>}
   */
  async delegate({ nb: input = {}, with: with_, ...options }) {
    const { descriptor, can } = this;
    descriptor.nb;

    const resource = descriptor.with.read(with_);
    if (resource.error) {
      throw Object.assign(
        new Error(`Invalid 'with' - ${resource.error.message}`),
        {
          cause: resource,
        }
      );
    }

    const nb = descriptor.nb.partial().read(input);
    if (nb.error) {
      throw Object.assign(new Error(`Invalid 'nb' - ${nb.error.message}`), {
        cause: nb,
      });
    }

    return delegate$3({
      capabilities: [createCapability({ can, with: resource.ok, nb: nb.ok })],
      ...options,
    });
  }

  get can() {
    return this.descriptor.can;
  }

  /**
   * @param {API.Source} source
   * @returns {API.MatchResult<API.DirectMatch<API.ParsedCapability<A, R, C>>>}
   */
  match(source) {
    const result = parseCapability(this.descriptor, source);
    return result.error
      ? result
      : { ok: new Match(source, result.ok, this.descriptor) };
  }
  toString() {
    return JSON.stringify({ can: this.descriptor.can });
  }
}

/**
 * Normalizes capability by removing empty nb field.
 *
 * @template {API.ParsedCapability} T
 * @param {T} source
 */

const createCapability = ({ can, with: with_, nb }) =>
  /** @type {API.InferCapability<T>} */ ({
    can,
    with: with_,
    ...(isEmpty(nb) ? {} : { nb }),
  });

/**
 * @param {object} object
 * @returns {object is {}}
 */
const isEmpty = (object) => {
  for (const _ in object) {
    return false;
  }
  return true;
};

/**
 * @template {API.Match} M
 * @template {API.Match} W
 * @implements {API.CapabilityParser<M|W>}
 * @extends {Unit<M|W>}
 */
class Or extends Unit {
  /**
   * @param {API.Matcher<M>} left
   * @param {API.Matcher<W>} right
   */
  constructor(left, right) {
    super();
    this.left = left;
    this.right = right;
  }

  /**
   * @param {API.Source} capability
   * @return {API.MatchResult<M|W>}
   */
  match(capability) {
    const left = this.left.match(capability);
    if (left.error) {
      const right = this.right.match(capability);
      if (right.error) {
        return right.error.name === "MalformedCapability"
          ? //
            right
          : //
            left;
      } else {
        return right;
      }
    } else {
      return left;
    }
  }

  toString() {
    return `${this.left.toString()}|${this.right.toString()}`;
  }
}

/**
 * @template {API.MatchSelector<API.Match>[]} Selectors
 * @implements {API.CapabilitiesParser<API.InferMembers<Selectors>>}
 * @extends {View<API.Amplify<API.InferMembers<Selectors>>>}
 */
class And extends View {
  /**
   * @param {Selectors} selectors
   */
  constructor(selectors) {
    super();
    this.selectors = selectors;
  }
  /**
   * @param {API.Source} capability
   * @returns {API.MatchResult<API.Amplify<API.InferMembers<Selectors>>>}
   */
  match(capability) {
    const group = [];
    for (const selector of this.selectors) {
      const result = selector.match(capability);
      if (result.error) {
        return result;
      } else {
        group.push(result.ok);
      }
    }

    return {
      ok: new AndMatch(/** @type {API.InferMembers<Selectors>} */ (group)),
    };
  }

  /**
   * @param {API.Source[]} capabilities
   */
  select(capabilities) {
    return selectGroup(this, capabilities);
  }
  /**
   * @template E
   * @template {API.Match} X
   * @param {API.MatchSelector<API.Match<E, X>>} other
   * @returns {API.CapabilitiesParser<[...API.InferMembers<Selectors>, API.Match<E, X>]>}
   */
  and(other) {
    return new And([...this.selectors, other]);
  }
  toString() {
    return `[${this.selectors.map(String).join(", ")}]`;
  }
}

/**
 * @template {API.ParsedCapability} T
 * @template {API.Match} M
 * @implements {API.TheCapabilityParser<API.DerivedMatch<T, M>>}
 * @extends {Unit<API.DerivedMatch<T, M>>}
 */

class Derive extends Unit {
  /**
   * @param {API.MatchSelector<M>} from
   * @param {API.TheCapabilityParser<API.DirectMatch<T>>} to
   * @param {API.Derives<T, API.InferDeriveProof<M['value']>>} derives
   */
  constructor(from, to, derives) {
    super();
    this.from = from;
    this.to = to;
    this.derives = derives;
  }

  /**
   * @type {typeof this.to['create']}
   */
  create(options) {
    return this.to.create(options);
  }
  /**
   * @type {typeof this.to['invoke']}
   */
  invoke(options) {
    return this.to.invoke(options);
  }
  /**
   * @type {typeof this.to['delegate']}
   */
  delegate(options) {
    return this.to.delegate(options);
  }
  get can() {
    return this.to.can;
  }
  /**
   * @param {API.Source} capability
   * @returns {API.MatchResult<API.DerivedMatch<T, M>>}
   */
  match(capability) {
    const match = this.to.match(capability);
    if (match.error) {
      return match;
    } else {
      return { ok: new DerivedMatch(match.ok, this.from, this.derives) };
    }
  }
  toString() {
    return this.to.toString();
  }
}

/**
 * @template {API.Ability} A
 * @template {API.URI} R
 * @template {API.Caveats} C
 * @implements {API.DirectMatch<API.ParsedCapability<A, R, C>>}
 */
class Match {
  /**
   * @param {API.Source} source
   * @param {API.ParsedCapability<A, R, C>} value
   * @param {Required<Descriptor<A, R, C>>} descriptor
   */
  constructor(source, value, descriptor) {
    this.source = [source];
    this.value = value;
    this.descriptor = descriptor;
  }
  get can() {
    return this.value.can;
  }

  get proofs() {
    const proofs = [this.source[0].delegation];
    Object.defineProperties(this, {
      proofs: { value: proofs },
    });
    return proofs;
  }

  /**
   * @param {API.CanIssue} context
   * @returns {API.DirectMatch<API.ParsedCapability<A, R, C>>|null}
   */
  prune(context) {
    if (context.canIssue(this.value, this.source[0].delegation.issuer.did())) {
      return null;
    } else {
      return this;
    }
  }

  /**
   * @param {API.Source[]} capabilities
   * @returns {API.Select<API.DirectMatch<API.ParsedCapability<A, R, C>>>}
   */
  select(capabilities) {
    const unknown = [];
    const errors = [];
    const matches = [];
    for (const capability of capabilities) {
      const result = resolveCapability(this.descriptor, this.value, capability);
      if (result.ok) {
        const claim = this.descriptor.derives(this.value, result.ok);
        if (claim.error) {
          errors.push(
            new DelegationError(
              [new EscalatedCapability(this.value, result.ok, claim.error)],
              this
            )
          );
        } else {
          matches.push(new Match(capability, result.ok, this.descriptor));
        }
      } else {
        switch (result.error.name) {
          case "UnknownCapability":
            unknown.push(result.error.capability);
            break;
          case "MalformedCapability":
          default:
            errors.push(new DelegationError([result.error], this));
        }
      }
    }

    return { matches, unknown, errors };
  }
  toString() {
    const { nb } = this.value;
    return JSON.stringify({
      can: this.descriptor.can,
      with: this.value.with,
      nb: nb && Object.keys(nb).length > 0 ? nb : undefined,
    });
  }
}

/**
 * @template {API.ParsedCapability} T
 * @template {API.Match} M
 * @implements {API.DerivedMatch<T, M>}
 */

class DerivedMatch {
  /**
   * @param {API.DirectMatch<T>} selected
   * @param {API.MatchSelector<M>} from
   * @param {API.Derives<T, API.InferDeriveProof<M['value']>>} derives
   */
  constructor(selected, from, derives) {
    this.selected = selected;
    this.from = from;
    this.derives = derives;
  }
  get can() {
    return this.value.can;
  }
  get source() {
    return this.selected.source;
  }
  get proofs() {
    const proofs = [];
    for (const { delegation } of this.selected.source) {
      proofs.push(delegation);
    }
    Object.defineProperties(this, { proofs: { value: proofs } });
    return proofs;
  }
  get value() {
    return this.selected.value;
  }

  /**
   * @param {API.CanIssue} context
   */
  prune(context) {
    const selected =
      /** @type {API.DirectMatch<T>|null} */
      (this.selected.prune(context));
    return selected
      ? new DerivedMatch(selected, this.from, this.derives)
      : null;
  }

  /**
   * @param {API.Source[]} capabilities
   */
  select(capabilities) {
    const { derives, selected, from } = this;
    const { value } = selected;

    const direct = selected.select(capabilities);

    const derived = from.select(capabilities);
    const matches = [];
    const errors = [];
    for (const match of derived.matches) {
      // If capability can not be derived it escalates
      const result = derives(value, match.value);
      if (result.error) {
        errors.push(
          new DelegationError(
            [new EscalatedCapability(value, match.value, result.error)],
            this
          )
        );
      } else {
        matches.push(match);
      }
    }

    return {
      unknown: intersection(direct.unknown, derived.unknown),
      errors: [
        ...errors,
        ...direct.errors,
        ...derived.errors.map((error) => new DelegationError([error], this)),
      ],
      matches: [
        ...direct.matches.map(
          (match) => new DerivedMatch(match, from, derives)
        ),
        ...matches,
      ],
    };
  }

  toString() {
    return this.selected.toString();
  }
}

/**
 * @template {API.MatchSelector<API.Match>[]} Selectors
 * @implements {API.Amplify<API.InferMembers<Selectors>>}
 */
class AndMatch {
  /**
   * @param {API.Match[]} matches
   */
  constructor(matches) {
    this.matches = matches;
  }
  get selectors() {
    return this.matches;
  }
  /**
   * @returns {API.Source[]}
   */
  get source() {
    const source = [];

    for (const match of this.matches) {
      source.push(...match.source);
    }
    Object.defineProperties(this, { source: { value: source } });
    return source;
  }

  /**
   * @param {API.CanIssue} context
   */
  prune(context) {
    const matches = [];
    for (const match of this.matches) {
      const pruned = match.prune(context);
      if (pruned) {
        matches.push(pruned);
      }
    }
    return matches.length === 0 ? null : new AndMatch(matches);
  }

  get proofs() {
    const proofs = [];

    for (const { delegation } of this.source) {
      proofs.push(delegation);
    }

    Object.defineProperties(this, { proofs: { value: proofs } });
    return proofs;
  }
  /**
   * @type {API.InferValue<API.InferMembers<Selectors>>}
   */
  get value() {
    const value = [];

    for (const match of this.matches) {
      value.push(match.value);
    }
    Object.defineProperties(this, { value: { value } });
    return /** @type {any} */ (value);
  }
  /**
   * @param {API.Source[]} capabilities
   */
  select(capabilities) {
    return selectGroup(this, capabilities);
  }
  toString() {
    return `[${this.matches.map((match) => match.toString()).join(", ")}]`;
  }
}

/**
 * Resolves ability `pattern` of the delegated capability from the ability
 * of the claimed capability. If pattern matches returns claimed ability
 * otherwise returns given `fallback`.
 *
 * @example
 * ```js
 * resolveAbility('*', 'store/add', null) // => 'store/add'
 * resolveAbility('store/*', 'store/add', null) // => 'store/add'
 * resolveAbility('store/add', 'store/add', null) // => 'store/add'
 * resolveAbility('store/', 'store/add', null) // => null
 * resolveAbility('store/a*', 'store/add', null) // => null
 * resolveAbility('store/list', 'store/add', null) // => null
 * ```
 *
 * @template {API.Ability} T
 * @template U
 * @param {string} pattern
 * @param {T} can
 * @param {U} fallback
 * @returns {T|U}
 */
const resolveAbility = (pattern, can, fallback) => {
  switch (pattern) {
    case can:
    case "*":
      return can;
    default:
      return pattern.endsWith("/*") && can.startsWith(pattern.slice(0, -1))
        ? can
        : fallback;
  }
};

/**
 * Resolves `source` resource of the delegated capability from the resource
 * `uri` of the claimed capability. If `source` is `"ucan:*""` or matches `uri`
 * then it returns `uri` back otherwise it returns `fallback`.
 *
 * @example
 * ```js
 * resolveResource('ucan:*', 'did:key:zAlice', null) // => 'did:key:zAlice'
 * resolveAbility('ucan:*', 'https://example.com', null) // => 'https://example.com'
 * resolveAbility('did:*', 'did:key:zAlice', null) // => null
 * resolveAbility('did:key:zAlice', 'did:key:zAlice', null) // => did:key:zAlice
 * ```
 * @template {string} T
 * @template U
 * @param {T} uri
 * @param {string} source
 * @param {U} fallback
 * @returns {T|U}
 */
const resolveResource = (source, uri, fallback) => {
  switch (source) {
    case uri:
    case "ucan:*":
      return uri;
    default:
      return fallback;
  }
};

/**
 * Parses capability from the `source` using a provided `parser`.
 *
 * @template {API.Ability} A
 * @template {API.URI} R
 * @template {API.Caveats} C
 * @param {Required<Descriptor<A, R, C>>} descriptor
 * @param {API.Source} source
 * @returns {API.Result<API.ParsedCapability<A, R, C>, API.InvalidCapability>}
 */
const parseCapability = (descriptor, source) => {
  const { delegation } = source;
  const capability = /** @type {API.Capability<A, R, C>} */ (source.capability);

  if (descriptor.can !== capability.can) {
    return { error: new UnknownCapability(capability) };
  }

  const uri = descriptor.with.read(capability.with);
  if (uri.error) {
    return { error: new MalformedCapability(capability, uri.error) };
  }

  const nb = descriptor.nb.read(capability.nb || {});
  if (nb.error) {
    return { error: new MalformedCapability(capability, nb.error) };
  }

  return { ok: new CapabilityView(descriptor.can, uri.ok, nb.ok, delegation) };
};

/**
 * Resolves delegated capability `source` from the `claimed` capability using
 * provided capability `parser`. It is similar to `parseCapability` except
 * `source` here is treated as capability pattern which is matched against the
 * `claimed` capability. This means we resolve `can` and `with` fields from the
 * `claimed` capability and inherit all missing `nb` fields from the claimed
 * capability.
 *
 * @template {API.Ability} A
 * @template {API.URI} R
 * @template {API.Caveats} C
 * @param {Required<Descriptor<A, R, C>>} descriptor
 * @param {API.ParsedCapability<A, R, C>} claimed
 * @param {API.Source} source
 * @returns {API.Result<API.ParsedCapability<A, R, C>, API.InvalidCapability>}
 */

const resolveCapability = (descriptor, claimed, { capability, delegation }) => {
  const can = resolveAbility(capability.can, claimed.can, null);
  if (can == null) {
    return { error: new UnknownCapability(capability) };
  }

  const resource = resolveResource(
    capability.with,
    claimed.with,
    capability.with
  );
  const uri = descriptor.with.read(resource);
  if (uri.error) {
    return { error: new MalformedCapability(capability, uri.error) };
  }

  const nb = descriptor.nb.read({
    ...claimed.nb,
    ...capability.nb,
  });

  if (nb.error) {
    return { error: new MalformedCapability(capability, nb.error) };
  }

  return { ok: new CapabilityView(can, uri.ok, nb.ok, delegation) };
};

/**
 * @template {API.Ability} A
 * @template {API.URI} R
 * @template C
 */
class CapabilityView {
  /**
   * @param {A} can
   * @param {R} with_
   * @param {C} nb
   * @param {API.Delegation} delegation
   */
  constructor(can, with_, nb, delegation) {
    this.can = can;
    this.with = with_;
    this.delegation = delegation;
    this.nb = nb;
  }
}

/**
 * @template {API.Match} M
 * @param {API.Matcher<M>} matcher
 * @param {API.Source[]} capabilities
 * @returns {API.Select<M>}
 */

const select = (matcher, capabilities) => {
  const unknown = [];
  const matches = [];
  const errors = [];
  for (const capability of capabilities) {
    const result = matcher.match(capability);
    if (result.error) {
      switch (result.error.name) {
        case "UnknownCapability":
          unknown.push(result.error.capability);
          break;
        case "MalformedCapability":
        default:
          errors.push(
            new DelegationError([result.error], result.error.capability)
          );
      }
    } else {
      matches.push(result.ok);
    }
  }

  return { matches, errors, unknown };
};

/**
 * @template {API.Selector<API.Match>[]} S
 * @param {{selectors:S}} self
 * @param {API.Source[]} capabilities
 */

const selectGroup = (self, capabilities) => {
  let unknown;
  const data = [];
  const errors = [];
  for (const selector of self.selectors) {
    const selected = selector.select(capabilities);
    unknown = unknown
      ? intersection(unknown, selected.unknown)
      : selected.unknown;

    for (const error of selected.errors) {
      errors.push(new DelegationError([error], self));
    }

    data.push(selected.matches);
  }

  const matches = combine(data).map((group) => new AndMatch(group));

  return {
    unknown:
      /* c8 ignore next */
      unknown || [],
    errors,
    matches,
  };
};

/**
 * @template {API.ParsedCapability} T
 * @template {API.ParsedCapability} U
 * @param {T} claimed
 * @param {U} delegated
 * @return {API.Result<true, API.Failure>}
 */
const defaultDerives = (claimed, delegated) => {
  if (delegated.with.endsWith("*")) {
    if (!claimed.with.startsWith(delegated.with.slice(0, -1))) {
      return error(
        `Resource ${claimed.with} does not match delegated ${delegated.with} `
      );
    }
  } else if (delegated.with !== claimed.with) {
    return error(
      `Resource ${claimed.with} is not contained by ${delegated.with}`
    );
  }

  /* c8 ignore next 2 */
  const caveats = delegated.nb || {};
  const nb = claimed.nb || {};
  const kv = entries$1(caveats);

  for (const [name, value] of kv) {
    if (nb[name] != value) {
      return error(`${String(name)}: ${nb[name]} violates ${value}`);
    }
  }

  return { ok: true };
};

/**
 * Returns true if the two passed Uint8Arrays have the same content
 */
function equals$4(a, b) {
  if (a === b) {
    return true;
  }
  if (a.byteLength !== b.byteLength) {
    return false;
  }
  for (let i = 0; i < a.byteLength; i++) {
    if (a[i] !== b[i]) {
      return false;
    }
  }
  return true;
}

// e.g. did:web:web3.storage or did:web:staging.web3.storage
const ProviderDID$4 = match$1({ method: "web" });

const SpaceDID$1 = match$1({ method: "key" });

const AccountDID$1 = match$1({ method: "mailto" });

const Await = struct({
  "ucan/await": tuple([string(), match$2()]),
});

/**
 * Checks that `with` on claimed capability is the same as `with`
 * in delegated capability. Note this will ignore `can` field.
 *
 * @param {Types.ParsedCapability} child
 * @param {Types.ParsedCapability} parent
 */
function equalWith(child, parent) {
  return child.with === parent.with
    ? ok({})
    : fail(
        `Can not derive ${child.can} with ${child.with} from ${parent.with}`
      );
}

/**
 * @param {unknown} child
 * @param {unknown} parent
 * @param {string} constraint
 */
function equal(child, parent, constraint) {
  if (parent === undefined || parent === "*") {
    return ok({});
  } else if (String(child) === String(parent)) {
    return ok({});
  } else {
    return fail(
      `Constrain violation: ${child} violates imposed ${constraint} constraint ${parent}`
    );
  }
}

/**
 * @template {Types.ParsedCapability<"store/add"|"store/get"|"store/remove", Types.URI<'did:'>, {link?: Types.Link<unknown, number, number, 0|1>}>} T
 * @param {T} claimed
 * @param {T} delegated
 * @returns {Types.Result<{}, Types.Failure>}
 */
const equalLink = (claimed, delegated) => {
  if (claimed.with !== delegated.with) {
    return fail(
      `Expected 'with: "${delegated.with}"' instead got '${claimed.with}'`
    );
  } else if (
    delegated.nb.link &&
    `${delegated.nb.link}` !== `${claimed.nb.link}`
  ) {
    return fail(
      `Link ${claimed.nb.link ? `${claimed.nb.link}` : ""} violates imposed ${
        delegated.nb.link
      } constraint.`
    );
  } else {
    return ok({});
  }
};

/**
 * @template {Types.ParsedCapability<"space/blob/add"|"space/blob/remove"|"web3.storage/blob/allocate"|"web3.storage/blob/accept", Types.URI<'did:'>, {blob: { digest: Uint8Array, size: number }}>} T
 * @param {T} claimed
 * @param {T} delegated
 * @returns {Types.Result<{}, Types.Failure>}
 */
const equalBlob = (claimed, delegated) => {
  if (claimed.with !== delegated.with) {
    return fail(
      `Expected 'with: "${delegated.with}"' instead got '${claimed.with}'`
    );
  } else if (
    delegated.nb.blob.digest &&
    !equals$4(delegated.nb.blob.digest, claimed.nb.blob.digest)
  ) {
    return fail(
      `Link ${
        claimed.nb.blob.digest ? `${claimed.nb.blob.digest}` : ""
      } violates imposed ${delegated.nb.blob.digest} constraint.`
    );
  } else if (
    claimed.nb.blob.size !== undefined &&
    delegated.nb.blob.size !== undefined
  ) {
    return claimed.nb.blob.size > delegated.nb.blob.size
      ? fail(
          `Size constraint violation: ${claimed.nb.blob.size} > ${delegated.nb.blob.size}`
        )
      : ok({});
  } else {
    return ok({});
  }
};

/**
 * @template {Types.ParsedCapability<"http/put", Types.URI<'did:'>, {body: { digest: Uint8Array, size: number }}>} T
 * @param {T} claimed
 * @param {T} delegated
 * @returns {Types.Result<{}, Types.Failure>}
 */
const equalBody = (claimed, delegated) => {
  if (claimed.with !== delegated.with) {
    return fail(
      `Expected 'with: "${delegated.with}"' instead got '${claimed.with}'`
    );
  } else if (
    delegated.nb.body.digest &&
    !equals$4(delegated.nb.body.digest, claimed.nb.body.digest)
  ) {
    return fail(
      `Link ${
        claimed.nb.body.digest ? `${claimed.nb.body.digest}` : ""
      } violates imposed ${delegated.nb.body.digest} constraint.`
    );
  } else if (
    claimed.nb.body.size !== undefined &&
    delegated.nb.body.size !== undefined
  ) {
    return claimed.nb.body.size > delegated.nb.body.size
      ? fail(
          `Size constraint violation: ${claimed.nb.body.size} > ${delegated.nb.body.size}`
        )
      : ok({});
  } else {
    return ok({});
  }
};

/**
 * Checks that `claimed` {@link Types.Link} meets an `imposed` constraint.
 *
 * @param {Types.UnknownLink} claimed
 * @param {Types.UnknownLink|undefined} imposed
 * @param {string} at
 * @returns {Types.Result<{}, Types.Failure>}
 */
const checkLink = (claimed, imposed, at) => {
  return equal(
    String(claimed),
    imposed === undefined ? undefined : String(imposed),
    at
  );
};

/**
 * @template T
 * @param {Types.Result<T , Types.Failure>} result
 * @returns {{error: Types.Failure, ok?:undefined}|undefined}
 */
const and$2 = (result) => (result.error ? result : undefined);

/**
 *
 * @param {import('@ucanto/interface').Ability} ability
 */
function parseAbility(ability) {
  const [namespace, ...segments] = ability.split("/");
  return { namespace, segments };
}

/**
 *
 * TODO: needs to account for caps derived from different namespaces like 'account/info' can be derived from 'store/add'
 *
 * @param {import('@ucanto/interface').Ability} parent
 * @param {import('@ucanto/interface').Ability} child
 */
function canDelegateAbility(parent, child) {
  const parsedParent = parseAbility(parent);
  const parsedChild = parseAbility(child);

  // Parent is wildcard
  if (parsedParent.namespace === "*" && parsedParent.segments.length === 0) {
    return true;
  }

  // Child is wild card so it can not be delegated from anything
  if (parsedChild.namespace === "*" && parsedChild.segments.length === 0) {
    return false;
  }

  // namespaces don't match
  if (parsedParent.namespace !== parsedChild.namespace) {
    return false;
  }

  // given that namespaces match and parent first segment is wildcard
  if (parsedParent.segments[0] === "*") {
    return true;
  }

  // Array equality
  if (parsedParent.segments.length !== parsedChild.segments.length) {
    return false;
  }

  // all segments must match
  return parsedParent.segments.reduce(
    (acc, v, i) => acc && parsedChild.segments[i] === v,
    true
  );
}

/**
 * Store Capabilities
 *
 * These can be imported directly with:
 * ```js
 * import * as Store from '@web3-storage/capabilities/store'
 * ```
 *
 * @module
 */

// @see https://github.com/multiformats/multicodec/blob/master/table.csv#L140
const code$b = 0x0202;

const CARLink$1 = match$2({ code: code$b, version: 1 });

/**
 * Capability can only be delegated (but not invoked) allowing audience to
 * derived any `store/` prefixed capability for the (memory) space identified
 * by DID in the `with` field.
 */
const store$1 = capability({
  can: "store/*",
  /**
   * DID of the (memory) space where CAR is intended to
   * be stored.
   */
  with: SpaceDID$1,
  derives: equalWith,
});

/**
 * `store/add` capability allows agent to store a CAR file into a (memory) space
 * identified by did:key in the `with` field. Agent must precompute CAR locally
 * and provide it's CID and size using `nb.link` and `nb.size` fields, allowing
 * a service to provision a write location for the agent to PUT or POST desired
 * CAR into.
 */
const add$a = capability({
  can: "store/add",
  /**
   * DID of the (memory) space where CAR is intended to
   * be stored.
   */
  with: SpaceDID$1,
  nb: struct({
    /**
     * CID of the CAR file to be stored. Service will provision write target
     * for this exact CAR file for agent to PUT or POST it. Attempt to write
     * any other content will fail.
     */
    link: CARLink$1,
    /**
     * Size of the CAR file to be stored. Service will provision write target
     * for this exact size. Attempt to write a larger CAR file will fail.
     */
    size: integer(),
    /**
     * Agent may optionally provide a link to a related CAR file using `origin`
     * field. This is useful when storing large DAGs, agent could shard it
     * across multiple CAR files and then link each shard with a previous one.
     *
     * Providing this relation tells service that given CAR is shard of the
     * larger DAG as opposed to it being intentionally partial DAG. When DAG is
     * not sharded, there will be only one `store/add` with `origin` left out.
     */
    origin: optional(),
  }),
  derives: (claim, from) => {
    const result = equalLink(claim, from);
    if (result.error) {
      return result;
    } else if (claim.nb.size !== undefined && from.nb.size !== undefined) {
      return claim.nb.size > from.nb.size
        ? fail(`Size constraint violation: ${claim.nb.size} > ${from.nb.size}`)
        : ok({});
    } else {
      return ok({});
    }
  },
});

/**
 * Capability to get store metadata by shard CID.
 * Use to check for inclusion, or get shard size and origin
 *
 * `nb.link` is optional to allow delegation of `store/get`
 * capability for any shard CID. If link is specified, then the
 * capability only allows a get for that specific CID.
 *
 * When used as as an invocation, `nb.link` must be specified.
 */
const get$h = capability({
  can: "store/get",
  with: SpaceDID$1,
  nb: struct({
    /**
     * shard CID to fetch info about.
     */
    link: CARLink$1.optional(),
  }),
  derives: equalLink,
});

/**
 * Capability can be used to remove the stored CAR file from the (memory)
 * space identified by `with` field.
 */
const remove$8 = capability({
  can: "store/remove",
  /**
   * DID of the (memory) space where CAR is intended to
   * be stored.
   */
  with: SpaceDID$1,
  nb: struct({
    /**
     * CID of the CAR file to be removed from the store.
     */
    link: CARLink$1,
  }),
  derives: equalLink,
});

/**
 * Capability can be invoked to request a list of stored CAR files in the
 * (memory) space identified by `with` field.
 */
const list$9 = capability({
  can: "store/list",
  /**
   * DID of the (memory) space where CAR is intended to
   * be stored.
   */
  with: SpaceDID$1,
  nb: struct({
    /**
     * A pointer that can be moved back and forth on the list.
     * It can be used to paginate a list for instance.
     */
    cursor: string().optional(),
    /**
     * Maximum number of items per page.
     */
    size: integer().optional(),
    /**
     * If true, return page of results preceding cursor. Defaults to false.
     */
    pre: boolean().optional(),
  }),
  derives: (claimed, delegated) => {
    if (claimed.with !== delegated.with) {
      return fail(
        `Expected 'with: "${delegated.with}"' instead got '${claimed.with}'`
      );
    }
    return ok({});
  },
});

add$a.or(remove$8).or(list$9);

/**
 * Upload Capabilities
 *
 * These can be imported directly with:
 * ```js
 * import * as Account from '@web3-storage/capabilities/upload'
 * ```
 *
 * @module
 */

/**
 * Capability can only be delegated (but not invoked) allowing audience to
 * derived any `upload/` prefixed capability for the (memory) space identified
 * by DID in the `with` field.
 */
const upload$1 = capability({
  can: "upload/*",
  /**
   * DID of the (memory) space where upload is add to the
   * upload list.
   */
  with: SpaceDID$1,
  derives: equalWith,
});

/**
 * Schema representing a link (a.k.a CID) to a CAR file. Enforces CAR codec code and CID v1.
 */
const CARLink = match$2({ code: code$c, version: 1 });

/**
 * Capability allows an agent to add an arbitrary DAG (root) to the upload list
 * of the specified (memory) space (identified by did:key in the `with` field).
 * It is recommended to provide an optional list of shard links that contain
 * fragments of this DAG, as it allows system to optimize block discovery, it is
 * also a way to communicate DAG partiality - this upload contains partial DAG
 * identified by the given `root`.
 *
 * Usually when agent wants to upload a DAG it will encode it as a one or more
 * CAR files (shards) and invoke `store/add` capability for each one. Once all
 * shards are stored it will invoke `upload/add` capability (providing link to
 * a DAG root and all the shards) to add it the upload list.
 *
 * That said `upload/add` could be invoked without invoking `store/add`s e.g.
 * because another (memory) space may already have those CARs.
 *
 * Note: If DAG with the given root is already in the upload list, invocation
 * will simply update `shards` to be a union of existing and new shards.
 */
const add$9 = capability({
  can: "upload/add",
  /**
   * DID of the (memory) space where uploaded is added.
   */
  with: SpaceDID$1,
  nb: struct({
    /**
     * Root CID of the DAG to be added to the upload list.
     */
    root: Link,
    /**
     * CIDs to the CAR files that contain blocks of the DAG.
     */
    shards: CARLink.array().optional(),
  }),
  derives: (self, from) => {
    return (
      and$2(equalWith(self, from)) ||
      and$2(equal(self.nb.root, from.nb.root, "root")) ||
      and$2(equal(self.nb.shards, from.nb.shards, "shards")) ||
      ok({})
    );
  },
});

/**
 * Capability to get upload metadata by root CID.
 * Use to check for inclusion, or find the shards for a root.
 *
 * `nb.root` is optional to allow delegation of `upload/get`
 * capability for any root. If root is specified, then the
 * capability only allows a get for that single cid.
 *
 * When used as as an invocation, `nb.root` must be specified.
 */
const get$g = capability({
  can: "upload/get",
  with: SpaceDID$1,
  nb: struct({
    /**
     * Root CID of the DAG to fetch upload info about.
     */
    root: optional(),
  }),
  derives: (self, from) => {
    const res = equalWith(self, from);
    if (res.error) {
      return res;
    }
    if (!from.nb.root) {
      return res;
    }
    // root must match if specified in the proof
    return equal(self.nb.root, from.nb.root, "root");
  },
});

/**
 * Capability removes an upload (identified by it's root CID) from the upload
 * list. Please note that removing an upload does not delete corresponding shards
 * from the store, however that could be done via `store/remove` invocations.
 */
const remove$7 = capability({
  can: "upload/remove",
  /**
   * DID of the (memory) space where uploaded is removed from.
   */
  with: SpaceDID$1,
  nb: struct({
    /**
     * Root CID of the DAG to be removed from the upload list.
     */
    root: Link,
  }),
  derives: (self, from) => {
    return (
      and$2(equalWith(self, from)) ||
      and$2(equal(self.nb.root, from.nb.root, "root")) ||
      ok({})
    );
  },
});

/**
 * Capability can be invoked to request a list of uploads in the (memory) space
 * identified by the `with` field.
 */
const list$8 = capability({
  can: "upload/list",
  with: SpaceDID$1,
  nb: struct({
    /**
     * A pointer that can be moved back and forth on the list.
     * It can be used to paginate a list for instance.
     */
    cursor: string().optional(),
    /**
     * Maximum number of items per page.
     */
    size: integer().optional(),
    /**
     * If true, return page of results preceding cursor. Defaults to false.
     */
    pre: boolean().optional(),
  }),
});

add$9.or(remove$7).or(list$8);

/**
 * Top Capabilities
 *
 * These can be imported directly with:
 * ```js
 * import * as Account from '@web3-storage/capabilities/top'
 * ```
 *
 * @module
 */

/**
 * Represents the top `{ can: '*', with: 'did:key:zAlice' }` capability, which we often
 * also call account linking.
 *
 * @see {@link https://github.com/ucan-wg/spec#52-top}
 */
const top = capability({
  can: "*",
  with: or$9(match$1(), literal("ucan:*")),
  derives: equalWith,
});

/**
 * Space Capabilities
 *
 * These can be imported directly with:
 * ```js
 * import * as Space from '@web3-storage/capabilities/space'
 * ```
 *
 * @module
 */

const space = capability({
  can: "space/*",
  with: SpaceDID$1,
  derives: equalWith,
});

/**
 * `space/info` can be derived from any of the `store/*`
 * capability that has matching `with`. This allows store service
 * to identify account based on any user request.
 */
const info = add$a
  .or(list$9)
  .or(remove$8)
  .or(add$9)
  .or(list$8)
  .or(remove$7)
  .derive({
    to: capability({
      can: "space/info",
      with: SpaceDID$1,
    }),
    derives: equalWith,
  });

capability({
  can: "space/allocate",
  with: SpaceDID$1,
  nb: struct({
    size: integer(),
  }),
  derives: (child, parent) => {
    const result = equalWith(child, parent);
    if (result.ok) {
      return child.nb.size <= parent.nb.size
        ? ok({})
        : fail(
            `Claimed size ${child.nb.size} escalates delegated size ${parent.nb.size}`
          );
    } else {
      return result;
    }
  },
});

/**
 * The capability grants permission for all content serve operations that fall under the "space/content/serve" namespace.
 * It can be derived from any of the `space/*` capability that has matching `with`.
 */

const contentServe = capability({
  can: "space/content/serve/*",
  with: SpaceDID$1,
  derives: equalWith,
});

/**
 * Capability can be invoked by an agent to record egress data for a given resource.
 * It can be derived from any of the `space/content/serve/*` capability that has matching `with`.
 */
const egressRecord$1 = capability({
  can: "space/content/serve/egress/record",
  with: SpaceDID$1,
  nb: struct({
    /** CID of the resource that was served. */
    resource: match$2(),
    /** Amount of bytes served. */
    bytes: integer().greaterThan(0),
    /** Timestamp of the event in milliseconds after Unix epoch. */
    servedAt: integer().greaterThan(-1),
  }),
  derives: equalWith,
});

/**
 * UCAN core capabilities.
 */

const UCANLink =
  /** @type {Schema.Schema<API.UCANLink, unknown>} */
  (match$2({ version: 1 }));

/**
 * Capability can only be delegated (but not invoked) allowing audience to
 * derived any `store/` prefixed capability for the (memory) space identified
 * by DID in the `with` field.
 */
capability({
  can: "ucan/*",
  with: match$1(),
  derives: equalWith,
});

/**
 * `ucan/revoke` capability is a replacement for the
 * [UCAN Revocation](https://github.com/ucan-wg/spec#66-revocation) that had
 * been proposed to a UCAN working group and had a tentative support from
 * members.
 *
 * Capability can be used to revoke `nb.ucan` authorization from all proofs
 * chains that lead to the UCAN issued or being delegated to the principal
 * identified by the `with` field. Note that revoked UCAN MUST continue to
 * be valid in the invocation where proof chain does not lead to the principal
 * identified by the `with` field.
 */
const revoke = capability({
  can: "ucan/revoke",
  /**
   * DID of the principal authorizing revocation.
   */
  with: match$1(),
  nb: struct({
    /**
     * UCAN being revoked from all proof chains that lead to the UCAN that is
     * either issued (iss) by or delegated to (aud) the principal identified
     * by the `with` field.
     */
    ucan: UCANLink,
    /**
     * Proof chain illustrating the path from revoked UCAN to the one that is
     * either issued (iss) by or delegated to (aud) the principal identified
     * by the `with` field.
     *
     * If the UCAN being revoked is either issued (iss) by or delegated to (aud)
     * the principal identified by the `with` field no `proof` is required and
     * it can be omitted or set to an empty array.
     *
     * It is RECOMMENDED that `proof` is provided in all other cases otherwise
     * it MAY not be possible to verify that revoking principal is a participant
     * in the proof chain.
     */
    proof: UCANLink.array().optional(),
  }),
  derives: (claim, from) =>
    // With field MUST be the same
    and$2(equalWith(claim, from)) ??
    // UCAN being revoked MUST be the same
    and$2(checkLink(claim.nb.ucan, from.nb.ucan, "nb.ucan")) ??
    // And proof chain MUST be the same
    equal(
      (claim.nb.proof ?? []).join("/"),
      (from.nb.proof ?? []).join("/"),
      "nb.proof"
    ),
});

/**
 * `ucan/conclude` capability represents a receipt using a special UCAN capability.
 *
 * The UCAN invocation specification defines receipt record, that is cryptographically
 * signed description of the invocation output and requested effects. Receipt
 * structure is very similar to UCAN except it has no notion of expiry nor it is
 * possible to delegate ability to issue receipt to another principal.
 */
const conclude$1 = capability({
  can: "ucan/conclude",
  /**
   * DID of the principal representing the Conclusion Authority.
   * MUST be the DID of the audience of the ran invocation.
   */
  with: match$1(),
  nb: struct({
    /**
     * CID of the content with the Receipt.
     */
    receipt: match$2(),
  }),
  derives: (claim, from) =>
    // With field MUST be the same
    and$2(equalWith(claim, from)) ||
    and$2(checkLink(claim.nb.receipt, from.nb.receipt, "nb.receipt")) ||
    ok({}),
});

/**
 * Issued by trusted authority (usually the one handling invocation) that attest
 * that specific UCAN delegation has been considered authentic.
 *
 * @see https://github.com/storacha/specs/blob/main/w3-session.md#authorization-session
 * 
 * @example
 * ```js
 * {
    iss: "did:web:web3.storage",
    aud: "did:key:z6Mkk89bC3JrVqKie71YEcc5M1SMVxuCgNx6zLZ8SYJsxALi",
    att: [{
      "with": "did:web:web3.storage",
      "can": "ucan/attest",
      "nb": {
        "proof": {
          "/": "bafyreifer23oxeyamllbmrfkkyvcqpujevuediffrpvrxmgn736f4fffui"
        }
      }
    }],
    exp: null
    sig: "..."
  }
 * ```
 */
const attest = capability({
  can: "ucan/attest",
  // Should be web3.storage DID
  with: match$1(),
  nb: struct({
    // UCAN delegation that is being attested.
    proof: match$2({ version: 1 }),
  }),
  derives: (claim, from) =>
    // With field MUST be the same
    and$2(equalWith(claim, from)) ??
    // UCAN link MUST be the same
    checkLink(claim.nb.proof, from.nb.proof, "nb.proof"),
});

/**
 * Access Capabilities
 *
 * These can be imported directly with:
 * ```js
 * import * as Access from '@web3-storage/capabilities/access'
 * ```
 *
 * @module
 */

/**
 * Account identifier.
 */
const Account$1 = match$1({ method: "mailto" });

/**
 * Describes the capability requested.
 */
const CapabilityRequest = struct({
  /**
   * If set to `"*"` it corresponds to "sudo" access.
   */
  can: string(),
});

/**
 * Authorization request describing set of desired capabilities.
 */
const AuthorizationRequest = struct({
  /**
   * DID of the Account authorization is requested from.
   */
  iss: Account$1,
  /**
   * Capabilities agent wishes to be granted.
   */
  att: CapabilityRequest.array(),
});

/**
 * Capability can only be delegated (but not invoked) allowing audience to
 * derived any `access/` prefixed capability for the agent identified
 * by did:key in the `with` field.
 */
const access$1 = capability({
  can: "access/*",
  with: match$3({ protocol: "did:" }),
});

/**
 * Capability can be invoked by an agent to request set of capabilities from
 * the account.
 */
const authorize = capability({
  can: "access/authorize",
  with: match$1({ method: "key" }),
  /**
   * Authorization request describing set of desired capabilities
   */
  nb: AuthorizationRequest,
  derives: (child, parent) => {
    return (
      and$2(equalWith(child, parent)) ||
      and$2(equal(child.nb.iss, parent.nb.iss, "iss")) ||
      and$2(subsetCapabilities(child.nb.att, parent.nb.att)) ||
      ok({})
    );
  },
});

/**
 * Capability is delegated by us to the user allowing them to complete the
 * authorization flow. It allows us to ensure that user clicks the link and
 * we don't have some rogue agent trying to impersonate user clicking the link
 * in order to get access to their account.
 */
capability({
  can: "access/confirm",
  with: DID,
  nb: struct({
    /**
     * Link to the `access/authorize` request that this delegation was created
     * for.
     */
    cause: match$2({ version: 1 }),
    iss: Account$1,
    aud: match$1(),
    att: CapabilityRequest.array(),
  }),
  derives: (claim, proof) => {
    return (
      and$2(equalWith(claim, proof)) ||
      and$2(equal(claim.nb.iss, proof.nb.iss, "iss")) ||
      and$2(equal(claim.nb.aud, proof.nb.aud, "aud")) ||
      and$2(subsetCapabilities(claim.nb.att, proof.nb.att)) ||
      and$2(checkLink(claim.nb.cause, proof.nb.cause, "nb.cause")) ||
      ok({})
    );
  },
});

const claim$2 = capability({
  can: "access/claim",
  with: match$1({ method: "key" }).or(match$1({ method: "mailto" })),
});

// https://github.com/storacha/specs/blob/main/w3-access.md#accessdelegate
const delegate$2 = capability({
  can: "access/delegate",
  /**
   * Field MUST be a space DID with a storage provider. Delegation will be stored just like any other DAG stored using store/add capability.
   *
   * @see https://github.com/storacha/specs/blob/main/w3-access.md#delegate-with
   */
  with: SpaceDID$1,
  nb: struct({
    // keys SHOULD be CIDs, but we won't require it in the schema
    /**
     * @type {Schema.Schema<AccessDelegateDelegations>}
     */
    delegations: dictionary({
      value: match$2(),
    }),
  }),
  derives: (claim, proof) => {
    return (
      and$2(equalWith(claim, proof)) ||
      and$2(subsetsNbDelegations(claim, proof)) ||
      ok({})
    );
  },
});

/**
 * @typedef {Schema.Dictionary<string, Types.Link<unknown, number, number, 0 | 1>>} AccessDelegateDelegations
 */

/**
 * Parsed Capability for access/delegate
 *
 * @typedef {object} ParsedAccessDelegate
 * @property {string} can
 * @property {object} nb
 * @property {AccessDelegateDelegations} [nb.delegations]
 */

/**
 * returns whether the claimed ucan is proves by the proof ucan.
 * both are access/delegate, or at least have same semantics for `nb.delegations`, which is a set of delegations.
 * checks that the claimed delegation set is equal to or less than the proven delegation set.
 * usable with {import('@ucanto/interface').Derives}.
 *
 * @param {ParsedAccessDelegate} claim
 * @param {ParsedAccessDelegate} proof
 */
function subsetsNbDelegations(claim, proof) {
  const missingProofs = setDifference(
    delegatedCids(claim),
    new Set(delegatedCids(proof))
  );
  if (missingProofs.size > 0) {
    return fail(`unauthorized nb.delegations ${[...missingProofs].join(", ")}`);
  }
  return ok({});
}

/**
 * Checks that set of requested capabilities is a subset of the capabilities
 * that had been allowed by the owner or the delegate.
 *
 * ⚠️ This function does not currently check that say `store/add` is allowed
 * when say `store/*` was delegated, because it seems very unlikely that we
 * will ever encounter delegations for `access/authorize` at all.
 *
 * @param {Schema.Infer<CapabilityRequest>[]} claim
 * @param {Schema.Infer<CapabilityRequest>[]} proof
 */
const subsetCapabilities = (claim, proof) => {
  const allowed = new Set(proof.map((p) => p.can));
  // If everything is allowed, no need to check further because it contains
  // all the capabilities.
  if (allowed.has("*")) {
    return ok({});
  }

  // Otherwise we compute delta between what is allowed and what is requested.
  const escalated = setDifference(
    claim.map((c) => c.can),
    allowed
  );

  if (escalated.size > 0) {
    return fail(`unauthorized nb.att.can ${[...escalated].join(", ")}`);
  }

  return ok({});
};

/**
 * iterate delegated UCAN CIDs from an access/delegate capability.nb.delegations value.
 *
 * @param {ParsedAccessDelegate} delegate
 * @returns {Iterable<string>}
 */
function* delegatedCids(delegate) {
  for (const d of Object.values(delegate.nb.delegations || {})) {
    yield d.toString();
  }
}

/**
 * @template S
 * @param {Iterable<S>} minuend - set to subtract from
 * @param {Set<S>} subtrahend - subtracted from minuend
 */
function setDifference(minuend, subtrahend) {
  /** @type {Set<S>} */
  const difference = new Set();
  for (const e of minuend) {
    if (!subtrahend.has(e)) {
      difference.add(e);
    }
  }
  return difference;
}

/**
 * create a did:mailto from an email address
 *
 * @param {import("./types.js").EmailAddress} email
 * @returns {import("./types.js").DidMailto}
 */
function fromEmail(email) {
  const { domain, local } = parseEmail(email);
  const did = /** @type {const} */ (
    `did:mailto:${encodeURIComponent(domain)}:${encodeURIComponent(local)}`
  );
  return did;
}
/**
 * @param {import("./types.js").DidMailto} did
 * @returns {import("./types.js").EmailAddress}
 */
function toEmail(did) {
  const parts = did.split(":");
  if (parts[1] !== "mailto") {
    throw new Error(`DID ${did} is not a mailto did.`);
  }
  return `${decodeURIComponent(parts[3])}@${decodeURIComponent(parts[2])}`;
}
/**
 * given a string, if it is an EmailAddress, return it, otherwise throw an error.
 * Use this to parse string input to `EmailAddress` type to pass to `fromEmail` (when needed).
 * This is not meant to be a general RFC5322 (et al) email address validator, which would be more expensive.
 *
 * @param {string} input
 * @returns {import("./types.js").EmailAddress}
 */
function email(input) {
  const { domain, local } = parseEmail(input);
  /** @type {import("./types.js").EmailAddress} */
  const emailAddress = `${local}@${domain}`;
  return emailAddress;
}
/**
 * @param {string} email
 */
function parseEmail(email) {
  const atParts = email.split("@");
  if (atParts.length < 2) {
    throw new TypeError(
      `expected at least 2 @-delimtied segments, but got ${atParts.length}`
    );
  }
  const domain = atParts.at(-1) ?? "";
  const local = atParts.slice(0, -1).join("@");
  return { domain, local };
}

/**
 * Returns a `Uint8Array` of the requested size. Referenced memory will
 * be initialized to 0.
 */
function alloc(size = 0) {
  return new Uint8Array(size);
}
/**
 * Where possible returns a Uint8Array of the requested size that references
 * uninitialized memory. Only use if you are certain you will immediately
 * overwrite every value in the returned `Uint8Array`.
 */
function allocUnsafe(size = 0) {
  return new Uint8Array(size);
}

/**
 * To guarantee Uint8Array semantics, convert nodejs Buffers
 * into vanilla Uint8Arrays
 */
function asUint8Array$1(buf) {
  return buf;
}

/**
 * Can be used with Array.sort to sort and array with Uint8Array entries
 */
function compare(a, b) {
  for (let i = 0; i < a.byteLength; i++) {
    if (a[i] < b[i]) {
      return -1;
    }
    if (a[i] > b[i]) {
      return 1;
    }
  }
  if (a.byteLength > b.byteLength) {
    return 1;
  }
  if (a.byteLength < b.byteLength) {
    return -1;
  }
  return 0;
}

/**
 * Returns a new Uint8Array created by concatenating the passed Uint8Arrays
 */
function concat(arrays, length) {
  if (length == null) {
    length = arrays.reduce((acc, curr) => acc + curr.length, 0);
  }
  const output = allocUnsafe(length);
  let offset = 0;
  for (const arr of arrays) {
    output.set(arr, offset);
    offset += arr.length;
  }
  return asUint8Array$1(output);
}

baseX$2({
  prefix: "9",
  name: "base10",
  alphabet: "0123456789",
});

rfc4648$2({
  prefix: "f",
  name: "base16",
  alphabet: "0123456789abcdef",
  bitsPerChar: 4,
});
rfc4648$2({
  prefix: "F",
  name: "base16upper",
  alphabet: "0123456789ABCDEF",
  bitsPerChar: 4,
});

rfc4648$2({
  prefix: "0",
  name: "base2",
  alphabet: "01",
  bitsPerChar: 1,
});

const alphabet$1 = Array.from(
  "🚀🪐☄🛰🌌🌑🌒🌓🌔🌕🌖🌗🌘🌍🌏🌎🐉☀💻🖥💾💿😂❤😍🤣😊🙏💕😭😘👍😅👏😁🔥🥰💔💖💙😢🤔😆🙄💪😉☺👌🤗💜😔😎😇🌹🤦🎉💞✌✨🤷😱😌🌸🙌😋💗💚😏💛🙂💓🤩😄😀🖤😃💯🙈👇🎶😒🤭❣😜💋👀😪😑💥🙋😞😩😡🤪👊🥳😥🤤👉💃😳✋😚😝😴🌟😬🙃🍀🌷😻😓⭐✅🥺🌈😈🤘💦✔😣🏃💐☹🎊💘😠☝😕🌺🎂🌻😐🖕💝🙊😹🗣💫💀👑🎵🤞😛🔴😤🌼😫⚽🤙☕🏆🤫👈😮🙆🍻🍃🐶💁😲🌿🧡🎁⚡🌞🎈❌✊👋😰🤨😶🤝🚶💰🍓💢🤟🙁🚨💨🤬✈🎀🍺🤓😙💟🌱😖👶🥴▶➡❓💎💸⬇😨🌚🦋😷🕺⚠🙅😟😵👎🤲🤠🤧📌🔵💅🧐🐾🍒😗🤑🌊🤯🐷☎💧😯💆👆🎤🙇🍑❄🌴💣🐸💌📍🥀🤢👅💡💩👐📸👻🤐🤮🎼🥵🚩🍎🍊👼💍📣🥂"
);
const alphabetBytesToChars = alphabet$1.reduce((p, c, i) => {
  p[i] = c;
  return p;
}, []);
const alphabetCharsToBytes = alphabet$1.reduce((p, c, i) => {
  const codePoint = c.codePointAt(0);
  if (codePoint == null) {
    throw new Error(`Invalid character: ${c}`);
  }
  p[codePoint] = i;
  return p;
}, []);
function encode$e(data) {
  return data.reduce((p, c) => {
    p += alphabetBytesToChars[c];
    return p;
  }, "");
}
function decode$k(str) {
  const byts = [];
  for (const char of str) {
    const codePoint = char.codePointAt(0);
    if (codePoint == null) {
      throw new Error(`Invalid character: ${char}`);
    }
    const byt = alphabetCharsToBytes[codePoint];
    if (byt == null) {
      throw new Error(`Non-base256emoji character: ${char}`);
    }
    byts.push(byt);
  }
  return new Uint8Array(byts);
}
from$g({
  prefix: "🚀",
  name: "base256emoji",
  encode: encode$e,
  decode: decode$k,
});

rfc4648$2({
  prefix: "7",
  name: "base8",
  alphabet: "01234567",
  bitsPerChar: 3,
});

from$g({
  prefix: "\x00",
  name: "identity",
  encode: (buf) => toString$1(buf),
  decode: (str) => fromString(str),
});

new TextEncoder();
new TextDecoder();

/**
 * Encoding utilities
 *
 * It is recommended that you import directly with:
 * ```js
 * import * as Encoding from '@web3-storage/access/encoding'
 *
 * // or
 *
 * import { encodeDelegations } from '@web3-storage/access/encoding'
 * ```
 *
 * @module
 */
/**
 * Decode bytes into Delegations
 *
 * @template {Types.Capabilities} [T=Types.Capabilities]
 * @param {import('./types.js').BytesDelegation<T>} bytes
 */
function bytesToDelegations(bytes) {
  if (!(bytes instanceof Uint8Array) || bytes.length === 0) {
    throw new TypeError("Input should be a non-empty Uint8Array.");
  }
  const reader = CarBufferReader.fromBytes(bytes);
  const roots = reader.getRoots();
  /** @type {Types.Delegation<T>[]} */
  const delegations = [];
  for (const root of roots) {
    const rootBlock = reader.get(root);
    if (rootBlock) {
      const blocks = new Map();
      for (const block of reader.blocks()) {
        if (block.cid.toString() !== root.toString())
          blocks.set(block.cid.toString(), block);
      }
      // @ts-ignore
      delegations.push(new Delegation(rootBlock, blocks));
    } else {
      throw new Error("Failed to find root from raw delegation.");
    }
  }
  return delegations;
}

/**
 * Takes array of delegations and propagates them to their respective audiences
 * through a given space (or the current space if none is provided).
 *
 * Returns error result if agent has no current space and no space was provided.
 * Also returns error result if invocation fails.
 *
 * @param {Agent} agent - Agent connected to the w3up service.
 * @param {object} input
 * @param {API.Delegation[]} input.delegations - Delegations to propagate.
 * @param {API.SpaceDID} [input.space] - Space to propagate through.
 * @param {API.Delegation[]} [input.proofs] - Optional set of proofs to be
 * included in the invocation.
 */
const delegate$1 = async (
  agent,
  { delegations, proofs = [], space = agent.currentSpace() }
) => {
  if (!space) {
    return fail("Space must be specified");
  }
  const entries = Object.values(delegations).map((proof) => [
    proof.cid.toString(),
    proof.cid,
  ]);
  const { out } = await agent.invokeAndExecute(delegate$2, {
    with: space,
    nb: {
      delegations: Object.fromEntries(entries),
    },
    // must be embedded here because it's referenced by cid in .nb.delegations
    proofs: [...delegations, ...proofs],
  });
  return out;
};
/**
 * Requests specified `access` level from specified `account`. It invokes
 * `access/authorize` capability, if invocation succeeds it will return a
 * `PendingAccessRequest` object that can be used to poll for the requested
 * delegation through `access/claim` capability.
 *
 * @param {API.Agent} agent
 * @param {object} input
 * @param {API.AccountDID} input.account - Account from which access is requested.
 * @param {API.ProviderDID} [input.provider] - Provider that will receive the invocation.
 * @param {API.DID} [input.audience] - Principal requesting an access.
 * @param {API.Access} [input.access] - Access been requested.
 * @returns {Promise<API.Result<PendingAccessRequest, API.AccessAuthorizeFailure|API.InvocationError>>}
 */
const request$2 = async (
  agent,
  {
    account,
    provider = /** @type {API.ProviderDID} */ (agent.connection.id.did()),
    audience: audience = agent.did(),
    access = spaceAccess,
  }
) => {
  // Request access from the account.
  const { out: result } = await agent.invokeAndExecute(authorize, {
    audience: parse$1(provider),
    with: audience,
    nb: {
      iss: account,
      // New ucan spec moved to recap style layout for capabilities and new
      // `access/request` will use similar format as opposed to legacy one,
      // in the meantime we translate new format to legacy format here.
      att: [...toCapabilities$1(access)],
    },
  });
  return result.error
    ? result
    : {
        ok: new PendingAccessRequest({
          ...result.ok,
          agent,
          audience,
          provider,
        }),
      };
};
/**
 * Claims access that has been delegated to the given audience, which by
 * default is the agent's DID.
 *
 * @param {API.Agent} agent
 * @param {object} input
 * @param {API.DID} [input.audience] - Principal requesting an access.
 * @param {API.ProviderDID} [input.provider] - Provider handling the invocation.
 * @returns {Promise<API.Result<GrantedAccess, API.AccessClaimFailure|API.InvocationError>>}
 */
const claim$1 = async (
  agent,
  {
    provider = /** @type {API.ProviderDID} */ (agent.connection.id.did()),
    audience = agent.did(),
  } = {}
) => {
  const { out: result } = await agent.invokeAndExecute(claim$2, {
    audience: parse$1(provider),
    with: audience,
  });
  if (result.error) {
    return result;
  } else {
    const delegations = Object.values(result.ok.delegations);
    const proofs = /** @type {API.Tuple<API.Delegation>} */ (
      delegations.flatMap((proof) => bytesToDelegations(proof))
    );
    return { ok: new GrantedAccess({ agent, proofs }) };
  }
};
/**
 * Represents a pending access request. It can be used to poll for the requested
 * delegation.
 */
class PendingAccessRequest {
  /**
   * @typedef {object} PendingAccessRequestModel
   * @property {API.Agent} agent - Agent handling interaction.
   * @property {API.DID} audience - Principal requesting an access.
   * @property {API.ProviderDID} provider - Provider handling request.
   * @property {API.UTCUnixTimestamp} expiration - Seconds in UTC.
   * @property {API.Link} request - Link to the `access/authorize` invocation.
   *
   * @param {PendingAccessRequestModel} model
   */
  constructor(model) {
    this.model = model;
  }
  get agent() {
    return this.model.agent;
  }
  get audience() {
    return this.model.audience;
  }
  get expiration() {
    return new Date(this.model.expiration * 1000);
  }
  get request() {
    return this.model.request;
  }
  get provider() {
    return this.model.provider;
  }
  /**
   * Low level method and most likely you want to use `.claim` instead. This method will poll
   * fetch delegations **just once** and will return proofs matching to this request. Please note
   * that there may not be any matches in which case result will be `{ ok: [] }`.
   *
   * If you do want to continuously poll until request is approved or expired, you should use
   * `.claim` method instead.
   *
   * @returns {Promise<API.Result<API.Delegation[], API.InvocationError|API.AccessClaimFailure|RequestExpired>>}
   */
  async poll() {
    const { agent, audience, provider, expiration } = this.model;
    const timeout = expiration * 1000 - Date.now();
    if (timeout <= 0) {
      return { error: new RequestExpired(this.model) };
    } else {
      const result = await claim$1(agent, { audience, provider });
      return result.error
        ? result
        : {
            ok: result.ok.proofs.filter((proof) =>
              isRequestedAccess(proof, this.model)
            ),
          };
    }
  }
  /**
   * Continuously polls delegations until this request is approved or expired. Returns
   * a `GrantedAccess` object (view over the delegations) that can be used in the
   * invocations or can be saved in the agent (store) using `.save()` method.
   *
   * @param {object} options
   * @param {number} [options.interval]
   * @param {AbortSignal} [options.signal]
   * @returns {Promise<API.Result<GrantedAccess, Error>>}
   */
  async claim({ signal, interval = 250 } = {}) {
    while (signal?.aborted !== true) {
      const result = await this.poll();
      // If polling failed, return the error.
      if (result.error) {
        return result;
      }
      // If we got some matching proofs, return them.
      else if (result.ok.length > 0) {
        return {
          ok: new GrantedAccess({
            agent: this.agent,
            proofs: /** @type {API.Tuple<API.Delegation>} */ (result.ok),
          }),
        };
      }
      await new Promise((resolve) => setTimeout(resolve, interval));
    }
    return {
      error: Object.assign(new Error("Aborted"), { reason: signal.reason }),
    };
  }
}
/**
 * Error returned when pending access request expires.
 */
class RequestExpired extends Failure {
  /**
   * @param {PendingAccessRequestModel} model
   */
  constructor(model) {
    super();
    this.model = model;
  }
  get name() {
    return "RequestExpired";
  }
  get request() {
    return this.model.request;
  }
  get expiredAt() {
    return new Date(this.model.expiration * 1000);
  }
  describe() {
    return `Access request expired at ${this.expiredAt} for ${this.request} request.`;
  }
}
/**
 * View over the UCAN Delegations that grant access to a specific principal.
 */
class GrantedAccess {
  /**
   * @typedef {object} GrantedAccessModel
   * @property {API.Agent} agent - Agent that processed the request.
   * @property {API.Tuple<API.Delegation>} proofs - Delegations that grant access.
   *
   * @param {GrantedAccessModel} model
   */
  constructor(model) {
    this.model = model;
  }
  get proofs() {
    return this.model.proofs;
  }
  /**
   * Saves access into the agents proofs store so that it can be retained
   * between sessions.
   *
   * @param {object} input
   * @param {API.Agent} [input.agent]
   */
  save({ agent = this.model.agent } = {}) {
    return importAuthorization(agent, this);
  }
}
/**
 * Checks if the given delegation is caused by the passed `request` for access.
 *
 * @param {API.Delegation} delegation
 * @param {object} selector
 * @param {API.Link} selector.request
 * @returns
 */
const isRequestedAccess = (delegation, { request }) =>
  // `access/confirm` handler adds facts to the delegation issued by the account
  // so that principal requesting access can identify correct delegation when
  // access is granted.
  delegation.facts.some((fact) => `${fact["access/request"]}` === `${request}`);
/**
 * Maps access object that uses UCAN 0.10 capabilities format as opposed
 * to legacy UCAN 0.9 format used by w3up  which predates new format.
 *
 * @param {API.Access} access
 * @returns {{ can: API.Ability }[]}
 */
const toCapabilities$1 = (access) => {
  const abilities = [];
  const entries = /** @type {[API.Ability, API.Unit][]} */ (
    Object.entries(access)
  );
  for (const [can, details] of entries) {
    if (details) {
      abilities.push({ can });
    }
  }
  return abilities;
};
/**
 * Set of capabilities required by the agent to manage a space.
 */
const spaceAccess = {
  "space/*": {},
  "blob/*": {},
  "index/*": {},
  "store/*": {},
  "upload/*": {},
  "access/*": {},
  "filecoin/*": {},
  "usage/*": {},
};
/**
 * Set of capabilities required for by the agent to manage an account.
 */
const accountAccess$1 = {
  "*": {},
};

var access = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  GrantedAccess: GrantedAccess,
  accountAccess: accountAccess$1,
  claim: claim$1,
  delegate: delegate$1,
  request: request$2,
  spaceAccess: spaceAccess,
  toCapabilities: toCapabilities$1,
});

var empty$4 = {};

var nodeCrypto = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  default: empty$4,
});

/*! noble-ed25519 - MIT License (c) 2019 Paul Miller (paulmillr.com) */
const _0n = BigInt(0);
const _1n = BigInt(1);
const _2n = BigInt(2);
const _8n = BigInt(8);
const CU_O = BigInt(
  "7237005577332262213973186563042994240857116359379907606001950938285454250989"
);
const CURVE = Object.freeze({
  a: BigInt(-1),
  d: BigInt(
    "37095705934669439343138083508754565189542113879843219016388785533085940283555"
  ),
  P: BigInt(
    "57896044618658097711785492504343953926634992332820282019728792003956564819949"
  ),
  l: CU_O,
  n: CU_O,
  h: BigInt(8),
  Gx: BigInt(
    "15112221349535400772501151409588531511454012693041857206046113283949847762202"
  ),
  Gy: BigInt(
    "46316835694926478169428394003475163141307993866256225615783033603165251855960"
  ),
});
const POW_2_256 = BigInt(
  "0x10000000000000000000000000000000000000000000000000000000000000000"
);
const SQRT_M1 = BigInt(
  "19681161376707505956807079304988542015446066515923890162744021073123829784752"
);
BigInt(
  "6853475219497561581579357271197624642482790079785650197046958215289687604742"
);
const SQRT_AD_MINUS_ONE = BigInt(
  "25063068953384623474111414158702152701244531502492656460079210482610430750235"
);
const INVSQRT_A_MINUS_D = BigInt(
  "54469307008909316920995813868745141605393597292927456921205312896311721017578"
);
const ONE_MINUS_D_SQ = BigInt(
  "1159843021668779879193775521855586647937357759715417654439879720876111806838"
);
const D_MINUS_ONE_SQ = BigInt(
  "40440834346308536858101042469323190826248399146238708352240133220865137265952"
);
class ExtendedPoint {
  constructor(x, y, z, t) {
    this.x = x;
    this.y = y;
    this.z = z;
    this.t = t;
  }
  static fromAffine(p) {
    if (!(p instanceof Point)) {
      throw new TypeError("ExtendedPoint#fromAffine: expected Point");
    }
    if (p.equals(Point.ZERO)) return ExtendedPoint.ZERO;
    return new ExtendedPoint(p.x, p.y, _1n, mod(p.x * p.y));
  }
  static toAffineBatch(points) {
    const toInv = invertBatch(points.map((p) => p.z));
    return points.map((p, i) => p.toAffine(toInv[i]));
  }
  static normalizeZ(points) {
    return this.toAffineBatch(points).map(this.fromAffine);
  }
  equals(other) {
    assertExtPoint(other);
    const { x: X1, y: Y1, z: Z1 } = this;
    const { x: X2, y: Y2, z: Z2 } = other;
    const X1Z2 = mod(X1 * Z2);
    const X2Z1 = mod(X2 * Z1);
    const Y1Z2 = mod(Y1 * Z2);
    const Y2Z1 = mod(Y2 * Z1);
    return X1Z2 === X2Z1 && Y1Z2 === Y2Z1;
  }
  negate() {
    return new ExtendedPoint(mod(-this.x), this.y, this.z, mod(-this.t));
  }
  double() {
    const { x: X1, y: Y1, z: Z1 } = this;
    const { a } = CURVE;
    const A = mod(X1 * X1);
    const B = mod(Y1 * Y1);
    const C = mod(_2n * mod(Z1 * Z1));
    const D = mod(a * A);
    const x1y1 = X1 + Y1;
    const E = mod(mod(x1y1 * x1y1) - A - B);
    const G = D + B;
    const F = G - C;
    const H = D - B;
    const X3 = mod(E * F);
    const Y3 = mod(G * H);
    const T3 = mod(E * H);
    const Z3 = mod(F * G);
    return new ExtendedPoint(X3, Y3, Z3, T3);
  }
  add(other) {
    assertExtPoint(other);
    const { x: X1, y: Y1, z: Z1, t: T1 } = this;
    const { x: X2, y: Y2, z: Z2, t: T2 } = other;
    const A = mod((Y1 - X1) * (Y2 + X2));
    const B = mod((Y1 + X1) * (Y2 - X2));
    const F = mod(B - A);
    if (F === _0n) return this.double();
    const C = mod(Z1 * _2n * T2);
    const D = mod(T1 * _2n * Z2);
    const E = D + C;
    const G = B + A;
    const H = D - C;
    const X3 = mod(E * F);
    const Y3 = mod(G * H);
    const T3 = mod(E * H);
    const Z3 = mod(F * G);
    return new ExtendedPoint(X3, Y3, Z3, T3);
  }
  subtract(other) {
    return this.add(other.negate());
  }
  precomputeWindow(W) {
    const windows = 1 + 256 / W;
    const points = [];
    let p = this;
    let base = p;
    for (let window = 0; window < windows; window++) {
      base = p;
      points.push(base);
      for (let i = 1; i < 2 ** (W - 1); i++) {
        base = base.add(p);
        points.push(base);
      }
      p = base.double();
    }
    return points;
  }
  wNAF(n, affinePoint) {
    if (!affinePoint && this.equals(ExtendedPoint.BASE))
      affinePoint = Point.BASE;
    const W = (affinePoint && affinePoint._WINDOW_SIZE) || 1;
    if (256 % W) {
      throw new Error(
        "Point#wNAF: Invalid precomputation window, must be power of 2"
      );
    }
    let precomputes = affinePoint && pointPrecomputes.get(affinePoint);
    if (!precomputes) {
      precomputes = this.precomputeWindow(W);
      if (affinePoint && W !== 1) {
        precomputes = ExtendedPoint.normalizeZ(precomputes);
        pointPrecomputes.set(affinePoint, precomputes);
      }
    }
    let p = ExtendedPoint.ZERO;
    let f = ExtendedPoint.BASE;
    const windows = 1 + 256 / W;
    const windowSize = 2 ** (W - 1);
    const mask = BigInt(2 ** W - 1);
    const maxNumber = 2 ** W;
    const shiftBy = BigInt(W);
    for (let window = 0; window < windows; window++) {
      const offset = window * windowSize;
      let wbits = Number(n & mask);
      n >>= shiftBy;
      if (wbits > windowSize) {
        wbits -= maxNumber;
        n += _1n;
      }
      const offset1 = offset;
      const offset2 = offset + Math.abs(wbits) - 1;
      const cond1 = window % 2 !== 0;
      const cond2 = wbits < 0;
      if (wbits === 0) {
        f = f.add(constTimeNegate(cond1, precomputes[offset1]));
      } else {
        p = p.add(constTimeNegate(cond2, precomputes[offset2]));
      }
    }
    return ExtendedPoint.normalizeZ([p, f])[0];
  }
  multiply(scalar, affinePoint) {
    return this.wNAF(normalizeScalar(scalar, CURVE.l), affinePoint);
  }
  multiplyUnsafe(scalar) {
    let n = normalizeScalar(scalar, CURVE.l, false);
    const G = ExtendedPoint.BASE;
    const P0 = ExtendedPoint.ZERO;
    if (n === _0n) return P0;
    if (this.equals(P0) || n === _1n) return this;
    if (this.equals(G)) return this.wNAF(n);
    let p = P0;
    let d = this;
    while (n > _0n) {
      if (n & _1n) p = p.add(d);
      d = d.double();
      n >>= _1n;
    }
    return p;
  }
  isSmallOrder() {
    return this.multiplyUnsafe(CURVE.h).equals(ExtendedPoint.ZERO);
  }
  isTorsionFree() {
    let p = this.multiplyUnsafe(CURVE.l / _2n).double();
    if (CURVE.l % _2n) p = p.add(this);
    return p.equals(ExtendedPoint.ZERO);
  }
  toAffine(invZ) {
    const { x, y, z } = this;
    const is0 = this.equals(ExtendedPoint.ZERO);
    if (invZ == null) invZ = is0 ? _8n : invert(z);
    const ax = mod(x * invZ);
    const ay = mod(y * invZ);
    const zz = mod(z * invZ);
    if (is0) return Point.ZERO;
    if (zz !== _1n) throw new Error("invZ was invalid");
    return new Point(ax, ay);
  }
  fromRistrettoBytes() {
    legacyRist();
  }
  toRistrettoBytes() {
    legacyRist();
  }
  fromRistrettoHash() {
    legacyRist();
  }
}
ExtendedPoint.BASE = new ExtendedPoint(
  CURVE.Gx,
  CURVE.Gy,
  _1n,
  mod(CURVE.Gx * CURVE.Gy)
);
ExtendedPoint.ZERO = new ExtendedPoint(_0n, _1n, _1n, _0n);
function constTimeNegate(condition, item) {
  const neg = item.negate();
  return condition ? neg : item;
}
function assertExtPoint(other) {
  if (!(other instanceof ExtendedPoint))
    throw new TypeError("ExtendedPoint expected");
}
function assertRstPoint(other) {
  if (!(other instanceof RistrettoPoint))
    throw new TypeError("RistrettoPoint expected");
}
function legacyRist() {
  throw new Error("Legacy method: switch to RistrettoPoint");
}
class RistrettoPoint {
  constructor(ep) {
    this.ep = ep;
  }
  static calcElligatorRistrettoMap(r0) {
    const { d } = CURVE;
    const r = mod(SQRT_M1 * r0 * r0);
    const Ns = mod((r + _1n) * ONE_MINUS_D_SQ);
    let c = BigInt(-1);
    const D = mod((c - d * r) * mod(r + d));
    let { isValid: Ns_D_is_sq, value: s } = uvRatio(Ns, D);
    let s_ = mod(s * r0);
    if (!edIsNegative(s_)) s_ = mod(-s_);
    if (!Ns_D_is_sq) s = s_;
    if (!Ns_D_is_sq) c = r;
    const Nt = mod(c * (r - _1n) * D_MINUS_ONE_SQ - D);
    const s2 = s * s;
    const W0 = mod((s + s) * D);
    const W1 = mod(Nt * SQRT_AD_MINUS_ONE);
    const W2 = mod(_1n - s2);
    const W3 = mod(_1n + s2);
    return new ExtendedPoint(
      mod(W0 * W3),
      mod(W2 * W1),
      mod(W1 * W3),
      mod(W0 * W2)
    );
  }
  static hashToCurve(hex) {
    hex = ensureBytes(hex, 64);
    const r1 = bytes255ToNumberLE(hex.slice(0, 32));
    const R1 = this.calcElligatorRistrettoMap(r1);
    const r2 = bytes255ToNumberLE(hex.slice(32, 64));
    const R2 = this.calcElligatorRistrettoMap(r2);
    return new RistrettoPoint(R1.add(R2));
  }
  static fromHex(hex) {
    hex = ensureBytes(hex, 32);
    const { a, d } = CURVE;
    const emsg =
      "RistrettoPoint.fromHex: the hex is not valid encoding of RistrettoPoint";
    const s = bytes255ToNumberLE(hex);
    if (!equalBytes(numberTo32BytesLE(s), hex) || edIsNegative(s))
      throw new Error(emsg);
    const s2 = mod(s * s);
    const u1 = mod(_1n + a * s2);
    const u2 = mod(_1n - a * s2);
    const u1_2 = mod(u1 * u1);
    const u2_2 = mod(u2 * u2);
    const v = mod(a * d * u1_2 - u2_2);
    const { isValid, value: I } = invertSqrt(mod(v * u2_2));
    const Dx = mod(I * u2);
    const Dy = mod(I * Dx * v);
    let x = mod((s + s) * Dx);
    if (edIsNegative(x)) x = mod(-x);
    const y = mod(u1 * Dy);
    const t = mod(x * y);
    if (!isValid || edIsNegative(t) || y === _0n) throw new Error(emsg);
    return new RistrettoPoint(new ExtendedPoint(x, y, _1n, t));
  }
  toRawBytes() {
    let { x, y, z, t } = this.ep;
    const u1 = mod(mod(z + y) * mod(z - y));
    const u2 = mod(x * y);
    const u2sq = mod(u2 * u2);
    const { value: invsqrt } = invertSqrt(mod(u1 * u2sq));
    const D1 = mod(invsqrt * u1);
    const D2 = mod(invsqrt * u2);
    const zInv = mod(D1 * D2 * t);
    let D;
    if (edIsNegative(t * zInv)) {
      let _x = mod(y * SQRT_M1);
      let _y = mod(x * SQRT_M1);
      x = _x;
      y = _y;
      D = mod(D1 * INVSQRT_A_MINUS_D);
    } else {
      D = D2;
    }
    if (edIsNegative(x * zInv)) y = mod(-y);
    let s = mod((z - y) * D);
    if (edIsNegative(s)) s = mod(-s);
    return numberTo32BytesLE(s);
  }
  toHex() {
    return bytesToHex(this.toRawBytes());
  }
  toString() {
    return this.toHex();
  }
  equals(other) {
    assertRstPoint(other);
    const a = this.ep;
    const b = other.ep;
    const one = mod(a.x * b.y) === mod(a.y * b.x);
    const two = mod(a.y * b.y) === mod(a.x * b.x);
    return one || two;
  }
  add(other) {
    assertRstPoint(other);
    return new RistrettoPoint(this.ep.add(other.ep));
  }
  subtract(other) {
    assertRstPoint(other);
    return new RistrettoPoint(this.ep.subtract(other.ep));
  }
  multiply(scalar) {
    return new RistrettoPoint(this.ep.multiply(scalar));
  }
  multiplyUnsafe(scalar) {
    return new RistrettoPoint(this.ep.multiplyUnsafe(scalar));
  }
}
RistrettoPoint.BASE = new RistrettoPoint(ExtendedPoint.BASE);
RistrettoPoint.ZERO = new RistrettoPoint(ExtendedPoint.ZERO);
const pointPrecomputes = new WeakMap();
class Point {
  constructor(x, y) {
    this.x = x;
    this.y = y;
  }
  _setWindowSize(windowSize) {
    this._WINDOW_SIZE = windowSize;
    pointPrecomputes.delete(this);
  }
  static fromHex(hex, strict = true) {
    const { d, P } = CURVE;
    hex = ensureBytes(hex, 32);
    const normed = hex.slice();
    normed[31] = hex[31] & -129;
    const y = bytesToNumberLE(normed);
    if (strict && y >= P) throw new Error("Expected 0 < hex < P");
    if (!strict && y >= POW_2_256) throw new Error("Expected 0 < hex < 2**256");
    const y2 = mod(y * y);
    const u = mod(y2 - _1n);
    const v = mod(d * y2 + _1n);
    let { isValid, value: x } = uvRatio(u, v);
    if (!isValid) throw new Error("Point.fromHex: invalid y coordinate");
    const isXOdd = (x & _1n) === _1n;
    const isLastByteOdd = (hex[31] & 0x80) !== 0;
    if (isLastByteOdd !== isXOdd) {
      x = mod(-x);
    }
    return new Point(x, y);
  }
  static async fromPrivateKey(privateKey) {
    return (await getExtendedPublicKey(privateKey)).point;
  }
  toRawBytes() {
    const bytes = numberTo32BytesLE(this.y);
    bytes[31] |= this.x & _1n ? 0x80 : 0;
    return bytes;
  }
  toHex() {
    return bytesToHex(this.toRawBytes());
  }
  toX25519() {
    const { y } = this;
    const u = mod((_1n + y) * invert(_1n - y));
    return numberTo32BytesLE(u);
  }
  isTorsionFree() {
    return ExtendedPoint.fromAffine(this).isTorsionFree();
  }
  equals(other) {
    return this.x === other.x && this.y === other.y;
  }
  negate() {
    return new Point(mod(-this.x), this.y);
  }
  add(other) {
    return ExtendedPoint.fromAffine(this)
      .add(ExtendedPoint.fromAffine(other))
      .toAffine();
  }
  subtract(other) {
    return this.add(other.negate());
  }
  multiply(scalar) {
    return ExtendedPoint.fromAffine(this).multiply(scalar, this).toAffine();
  }
}
Point.BASE = new Point(CURVE.Gx, CURVE.Gy);
Point.ZERO = new Point(_0n, _1n);
class Signature {
  constructor(r, s) {
    this.r = r;
    this.s = s;
    this.assertValidity();
  }
  static fromHex(hex) {
    const bytes = ensureBytes(hex, 64);
    const r = Point.fromHex(bytes.slice(0, 32), false);
    const s = bytesToNumberLE(bytes.slice(32, 64));
    return new Signature(r, s);
  }
  assertValidity() {
    const { r, s } = this;
    if (!(r instanceof Point)) throw new Error("Expected Point instance");
    normalizeScalar(s, CURVE.l, false);
    return this;
  }
  toRawBytes() {
    const u8 = new Uint8Array(64);
    u8.set(this.r.toRawBytes());
    u8.set(numberTo32BytesLE(this.s), 32);
    return u8;
  }
  toHex() {
    return bytesToHex(this.toRawBytes());
  }
}
function concatBytes(...arrays) {
  if (!arrays.every((a) => a instanceof Uint8Array))
    throw new Error("Expected Uint8Array list");
  if (arrays.length === 1) return arrays[0];
  const length = arrays.reduce((a, arr) => a + arr.length, 0);
  const result = new Uint8Array(length);
  for (let i = 0, pad = 0; i < arrays.length; i++) {
    const arr = arrays[i];
    result.set(arr, pad);
    pad += arr.length;
  }
  return result;
}
const hexes = Array.from({ length: 256 }, (v, i) =>
  i.toString(16).padStart(2, "0")
);
function bytesToHex(uint8a) {
  if (!(uint8a instanceof Uint8Array)) throw new Error("Uint8Array expected");
  let hex = "";
  for (let i = 0; i < uint8a.length; i++) {
    hex += hexes[uint8a[i]];
  }
  return hex;
}
function hexToBytes(hex) {
  if (typeof hex !== "string") {
    throw new TypeError("hexToBytes: expected string, got " + typeof hex);
  }
  if (hex.length % 2)
    throw new Error("hexToBytes: received invalid unpadded hex");
  const array = new Uint8Array(hex.length / 2);
  for (let i = 0; i < array.length; i++) {
    const j = i * 2;
    const hexByte = hex.slice(j, j + 2);
    const byte = Number.parseInt(hexByte, 16);
    if (Number.isNaN(byte) || byte < 0)
      throw new Error("Invalid byte sequence");
    array[i] = byte;
  }
  return array;
}
function numberTo32BytesBE(num) {
  const length = 32;
  const hex = num.toString(16).padStart(length * 2, "0");
  return hexToBytes(hex);
}
function numberTo32BytesLE(num) {
  return numberTo32BytesBE(num).reverse();
}
function edIsNegative(num) {
  return (mod(num) & _1n) === _1n;
}
function bytesToNumberLE(uint8a) {
  if (!(uint8a instanceof Uint8Array)) throw new Error("Expected Uint8Array");
  return BigInt("0x" + bytesToHex(Uint8Array.from(uint8a).reverse()));
}
const MAX_255B = BigInt(
  "0x7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff"
);
function bytes255ToNumberLE(bytes) {
  return mod(bytesToNumberLE(bytes) & MAX_255B);
}
function mod(a, b = CURVE.P) {
  const res = a % b;
  return res >= _0n ? res : b + res;
}
function invert(number, modulo = CURVE.P) {
  if (number === _0n || modulo <= _0n) {
    throw new Error(
      `invert: expected positive integers, got n=${number} mod=${modulo}`
    );
  }
  let a = mod(number, modulo);
  let b = modulo;
  let x = _0n,
    u = _1n;
  while (a !== _0n) {
    const q = b / a;
    const r = b % a;
    const m = x - u * q;
    (b = a), (a = r), (x = u), (u = m);
  }
  const gcd = b;
  if (gcd !== _1n) throw new Error("invert: does not exist");
  return mod(x, modulo);
}
function invertBatch(nums, p = CURVE.P) {
  const tmp = new Array(nums.length);
  const lastMultiplied = nums.reduce((acc, num, i) => {
    if (num === _0n) return acc;
    tmp[i] = acc;
    return mod(acc * num, p);
  }, _1n);
  const inverted = invert(lastMultiplied, p);
  nums.reduceRight((acc, num, i) => {
    if (num === _0n) return acc;
    tmp[i] = mod(acc * tmp[i], p);
    return mod(acc * num, p);
  }, inverted);
  return tmp;
}
function pow2(x, power) {
  const { P } = CURVE;
  let res = x;
  while (power-- > _0n) {
    res *= res;
    res %= P;
  }
  return res;
}
function pow_2_252_3(x) {
  const { P } = CURVE;
  const _5n = BigInt(5);
  const _10n = BigInt(10);
  const _20n = BigInt(20);
  const _40n = BigInt(40);
  const _80n = BigInt(80);
  const x2 = (x * x) % P;
  const b2 = (x2 * x) % P;
  const b4 = (pow2(b2, _2n) * b2) % P;
  const b5 = (pow2(b4, _1n) * x) % P;
  const b10 = (pow2(b5, _5n) * b5) % P;
  const b20 = (pow2(b10, _10n) * b10) % P;
  const b40 = (pow2(b20, _20n) * b20) % P;
  const b80 = (pow2(b40, _40n) * b40) % P;
  const b160 = (pow2(b80, _80n) * b80) % P;
  const b240 = (pow2(b160, _80n) * b80) % P;
  const b250 = (pow2(b240, _10n) * b10) % P;
  const pow_p_5_8 = (pow2(b250, _2n) * x) % P;
  return { pow_p_5_8, b2 };
}
function uvRatio(u, v) {
  const v3 = mod(v * v * v);
  const v7 = mod(v3 * v3 * v);
  const pow = pow_2_252_3(u * v7).pow_p_5_8;
  let x = mod(u * v3 * pow);
  const vx2 = mod(v * x * x);
  const root1 = x;
  const root2 = mod(x * SQRT_M1);
  const useRoot1 = vx2 === u;
  const useRoot2 = vx2 === mod(-u);
  const noRoot = vx2 === mod(-u * SQRT_M1);
  if (useRoot1) x = root1;
  if (useRoot2 || noRoot) x = root2;
  if (edIsNegative(x)) x = mod(-x);
  return { isValid: useRoot1 || useRoot2, value: x };
}
function invertSqrt(number) {
  return uvRatio(_1n, number);
}
function modlLE(hash) {
  return mod(bytesToNumberLE(hash), CURVE.l);
}
function equalBytes(b1, b2) {
  if (b1.length !== b2.length) {
    return false;
  }
  for (let i = 0; i < b1.length; i++) {
    if (b1[i] !== b2[i]) {
      return false;
    }
  }
  return true;
}
function ensureBytes(hex, expectedLength) {
  const bytes =
    hex instanceof Uint8Array ? Uint8Array.from(hex) : hexToBytes(hex);
  if (typeof expectedLength === "number" && bytes.length !== expectedLength)
    throw new Error(`Expected ${expectedLength} bytes`);
  return bytes;
}
function normalizeScalar(num, max, strict = true) {
  if (!max) throw new TypeError("Specify max value");
  if (typeof num === "number" && Number.isSafeInteger(num)) num = BigInt(num);
  if (typeof num === "bigint" && num < max) {
    if (strict) {
      if (_0n < num) return num;
    } else {
      if (_0n <= num) return num;
    }
  }
  throw new TypeError("Expected valid scalar: 0 < scalar < max");
}
function adjustBytes25519(bytes) {
  bytes[0] &= 248;
  bytes[31] &= 127;
  bytes[31] |= 64;
  return bytes;
}
function checkPrivateKey(key) {
  key =
    typeof key === "bigint" || typeof key === "number"
      ? numberTo32BytesBE(normalizeScalar(key, POW_2_256))
      : ensureBytes(key);
  if (key.length !== 32) throw new Error(`Expected 32 bytes`);
  return key;
}
function getKeyFromHash(hashed) {
  const head = adjustBytes25519(hashed.slice(0, 32));
  const prefix = hashed.slice(32, 64);
  const scalar = modlLE(head);
  const point = Point.BASE.multiply(scalar);
  const pointBytes = point.toRawBytes();
  return { head, prefix, scalar, point, pointBytes };
}
let _sha512Sync;
async function getExtendedPublicKey(key) {
  return getKeyFromHash(await utils$1.sha512(checkPrivateKey(key)));
}
async function getPublicKey(privateKey) {
  return (await getExtendedPublicKey(privateKey)).pointBytes;
}
async function sign(message, privateKey) {
  message = ensureBytes(message);
  const { prefix, scalar, pointBytes } = await getExtendedPublicKey(privateKey);
  const r = modlLE(await utils$1.sha512(prefix, message));
  const R = Point.BASE.multiply(r);
  const k = modlLE(await utils$1.sha512(R.toRawBytes(), pointBytes, message));
  const s = mod(r + k * scalar, CURVE.l);
  return new Signature(R, s).toRawBytes();
}
function prepareVerification(sig, message, publicKey) {
  message = ensureBytes(message);
  if (!(publicKey instanceof Point))
    publicKey = Point.fromHex(publicKey, false);
  const { r, s } =
    sig instanceof Signature ? sig.assertValidity() : Signature.fromHex(sig);
  const SB = ExtendedPoint.BASE.multiplyUnsafe(s);
  return { r, s, SB, pub: publicKey, msg: message };
}
function finishVerification(publicKey, r, SB, hashed) {
  const k = modlLE(hashed);
  const kA = ExtendedPoint.fromAffine(publicKey).multiplyUnsafe(k);
  const RkA = ExtendedPoint.fromAffine(r).add(kA);
  return RkA.subtract(SB).multiplyUnsafe(CURVE.h).equals(ExtendedPoint.ZERO);
}
async function verify(sig, message, publicKey) {
  const { r, SB, msg, pub } = prepareVerification(sig, message, publicKey);
  const hashed = await utils$1.sha512(r.toRawBytes(), pub.toRawBytes(), msg);
  return finishVerification(pub, r, SB, hashed);
}
Point.BASE._setWindowSize(8);
const crypto$1 = {
  node: nodeCrypto,
  web: typeof self === "object" && "crypto" in self ? self.crypto : undefined,
};
const utils$1 = {
  bytesToHex,
  hexToBytes,
  concatBytes,
  getExtendedPublicKey,
  mod,
  invert,
  TORSION_SUBGROUP: [
    "0100000000000000000000000000000000000000000000000000000000000000",
    "c7176a703d4dd84fba3c0b760d10670f2a2053fa2c39ccc64ec7fd7792ac037a",
    "0000000000000000000000000000000000000000000000000000000000000080",
    "26e8958fc2b227b045c3f489f2ef98f0d5dfac05d3c63339b13802886d53fc05",
    "ecffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff7f",
    "26e8958fc2b227b045c3f489f2ef98f0d5dfac05d3c63339b13802886d53fc85",
    "0000000000000000000000000000000000000000000000000000000000000000",
    "c7176a703d4dd84fba3c0b760d10670f2a2053fa2c39ccc64ec7fd7792ac03fa",
  ],
  hashToPrivateScalar: (hash) => {
    hash = ensureBytes(hash);
    if (hash.length < 40 || hash.length > 1024)
      throw new Error("Expected 40-1024 bytes of private key as per FIPS 186");
    return mod(bytesToNumberLE(hash), CURVE.l - _1n) + _1n;
  },
  randomBytes: (bytesLength = 32) => {
    if (crypto$1.web) {
      return crypto$1.web.getRandomValues(new Uint8Array(bytesLength));
    } else if (crypto$1.node) {
      const { randomBytes } = crypto$1.node;
      return new Uint8Array(randomBytes(bytesLength).buffer);
    } else {
      throw new Error("The environment doesn't have randomBytes function");
    }
  },
  randomPrivateKey: () => {
    return utils$1.randomBytes(32);
  },
  sha512: async (...messages) => {
    const message = concatBytes(...messages);
    if (crypto$1.web) {
      const buffer = await crypto$1.web.subtle.digest(
        "SHA-512",
        message.buffer
      );
      return new Uint8Array(buffer);
    } else if (crypto$1.node) {
      return Uint8Array.from(
        crypto$1.node.createHash("sha512").update(message).digest()
      );
    } else {
      throw new Error("The environment doesn't have sha512 function");
    }
  },
  precompute(windowSize = 8, point = Point.BASE) {
    const cached = point.equals(Point.BASE)
      ? point
      : new Point(point.x, point.y);
    cached._setWindowSize(windowSize);
    cached.multiply(_2n);
    return cached;
  },
  sha512Sync: undefined,
};
Object.defineProperties(utils$1, {
  sha512Sync: {
    configurable: false,
    get() {
      return _sha512Sync;
    },
    set(val) {
      if (!_sha512Sync) _sha512Sync = val;
    },
  },
});

/**
 * @param {API.DID} did
 * @param {API.PrincipalParser[]} parsers
 * @return {API.Verifier}
 */
const parseWith = (did, parsers) => {
  if (did.startsWith("did:")) {
    for (const parser of parsers) {
      try {
        return parser.parse(did);
      } catch (_) {}
    }
    throw new Error(`Unsupported did ${did}`);
  } else {
    throw new Error(`Expected did instead got ${did}`);
  }
};

/**
 * @param {API.PrincipalParser} left
 * @param {API.PrincipalParser} right
 * @returns {API.ComposedDIDParser}
 */
const or$7 = (left, right) => new Parser([left, right]);

/**
 * @implements {API.ComposedDIDParser}
 */
class Parser {
  /**
   * @param {API.PrincipalParser[]} variants
   */
  constructor(variants) {
    this.variants = variants;
  }

  /**
   * @param {API.DID} did
   */
  parse(did) {
    return parseWith(did, this.variants);
  }

  /**
   * @param {API.PrincipalParser} parser
   */
  or(parser) {
    return new Parser([...this.variants, parser]);
  }
}

/**
 * @template {API.DID} ID
 * @template {API.MulticodecCode} SigAlg
 * @param {API.VerifierKey<SigAlg>} key
 * @param {ID} id
 * @returns {API.Verifier<ID, SigAlg>}
 */
const withDID$1 = (key, id) => new VerifierWithDID(id, key);

/**
 * @template {API.DID} ID
 * @template {API.MulticodecCode} SigAlg
 * @implements {API.Verifier<ID, SigAlg>}
 */
class VerifierWithDID {
  /**
   * @param {ID} id
   * @param {API.VerifierKey<SigAlg>} key
   */
  constructor(id, key) {
    this.id = id;
    this.key = key;
  }
  did() {
    return this.id;
  }

  toDIDKey() {
    return this.key.toDIDKey();
  }

  /**
   * @template T
   * @param {API.ByteView<T>} payload
   * @param {API.Signature<T, SigAlg>} signature
   * @returns {API.Await<boolean>}
   */
  verify(payload, signature) {
    return this.key.verify(payload, signature);
  }

  /**
   * @template {API.DID} ID
   * @param {ID} id
   */
  withDID(id) {
    return withDID$1(this.key, id);
  }
}

/** @type {API.EdVerifier['code']} */
const code$a = 0xed;

/** @type {API.SigAlg} */
const signatureCode$1 = EdDSA;
const signatureAlgorithm$2 = "EdDSA";
const PUBLIC_TAG_SIZE$1 = encodingLength$3(code$a);
const SIZE$1 = 32 + PUBLIC_TAG_SIZE$1;

/**
 * Takes ed25519 public key tagged with `0xed` multiformat code and creates a
 * corresponding `Principal` that can be used to verify signatures.
 *
 * @param {Uint8Array} bytes
 * @returns {API.EdVerifier}
 */
const decode$j = (bytes) => {
  const [algorithm] = decode$A(bytes);
  if (algorithm !== code$a) {
    throw new RangeError(
      `Unsupported key algorithm with multicode 0x${code$a.toString(16)}`
    );
  } else if (bytes.byteLength !== SIZE$1) {
    throw new RangeError(
      `Expected Uint8Array with byteLength ${SIZE$1}, instead got Uint8Array with byteLength ${bytes.byteLength}`
    );
  } else {
    return new Ed25519Verifier(
      bytes.buffer,
      bytes.byteOffset,
      bytes.byteLength
    );
  }
};

/**
 * @implements {API.EdVerifier}
 */
class Ed25519Verifier extends Uint8Array {
  /** @type {typeof code} */
  get code() {
    return code$a;
  }
  /** @type {typeof signatureCode} */
  get signatureCode() {
    return signatureCode$1;
  }
  /** @type {typeof signatureAlgorithm} */
  get signatureAlgorithm() {
    return signatureAlgorithm$2;
  }
  /**
   * Raw public key without a multiformat code.
   *
   * @readonly
   */
  get publicKey() {
    const key = new Uint8Array(
      this.buffer,
      this.byteOffset + PUBLIC_TAG_SIZE$1
    );
    Object.defineProperties(this, {
      publicKey: {
        value: key,
      },
    });
    return key;
  }
  /**
   * DID of the Principal in `did:key` format.
   * @returns {API.DID<"key">}
   */
  did() {
    return `did:key:${base58btc$2.encode(this)}`;
  }
  /**
   * @template T
   * @param {API.ByteView<T>} payload
   * @param {API.Signature<T, Signature.EdDSA>} signature
   * @returns {API.Await<boolean>}
   */
  verify(payload, signature) {
    return (
      signature.code === signatureCode$1 &&
      verify(signature.raw, payload, this.publicKey)
    );
  }

  /**
   * @template {API.DID} ID
   * @param {ID} id
   * @returns {API.Verifier<ID, typeof signatureCode>}
   */
  withDID(id) {
    return withDID$1(this, id);
  }

  toDIDKey() {
    return this.did();
  }
}

/**
 * @template {API.SignerImporter} L
 * @template {API.SignerImporter} R
 * @param {L} left
 * @param {R} right
 * @returns {API.CompositeImporter<[L, R]>}
 */
const or$6 = (left, right) => new Importer([left, right]);

/**
 * @template {[API.SignerImporter, ...API.SignerImporter[]]} Importers
 * @implements {API.CompositeImporter<Importers>}
 */
class Importer {
  /**
   * @param {Importers} variants
   */
  constructor(variants) {
    this.variants = variants;
    this.from = create$c(variants);
  }

  /**
   * @template {API.SignerImporter} Other
   * @param {Other} other
   * @returns {API.CompositeImporter<[Other, ...Importers]>}
   */
  or(other) {
    return new Importer([other, ...this.variants]);
  }
}

/**
 * @template {[API.SignerImporter, ...API.SignerImporter[]]} Importers
 * @param {Importers} importers
 */
const create$c = (importers) => {
  /**
   * @template {API.DID} ID - DID that can be imported, which may be a type union.
   * @template {API.SigAlg} Alg - Multicodec code corresponding to signature algorithm.
   * @param {API.SignerArchive<ID, Alg>} archive
   * @returns {API.Signer<ID, Alg>}
   */
  const from = (archive) => {
    if (archive.id.startsWith("did:key:")) {
      return /** @type {API.Signer<ID, Alg>} */ (
        importWith(archive, importers)
      );
    } else {
      for (const [name, key] of Object.entries(archive.keys)) {
        const id = /** @type {API.DIDKey} */ (name);
        const signer = /** @type {API.Signer<API.DIDKey, Alg>} */ (
          importWith(
            {
              id,
              keys: { [id]: key },
            },
            importers
          )
        );

        return signer.withDID(archive.id);
      }

      throw new Error(`Archive ${archive.id} contains no keys`);
    }
  };

  return /** @type {API.Intersection<Importers[number]['from']>} */ (from);
};

/**
 * @param {API.SignerArchive} archive
 * @param {API.SignerImporter[]} importers
 * @returns {API.Signer}
 */
const importWith = (archive, importers) => {
  for (const importer of importers) {
    try {
      return importer.from(archive);
    } catch (_) {}
  }
  throw new Error(`Unsupported signer`);
};
/**
 * @template {number} Code
 * @template {API.DID} ID
 * @param {API.Signer<API.DID<'key'>, Code>} signer
 * @param {ID} id
 * @returns {API.Signer<ID, Code>}
 */
const withDID = ({ signer, verifier }, id) =>
  new SignerWithDID(signer, verifier.withDID(id));

/**
 * @template {API.DID} ID
 * @template {number} Code
 * @implements {API.Signer<ID, Code>}
 */
class SignerWithDID {
  /**
   * @param {API.Signer<API.DID<'key'>, Code>} key
   * @param {API.Verifier<ID, Code>} verifier
   */
  constructor(key, verifier) {
    this.key = key;
    this.verifier = verifier;
  }
  /** @type {API.Signer<ID, Code>} */
  get signer() {
    return this;
  }

  get signatureAlgorithm() {
    return this.key.signatureAlgorithm;
  }
  get signatureCode() {
    return this.key.signatureCode;
  }

  /**
   * @returns {ID}
   */
  did() {
    return this.verifier.did();
  }

  toDIDKey() {
    return this.verifier.toDIDKey();
  }

  /**
   * @template {API.DID} ID
   * @param {ID} id
   */
  withDID(id) {
    return withDID(this.key, id);
  }

  /**
   * @template T
   * @param {API.ByteView<T>} payload
   */
  sign(payload) {
    return this.key.sign(payload);
  }
  /**
   * @template T
   * @param {API.ByteView<T>} payload
   * @param {API.Signature<T, Code>} signature
   */
  verify(payload, signature) {
    return this.verifier.verify(payload, signature);
  }

  toArchive() {
    const { keys } = this.key.toArchive();
    return {
      id: this.did(),
      keys,
    };
  }
}

const code$9 = 0x1300;

/** @type {'EdDSA'} */
const signatureAlgorithm$1 = signatureAlgorithm$2;

const PRIVATE_TAG_SIZE = encodingLength$3(code$9);
const PUBLIC_TAG_SIZE = encodingLength$3(code$a);
const KEY_SIZE$1 = 32;
const SIZE = PRIVATE_TAG_SIZE + KEY_SIZE$1 + PUBLIC_TAG_SIZE + KEY_SIZE$1;

const PUB_KEY_OFFSET = PRIVATE_TAG_SIZE + KEY_SIZE$1;

/**
 * Generates new issuer by generating underlying ED25519 keypair.
 * @returns {Promise<API.EdSigner>}
 */
const generate$2 = () => derive(utils$1.randomPrivateKey());

/**
 * Derives issuer from 32 byte long secret key.
 * @param {Uint8Array} secret
 * @returns {Promise<API.EdSigner>}
 */
const derive = async (secret) => {
  if (secret.byteLength !== KEY_SIZE$1) {
    throw new Error(
      `Expected Uint8Array with byteLength of ${KEY_SIZE$1} instead not ${secret.byteLength}`
    );
  }

  const publicKey = await getPublicKey(secret);
  const signer = new Ed25519Signer(SIZE);

  encodeTo$3(code$9, signer, 0);
  signer.set(secret, PRIVATE_TAG_SIZE);

  encodeTo$3(code$a, signer, PRIVATE_TAG_SIZE + KEY_SIZE$1);
  signer.set(publicKey, PRIVATE_TAG_SIZE + KEY_SIZE$1 + PUBLIC_TAG_SIZE);

  return signer;
};

/**
 * @param {API.SignerArchive<API.DID, typeof signatureCode>} archive
 * @returns {API.EdSigner}
 */
const from$b = ({ id, keys }) => {
  if (id.startsWith("did:key:")) {
    const key = keys[/** @type {API.DIDKey} */ (id)];
    if (key instanceof Uint8Array) {
      return decode$i(key);
    }
  }
  throw new TypeError(`Unsupported archive format`);
};

/**
 * @template {API.SignerImporter} O
 * @param {O} other
 */
const or$5 = (other) => or$6({ from: from$b }, other);

/**
 * @param {Uint8Array} bytes
 * @returns {API.EdSigner}
 */
const decode$i = (bytes) => {
  if (bytes.byteLength !== SIZE) {
    throw new Error(
      `Expected Uint8Array with byteLength of ${SIZE} instead not ${bytes.byteLength}`
    );
  }

  {
    const [keyCode] = decode$A(bytes);
    if (keyCode !== code$9) {
      throw new Error(`Given bytes must be a multiformat with ${code$9} tag`);
    }
  }

  {
    const [code] = decode$A(bytes.subarray(PUB_KEY_OFFSET));
    if (code !== code$a) {
      throw new Error(
        `Given bytes must contain public key in multiformats with ${code$a} tag`
      );
    }
  }

  return new Ed25519Signer(bytes);
};

/**
 * @implements {API.EdSigner}
 */
class Ed25519Signer extends Uint8Array {
  /** @type {typeof code} */
  get code() {
    return code$9;
  }
  get signer() {
    return this;
  }
  /** @type {API.EdVerifier} */
  get verifier() {
    const bytes = new Uint8Array(this.buffer, PRIVATE_TAG_SIZE + KEY_SIZE$1);
    const verifier = decode$j(bytes);

    Object.defineProperties(this, {
      verifier: {
        value: verifier,
      },
    });

    return verifier;
  }

  /**
   * Raw public key without multiformat code.
   */
  get secret() {
    const secret = new Uint8Array(this.buffer, PRIVATE_TAG_SIZE, KEY_SIZE$1);
    Object.defineProperties(this, {
      secret: {
        value: secret,
      },
    });

    return secret;
  }

  /**
   * DID of this principal in `did:key` format.
   */
  did() {
    return this.verifier.did();
  }

  toDIDKey() {
    return this.verifier.toDIDKey();
  }

  /**
   * @template {API.DID} ID
   * @param {ID} id
   * @returns {API.Signer<ID, typeof Signature.EdDSA>}
   */
  withDID(id) {
    return withDID(this, id);
  }

  /**
   * @template T
   * @param {API.ByteView<T>} payload
   * @returns {Promise<API.SignatureView<T, typeof Signature.EdDSA>>}
   */
  async sign(payload) {
    const raw = await sign(payload, this.secret);

    return create$f(this.signatureCode, raw);
  }
  /**
   * @template T
   * @param {API.ByteView<T>} payload
   * @param {API.Signature<T, typeof this.signatureCode>} signature
   */

  verify(payload, signature) {
    return this.verifier.verify(payload, signature);
  }

  get signatureAlgorithm() {
    return signatureAlgorithm$1;
  }
  get signatureCode() {
    return EdDSA;
  }

  encode() {
    return this;
  }

  toArchive() {
    const id = this.did();
    return {
      id,
      keys: { [id]: this.encode() },
    };
  }
}

/**
 * Internal assertion helpers.
 * @module
 */
/** Asserts something is positive integer. */
/** Is number an Uint8Array? Copied from utils for perf. */
function isBytes$1(a) {
  return (
    a instanceof Uint8Array ||
    (ArrayBuffer.isView(a) && a.constructor.name === "Uint8Array")
  );
}
/** Asserts something is Uint8Array. */
function abytes(b, ...lengths) {
  if (!isBytes$1(b)) throw new Error("Uint8Array expected");
  if (lengths.length > 0 && !lengths.includes(b.length))
    throw new Error(
      "Uint8Array expected of length " + lengths + ", got length=" + b.length
    );
}
/** Asserts a hash instance has not been destroyed / finished */
function aexists(instance, checkFinished = true) {
  if (instance.destroyed) throw new Error("Hash instance has been destroyed");
  if (checkFinished && instance.finished)
    throw new Error("Hash#digest() has already been called");
}
/** Asserts output is properly-sized byte array */
function aoutput(out, instance) {
  abytes(out);
  const min = instance.outputLen;
  if (out.length < min) {
    throw new Error(
      "digestInto() expects output buffer of length at least " + min
    );
  }
}

/**
 * Utilities for hex, bytes, CSPRNG.
 * @module
 */
/*! noble-hashes - MIT License (c) 2022 Paul Miller (paulmillr.com) */
// We use WebCrypto aka globalThis.crypto, which exists in browsers and node.js 16+.
// node.js versions earlier than v19 don't declare it in global scope.
// For node.js, package.json#exports field mapping rewrites import
// from `crypto` to `cryptoNode`, which imports native module.
// Makes the utils un-importable in browsers without a bundler.
// Once node.js 18 is deprecated (2025-04-30), we can just drop the import.
// Cast array to view
function createView(arr) {
  return new DataView(arr.buffer, arr.byteOffset, arr.byteLength);
}
/** The rotate right (circular right shift) operation for uint32 */
function rotr(word, shift) {
  return (word << (32 - shift)) | (word >>> shift);
}
/**
 * Convert JS string to byte array.
 * @example utf8ToBytes('abc') // new Uint8Array([97, 98, 99])
 */
function utf8ToBytes(str) {
  if (typeof str !== "string")
    throw new Error("utf8ToBytes expected string, got " + typeof str);
  return new Uint8Array(new TextEncoder().encode(str)); // https://bugzil.la/1681809
}
/**
 * Normalizes (non-hex) string or Uint8Array to Uint8Array.
 * Warning: when Uint8Array is passed, it would NOT get copied.
 * Keep in mind for future mutable operations.
 */
function toBytes$3(data) {
  if (typeof data === "string") data = utf8ToBytes(data);
  abytes(data);
  return data;
}
/** For runtime check if class implements interface */
class Hash {
  // Safe version that clones internal state
  clone() {
    return this._cloneInto();
  }
}
/** Wraps hash function, creating an interface on top of it */
function wrapConstructor(hashCons) {
  const hashC = (msg) => hashCons().update(toBytes$3(msg)).digest();
  const tmp = hashCons();
  hashC.outputLen = tmp.outputLen;
  hashC.blockLen = tmp.blockLen;
  hashC.create = () => hashCons();
  return hashC;
}

/**
 * Internal Merkle-Damgard hash utils.
 * @module
 */
/** Polyfill for Safari 14. https://caniuse.com/mdn-javascript_builtins_dataview_setbiguint64 */
function setBigUint64(view, byteOffset, value, isLE) {
  if (typeof view.setBigUint64 === "function")
    return view.setBigUint64(byteOffset, value, isLE);
  const _32n = BigInt(32);
  const _u32_max = BigInt(0xffffffff);
  const wh = Number((value >> _32n) & _u32_max);
  const wl = Number(value & _u32_max);
  const h = isLE ? 4 : 0;
  const l = isLE ? 0 : 4;
  view.setUint32(byteOffset + h, wh, isLE);
  view.setUint32(byteOffset + l, wl, isLE);
}
/** Choice: a ? b : c */
function Chi(a, b, c) {
  return (a & b) ^ (~a & c);
}
/** Majority function, true if any two inputs is true. */
function Maj(a, b, c) {
  return (a & b) ^ (a & c) ^ (b & c);
}
/**
 * Merkle-Damgard hash construction base class.
 * Could be used to create MD5, RIPEMD, SHA1, SHA2.
 */
class HashMD extends Hash {
  constructor(blockLen, outputLen, padOffset, isLE) {
    super();
    this.blockLen = blockLen;
    this.outputLen = outputLen;
    this.padOffset = padOffset;
    this.isLE = isLE;
    this.finished = false;
    this.length = 0;
    this.pos = 0;
    this.destroyed = false;
    this.buffer = new Uint8Array(blockLen);
    this.view = createView(this.buffer);
  }
  update(data) {
    aexists(this);
    const { view, buffer, blockLen } = this;
    data = toBytes$3(data);
    const len = data.length;
    for (let pos = 0; pos < len; ) {
      const take = Math.min(blockLen - this.pos, len - pos);
      // Fast path: we have at least one block in input, cast it to view and process
      if (take === blockLen) {
        const dataView = createView(data);
        for (; blockLen <= len - pos; pos += blockLen)
          this.process(dataView, pos);
        continue;
      }
      buffer.set(data.subarray(pos, pos + take), this.pos);
      this.pos += take;
      pos += take;
      if (this.pos === blockLen) {
        this.process(view, 0);
        this.pos = 0;
      }
    }
    this.length += data.length;
    this.roundClean();
    return this;
  }
  digestInto(out) {
    aexists(this);
    aoutput(out, this);
    this.finished = true;
    // Padding
    // We can avoid allocation of buffer for padding completely if it
    // was previously not allocated here. But it won't change performance.
    const { buffer, view, blockLen, isLE } = this;
    let { pos } = this;
    // append the bit '1' to the message
    buffer[pos++] = 0b10000000;
    this.buffer.subarray(pos).fill(0);
    // we have less than padOffset left in buffer, so we cannot put length in
    // current block, need process it and pad again
    if (this.padOffset > blockLen - pos) {
      this.process(view, 0);
      pos = 0;
    }
    // Pad until full block byte with zeros
    for (let i = pos; i < blockLen; i++) buffer[i] = 0;
    // Note: sha512 requires length to be 128bit integer, but length in JS will overflow before that
    // You need to write around 2 exabytes (u64_max / 8 / (1024**6)) for this to happen.
    // So we just write lowest 64 bits of that value.
    setBigUint64(view, blockLen - 8, BigInt(this.length * 8), isLE);
    this.process(view, 0);
    const oview = createView(out);
    const len = this.outputLen;
    // NOTE: we do division by 4 later, which should be fused in single op with modulo by JIT
    if (len % 4) throw new Error("_sha2: outputLen should be aligned to 32bit");
    const outLen = len / 4;
    const state = this.get();
    if (outLen > state.length)
      throw new Error("_sha2: outputLen bigger than state");
    for (let i = 0; i < outLen; i++) oview.setUint32(4 * i, state[i], isLE);
  }
  digest() {
    const { buffer, outputLen } = this;
    this.digestInto(buffer);
    const res = buffer.slice(0, outputLen);
    this.destroy();
    return res;
  }
  _cloneInto(to) {
    to || (to = new this.constructor());
    to.set(...this.get());
    const { blockLen, buffer, length, finished, destroyed, pos } = this;
    to.length = length;
    to.pos = pos;
    to.finished = finished;
    to.destroyed = destroyed;
    if (length % blockLen) to.buffer.set(buffer);
    return to;
  }
}

/**
 * SHA2-256 a.k.a. sha256. In JS, it is the fastest hash, even faster than Blake3.
 *
 * To break sha256 using birthday attack, attackers need to try 2^128 hashes.
 * BTC network is doing 2^70 hashes/sec (2^95 hashes/year) as per 2025.
 *
 * Check out [FIPS 180-4](https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.180-4.pdf).
 * @module
 */
/** Round constants: first 32 bits of fractional parts of the cube roots of the first 64 primes 2..311). */
// prettier-ignore
const SHA256_K = /* @__PURE__ */ new Uint32Array([
    0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5, 0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5,
    0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3, 0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174,
    0xe49b69c1, 0xefbe4786, 0x0fc19dc6, 0x240ca1cc, 0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,
    0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7, 0xc6e00bf3, 0xd5a79147, 0x06ca6351, 0x14292967,
    0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13, 0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85,
    0xa2bfe8a1, 0xa81a664b, 0xc24b8b70, 0xc76c51a3, 0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,
    0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5, 0x391c0cb3, 0x4ed8aa4a, 0x5b9cca4f, 0x682e6ff3,
    0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208, 0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2
]);
/** Initial state: first 32 bits of fractional parts of the square roots of the first 8 primes 2..19. */
// prettier-ignore
const SHA256_IV = /* @__PURE__ */ new Uint32Array([
    0x6a09e667, 0xbb67ae85, 0x3c6ef372, 0xa54ff53a, 0x510e527f, 0x9b05688c, 0x1f83d9ab, 0x5be0cd19
]);
/**
 * Temporary buffer, not used to store anything between runs.
 * Named this way because it matches specification.
 */
const SHA256_W = /* @__PURE__ */ new Uint32Array(64);
let SHA256$1 = class SHA256 extends HashMD {
  constructor() {
    super(64, 32, 8, false);
    // We cannot use array here since array allows indexing by variable
    // which means optimizer/compiler cannot use registers.
    this.A = SHA256_IV[0] | 0;
    this.B = SHA256_IV[1] | 0;
    this.C = SHA256_IV[2] | 0;
    this.D = SHA256_IV[3] | 0;
    this.E = SHA256_IV[4] | 0;
    this.F = SHA256_IV[5] | 0;
    this.G = SHA256_IV[6] | 0;
    this.H = SHA256_IV[7] | 0;
  }
  get() {
    const { A, B, C, D, E, F, G, H } = this;
    return [A, B, C, D, E, F, G, H];
  }
  // prettier-ignore
  set(A, B, C, D, E, F, G, H) {
        this.A = A | 0;
        this.B = B | 0;
        this.C = C | 0;
        this.D = D | 0;
        this.E = E | 0;
        this.F = F | 0;
        this.G = G | 0;
        this.H = H | 0;
    }
  process(view, offset) {
    // Extend the first 16 words into the remaining 48 words w[16..63] of the message schedule array
    for (let i = 0; i < 16; i++, offset += 4)
      SHA256_W[i] = view.getUint32(offset, false);
    for (let i = 16; i < 64; i++) {
      const W15 = SHA256_W[i - 15];
      const W2 = SHA256_W[i - 2];
      const s0 = rotr(W15, 7) ^ rotr(W15, 18) ^ (W15 >>> 3);
      const s1 = rotr(W2, 17) ^ rotr(W2, 19) ^ (W2 >>> 10);
      SHA256_W[i] = (s1 + SHA256_W[i - 7] + s0 + SHA256_W[i - 16]) | 0;
    }
    // Compression function main loop, 64 rounds
    let { A, B, C, D, E, F, G, H } = this;
    for (let i = 0; i < 64; i++) {
      const sigma1 = rotr(E, 6) ^ rotr(E, 11) ^ rotr(E, 25);
      const T1 = (H + sigma1 + Chi(E, F, G) + SHA256_K[i] + SHA256_W[i]) | 0;
      const sigma0 = rotr(A, 2) ^ rotr(A, 13) ^ rotr(A, 22);
      const T2 = (sigma0 + Maj(A, B, C)) | 0;
      H = G;
      G = F;
      F = E;
      E = (D + T1) | 0;
      D = C;
      C = B;
      B = A;
      A = (T1 + T2) | 0;
    }
    // Add the compressed chunk to the current hash value
    A = (A + this.A) | 0;
    B = (B + this.B) | 0;
    C = (C + this.C) | 0;
    D = (D + this.D) | 0;
    E = (E + this.E) | 0;
    F = (F + this.F) | 0;
    G = (G + this.G) | 0;
    H = (H + this.H) | 0;
    this.set(A, B, C, D, E, F, G, H);
  }
  roundClean() {
    SHA256_W.fill(0);
  }
  destroy() {
    this.set(0, 0, 0, 0, 0, 0, 0, 0);
    this.buffer.fill(0);
  }
};
/** SHA2-256 hash function */
const sha256$3 = /* @__PURE__ */ wrapConstructor(() => new SHA256$1());

/*! scure-base - MIT License (c) 2022 Paul Miller (paulmillr.com) */
function isBytes(a) {
  return (
    a instanceof Uint8Array ||
    (ArrayBuffer.isView(a) && a.constructor.name === "Uint8Array")
  );
}
function isArrayOf(isString, arr) {
  if (!Array.isArray(arr)) return false;
  if (arr.length === 0) return true;
  if (isString) {
    return arr.every((item) => typeof item === "string");
  } else {
    return arr.every((item) => Number.isSafeInteger(item));
  }
}
// no abytes: seems to have 10% slowdown. Why?!
function afn(input) {
  if (typeof input !== "function") throw new Error("function expected");
  return true;
}
function astr(label, input) {
  if (typeof input !== "string") throw new Error(`${label}: string expected`);
  return true;
}
function anumber(n) {
  if (!Number.isSafeInteger(n)) throw new Error(`invalid integer: ${n}`);
}
function aArr(input) {
  if (!Array.isArray(input)) throw new Error("array expected");
}
function astrArr(label, input) {
  if (!isArrayOf(true, input))
    throw new Error(`${label}: array of strings expected`);
}
function anumArr(label, input) {
  if (!isArrayOf(false, input))
    throw new Error(`${label}: array of numbers expected`);
}
/**
 * @__NO_SIDE_EFFECTS__
 */
function chain(...args) {
  const id = (a) => a;
  // Wrap call in closure so JIT can inline calls
  const wrap = (a, b) => (c) => a(b(c));
  // Construct chain of args[-1].encode(args[-2].encode([...]))
  const encode = args.map((x) => x.encode).reduceRight(wrap, id);
  // Construct chain of args[0].decode(args[1].decode(...))
  const decode = args.map((x) => x.decode).reduce(wrap, id);
  return { encode, decode };
}
/**
 * Encodes integer radix representation to array of strings using alphabet and back.
 * Could also be array of strings.
 * @__NO_SIDE_EFFECTS__
 */
function alphabet(letters) {
  // mapping 1 to "b"
  const lettersA = typeof letters === "string" ? letters.split("") : letters;
  const len = lettersA.length;
  astrArr("alphabet", lettersA);
  // mapping "b" to 1
  const indexes = new Map(lettersA.map((l, i) => [l, i]));
  return {
    encode: (digits) => {
      aArr(digits);
      return digits.map((i) => {
        if (!Number.isSafeInteger(i) || i < 0 || i >= len)
          throw new Error(
            `alphabet.encode: digit index outside alphabet "${i}". Allowed: ${letters}`
          );
        return lettersA[i];
      });
    },
    decode: (input) => {
      aArr(input);
      return input.map((letter) => {
        astr("alphabet.decode", letter);
        const i = indexes.get(letter);
        if (i === undefined)
          throw new Error(`Unknown letter: "${letter}". Allowed: ${letters}`);
        return i;
      });
    },
  };
}
/**
 * @__NO_SIDE_EFFECTS__
 */
function join$1(separator = "") {
  astr("join", separator);
  return {
    encode: (from) => {
      astrArr("join.decode", from);
      return from.join(separator);
    },
    decode: (to) => {
      astr("join.decode", to);
      return to.split(separator);
    },
  };
}
/**
 * Pad strings array so it has integer number of bits
 * @__NO_SIDE_EFFECTS__
 */
function padding$1(bits, chr = "=") {
  anumber(bits);
  astr("padding", chr);
  return {
    encode(data) {
      astrArr("padding.encode", data);
      while ((data.length * bits) % 8) data.push(chr);
      return data;
    },
    decode(input) {
      astrArr("padding.decode", input);
      let end = input.length;
      if ((end * bits) % 8)
        throw new Error(
          "padding: invalid, string should have whole number of bytes"
        );
      for (; end > 0 && input[end - 1] === chr; end--) {
        const last = end - 1;
        const byte = last * bits;
        if (byte % 8 === 0)
          throw new Error("padding: invalid, string has too much padding");
      }
      return input.slice(0, end);
    },
  };
}
/**
 * Slow: O(n^2) time complexity
 */
function convertRadix(data, from, to) {
  // base 1 is impossible
  if (from < 2)
    throw new Error(
      `convertRadix: invalid from=${from}, base cannot be less than 2`
    );
  if (to < 2)
    throw new Error(
      `convertRadix: invalid to=${to}, base cannot be less than 2`
    );
  aArr(data);
  if (!data.length) return [];
  let pos = 0;
  const res = [];
  const digits = Array.from(data, (d) => {
    anumber(d);
    if (d < 0 || d >= from) throw new Error(`invalid integer: ${d}`);
    return d;
  });
  const dlen = digits.length;
  while (true) {
    let carry = 0;
    let done = true;
    for (let i = pos; i < dlen; i++) {
      const digit = digits[i];
      const fromCarry = from * carry;
      const digitBase = fromCarry + digit;
      if (
        !Number.isSafeInteger(digitBase) ||
        fromCarry / from !== carry ||
        digitBase - digit !== fromCarry
      ) {
        throw new Error("convertRadix: carry overflow");
      }
      const div = digitBase / to;
      carry = digitBase % to;
      const rounded = Math.floor(div);
      digits[i] = rounded;
      if (!Number.isSafeInteger(rounded) || rounded * to + carry !== digitBase)
        throw new Error("convertRadix: carry overflow");
      if (!done) continue;
      else if (!rounded) pos = i;
      else done = false;
    }
    res.push(carry);
    if (done) break;
  }
  for (let i = 0; i < data.length - 1 && data[i] === 0; i++) res.push(0);
  return res.reverse();
}
const gcd = (a, b) => (b === 0 ? a : gcd(b, a % b));
const radix2carry = /* @__NO_SIDE_EFFECTS__ */ (from, to) =>
  from + (to - gcd(from, to));
const powers = /* @__PURE__ */ (() => {
  let res = [];
  for (let i = 0; i < 40; i++) res.push(2 ** i);
  return res;
})();
/**
 * Implemented with numbers, because BigInt is 5x slower
 */
function convertRadix2(data, from, to, padding) {
  aArr(data);
  if (from <= 0 || from > 32)
    throw new Error(`convertRadix2: wrong from=${from}`);
  if (to <= 0 || to > 32) throw new Error(`convertRadix2: wrong to=${to}`);
  if (radix2carry(from, to) > 32) {
    throw new Error(
      `convertRadix2: carry overflow from=${from} to=${to} carryBits=${radix2carry(
        from,
        to
      )}`
    );
  }
  let carry = 0;
  let pos = 0; // bitwise position in current element
  const max = powers[from];
  const mask = powers[to] - 1;
  const res = [];
  for (const n of data) {
    anumber(n);
    if (n >= max)
      throw new Error(`convertRadix2: invalid data word=${n} from=${from}`);
    carry = (carry << from) | n;
    if (pos + from > 32)
      throw new Error(`convertRadix2: carry overflow pos=${pos} from=${from}`);
    pos += from;
    for (; pos >= to; pos -= to) res.push(((carry >> (pos - to)) & mask) >>> 0);
    const pow = powers[pos];
    if (pow === undefined) throw new Error("invalid carry");
    carry &= pow - 1; // clean carry, otherwise it will cause overflow
  }
  carry = (carry << (to - pos)) & mask;
  if (!padding && pos >= from) throw new Error("Excess padding");
  if (!padding && carry > 0) throw new Error(`Non-zero padding: ${carry}`);
  if (padding && pos > 0) res.push(carry >>> 0);
  return res;
}
/**
 * @__NO_SIDE_EFFECTS__
 */
function radix(num) {
  anumber(num);
  const _256 = 2 ** 8;
  return {
    encode: (bytes) => {
      if (!isBytes(bytes))
        throw new Error("radix.encode input should be Uint8Array");
      return convertRadix(Array.from(bytes), _256, num);
    },
    decode: (digits) => {
      anumArr("radix.decode", digits);
      return Uint8Array.from(convertRadix(digits, num, _256));
    },
  };
}
/**
 * If both bases are power of same number (like `2**8 <-> 2**64`),
 * there is a linear algorithm. For now we have implementation for power-of-two bases only.
 * @__NO_SIDE_EFFECTS__
 */
function radix2(bits, revPadding = false) {
  anumber(bits);
  if (bits <= 0 || bits > 32)
    throw new Error("radix2: bits should be in (0..32]");
  if (radix2carry(8, bits) > 32 || radix2carry(bits, 8) > 32)
    throw new Error("radix2: carry overflow");
  return {
    encode: (bytes) => {
      if (!isBytes(bytes))
        throw new Error("radix2.encode input should be Uint8Array");
      return convertRadix2(Array.from(bytes), 8, bits, !revPadding);
    },
    decode: (digits) => {
      anumArr("radix2.decode", digits);
      return Uint8Array.from(convertRadix2(digits, bits, 8, revPadding));
    },
  };
}
function checksum(len, fn) {
  anumber(len);
  afn(fn);
  return {
    encode(data) {
      if (!isBytes(data))
        throw new Error("checksum.encode: input should be Uint8Array");
      const sum = fn(data).slice(0, len);
      const res = new Uint8Array(data.length + len);
      res.set(data);
      res.set(sum, data.length);
      return res;
    },
    decode(data) {
      if (!isBytes(data))
        throw new Error("checksum.decode: input should be Uint8Array");
      const payload = data.slice(0, -len);
      const oldChecksum = data.slice(-len);
      const newChecksum = fn(payload).slice(0, len);
      for (let i = 0; i < len; i++)
        if (newChecksum[i] !== oldChecksum[i])
          throw new Error("Invalid checksum");
      return payload;
    },
  };
}
// prettier-ignore
const utils = {
    alphabet, chain, checksum, convertRadix, convertRadix2, radix, radix2, join: join$1, padding: padding$1,
};

/**
 * Audited & minimal JS implementation of
 * [BIP39 mnemonic phrases](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki).
 * @module
 * @example
```js
import * as bip39 from '@scure/bip39';
import { wordlist } from '@scure/bip39/wordlists/english';
const mn = bip39.generateMnemonic(wordlist);
console.log(mn);
const ent = bip39.mnemonicToEntropy(mn, wordlist)
bip39.entropyToMnemonic(ent, wordlist);
bip39.validateMnemonic(mn, wordlist);
await bip39.mnemonicToSeed(mn, 'password');
bip39.mnemonicToSeedSync(mn, 'password');

// Wordlists
import { wordlist as czech } from '@scure/bip39/wordlists/czech';
import { wordlist as english } from '@scure/bip39/wordlists/english';
import { wordlist as french } from '@scure/bip39/wordlists/french';
import { wordlist as italian } from '@scure/bip39/wordlists/italian';
import { wordlist as japanese } from '@scure/bip39/wordlists/japanese';
import { wordlist as korean } from '@scure/bip39/wordlists/korean';
import { wordlist as portuguese } from '@scure/bip39/wordlists/portuguese';
import { wordlist as simplifiedChinese } from '@scure/bip39/wordlists/simplified-chinese';
import { wordlist as spanish } from '@scure/bip39/wordlists/spanish';
import { wordlist as traditionalChinese } from '@scure/bip39/wordlists/traditional-chinese';
```
 */
/*! scure-bip39 - MIT License (c) 2022 Patricio Palladino, Paul Miller (paulmillr.com) */
// Japanese wordlist
const isJapanese = (wordlist) =>
  wordlist[0] === "\u3042\u3044\u3053\u304f\u3057\u3093";
// Normalization replaces equivalent sequences of characters
// so that any two texts that are equivalent will be reduced
// to the same sequence of code points, called the normal form of the original text.
// https://tonsky.me/blog/unicode/#why-is-a----
function nfkd(str) {
  if (typeof str !== "string")
    throw new TypeError("invalid mnemonic type: " + typeof str);
  return str.normalize("NFKD");
}
function normalize(str) {
  const norm = nfkd(str);
  const words = norm.split(" ");
  if (![12, 15, 18, 21, 24].includes(words.length))
    throw new Error("Invalid mnemonic");
  return { nfkd: norm, words };
}
function aentropy(ent) {
  abytes(ent, 16, 20, 24, 28, 32);
}
const calcChecksum = (entropy) => {
  // Checksum is ent.length/4 bits long
  const bitsLeft = 8 - entropy.length / 4;
  // Zero rightmost "bitsLeft" bits in byte
  // For example: bitsLeft=4 val=10111101 -> 10110000
  return new Uint8Array([(sha256$3(entropy)[0] >> bitsLeft) << bitsLeft]);
};
function getCoder(wordlist) {
  if (
    !Array.isArray(wordlist) ||
    wordlist.length !== 2048 ||
    typeof wordlist[0] !== "string"
  )
    throw new Error("Wordlist: expected array of 2048 strings");
  wordlist.forEach((i) => {
    if (typeof i !== "string")
      throw new Error("wordlist: non-string element: " + i);
  });
  return utils.chain(
    utils.checksum(1, calcChecksum),
    utils.radix2(11, true),
    utils.alphabet(wordlist)
  );
}
/**
 * Reversible: Converts mnemonic string to raw entropy in form of byte array.
 * @param mnemonic 12-24 words
 * @param wordlist imported wordlist for specific language
 * @example
 * const mnem = 'legal winner thank year wave sausage worth useful legal winner thank yellow';
 * mnemonicToEntropy(mnem, wordlist)
 * // Produces
 * new Uint8Array([
 *   0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f,
 *   0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f
 * ])
 */
function mnemonicToEntropy(mnemonic, wordlist) {
  const { words } = normalize(mnemonic);
  const entropy = getCoder(wordlist).decode(words);
  aentropy(entropy);
  return entropy;
}
/**
 * Reversible: Converts raw entropy in form of byte array to mnemonic string.
 * @param entropy byte array
 * @param wordlist imported wordlist for specific language
 * @returns 12-24 words
 * @example
 * const ent = new Uint8Array([
 *   0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f,
 *   0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f
 * ]);
 * entropyToMnemonic(ent, wordlist);
 * // 'legal winner thank year wave sausage worth useful legal winner thank yellow'
 */
function entropyToMnemonic(entropy, wordlist) {
  aentropy(entropy);
  const words = getCoder(wordlist).encode(entropy);
  return words.join(isJapanese(wordlist) ? "\u3000" : " ");
}

const wordlist = `abandon
ability
able
about
above
absent
absorb
abstract
absurd
abuse
access
accident
account
accuse
achieve
acid
acoustic
acquire
across
act
action
actor
actress
actual
adapt
add
addict
address
adjust
admit
adult
advance
advice
aerobic
affair
afford
afraid
again
age
agent
agree
ahead
aim
air
airport
aisle
alarm
album
alcohol
alert
alien
all
alley
allow
almost
alone
alpha
already
also
alter
always
amateur
amazing
among
amount
amused
analyst
anchor
ancient
anger
angle
angry
animal
ankle
announce
annual
another
answer
antenna
antique
anxiety
any
apart
apology
appear
apple
approve
april
arch
arctic
area
arena
argue
arm
armed
armor
army
around
arrange
arrest
arrive
arrow
art
artefact
artist
artwork
ask
aspect
assault
asset
assist
assume
asthma
athlete
atom
attack
attend
attitude
attract
auction
audit
august
aunt
author
auto
autumn
average
avocado
avoid
awake
aware
away
awesome
awful
awkward
axis
baby
bachelor
bacon
badge
bag
balance
balcony
ball
bamboo
banana
banner
bar
barely
bargain
barrel
base
basic
basket
battle
beach
bean
beauty
because
become
beef
before
begin
behave
behind
believe
below
belt
bench
benefit
best
betray
better
between
beyond
bicycle
bid
bike
bind
biology
bird
birth
bitter
black
blade
blame
blanket
blast
bleak
bless
blind
blood
blossom
blouse
blue
blur
blush
board
boat
body
boil
bomb
bone
bonus
book
boost
border
boring
borrow
boss
bottom
bounce
box
boy
bracket
brain
brand
brass
brave
bread
breeze
brick
bridge
brief
bright
bring
brisk
broccoli
broken
bronze
broom
brother
brown
brush
bubble
buddy
budget
buffalo
build
bulb
bulk
bullet
bundle
bunker
burden
burger
burst
bus
business
busy
butter
buyer
buzz
cabbage
cabin
cable
cactus
cage
cake
call
calm
camera
camp
can
canal
cancel
candy
cannon
canoe
canvas
canyon
capable
capital
captain
car
carbon
card
cargo
carpet
carry
cart
case
cash
casino
castle
casual
cat
catalog
catch
category
cattle
caught
cause
caution
cave
ceiling
celery
cement
census
century
cereal
certain
chair
chalk
champion
change
chaos
chapter
charge
chase
chat
cheap
check
cheese
chef
cherry
chest
chicken
chief
child
chimney
choice
choose
chronic
chuckle
chunk
churn
cigar
cinnamon
circle
citizen
city
civil
claim
clap
clarify
claw
clay
clean
clerk
clever
click
client
cliff
climb
clinic
clip
clock
clog
close
cloth
cloud
clown
club
clump
cluster
clutch
coach
coast
coconut
code
coffee
coil
coin
collect
color
column
combine
come
comfort
comic
common
company
concert
conduct
confirm
congress
connect
consider
control
convince
cook
cool
copper
copy
coral
core
corn
correct
cost
cotton
couch
country
couple
course
cousin
cover
coyote
crack
cradle
craft
cram
crane
crash
crater
crawl
crazy
cream
credit
creek
crew
cricket
crime
crisp
critic
crop
cross
crouch
crowd
crucial
cruel
cruise
crumble
crunch
crush
cry
crystal
cube
culture
cup
cupboard
curious
current
curtain
curve
cushion
custom
cute
cycle
dad
damage
damp
dance
danger
daring
dash
daughter
dawn
day
deal
debate
debris
decade
december
decide
decline
decorate
decrease
deer
defense
define
defy
degree
delay
deliver
demand
demise
denial
dentist
deny
depart
depend
deposit
depth
deputy
derive
describe
desert
design
desk
despair
destroy
detail
detect
develop
device
devote
diagram
dial
diamond
diary
dice
diesel
diet
differ
digital
dignity
dilemma
dinner
dinosaur
direct
dirt
disagree
discover
disease
dish
dismiss
disorder
display
distance
divert
divide
divorce
dizzy
doctor
document
dog
doll
dolphin
domain
donate
donkey
donor
door
dose
double
dove
draft
dragon
drama
drastic
draw
dream
dress
drift
drill
drink
drip
drive
drop
drum
dry
duck
dumb
dune
during
dust
dutch
duty
dwarf
dynamic
eager
eagle
early
earn
earth
easily
east
easy
echo
ecology
economy
edge
edit
educate
effort
egg
eight
either
elbow
elder
electric
elegant
element
elephant
elevator
elite
else
embark
embody
embrace
emerge
emotion
employ
empower
empty
enable
enact
end
endless
endorse
enemy
energy
enforce
engage
engine
enhance
enjoy
enlist
enough
enrich
enroll
ensure
enter
entire
entry
envelope
episode
equal
equip
era
erase
erode
erosion
error
erupt
escape
essay
essence
estate
eternal
ethics
evidence
evil
evoke
evolve
exact
example
excess
exchange
excite
exclude
excuse
execute
exercise
exhaust
exhibit
exile
exist
exit
exotic
expand
expect
expire
explain
expose
express
extend
extra
eye
eyebrow
fabric
face
faculty
fade
faint
faith
fall
false
fame
family
famous
fan
fancy
fantasy
farm
fashion
fat
fatal
father
fatigue
fault
favorite
feature
february
federal
fee
feed
feel
female
fence
festival
fetch
fever
few
fiber
fiction
field
figure
file
film
filter
final
find
fine
finger
finish
fire
firm
first
fiscal
fish
fit
fitness
fix
flag
flame
flash
flat
flavor
flee
flight
flip
float
flock
floor
flower
fluid
flush
fly
foam
focus
fog
foil
fold
follow
food
foot
force
forest
forget
fork
fortune
forum
forward
fossil
foster
found
fox
fragile
frame
frequent
fresh
friend
fringe
frog
front
frost
frown
frozen
fruit
fuel
fun
funny
furnace
fury
future
gadget
gain
galaxy
gallery
game
gap
garage
garbage
garden
garlic
garment
gas
gasp
gate
gather
gauge
gaze
general
genius
genre
gentle
genuine
gesture
ghost
giant
gift
giggle
ginger
giraffe
girl
give
glad
glance
glare
glass
glide
glimpse
globe
gloom
glory
glove
glow
glue
goat
goddess
gold
good
goose
gorilla
gospel
gossip
govern
gown
grab
grace
grain
grant
grape
grass
gravity
great
green
grid
grief
grit
grocery
group
grow
grunt
guard
guess
guide
guilt
guitar
gun
gym
habit
hair
half
hammer
hamster
hand
happy
harbor
hard
harsh
harvest
hat
have
hawk
hazard
head
health
heart
heavy
hedgehog
height
hello
helmet
help
hen
hero
hidden
high
hill
hint
hip
hire
history
hobby
hockey
hold
hole
holiday
hollow
home
honey
hood
hope
horn
horror
horse
hospital
host
hotel
hour
hover
hub
huge
human
humble
humor
hundred
hungry
hunt
hurdle
hurry
hurt
husband
hybrid
ice
icon
idea
identify
idle
ignore
ill
illegal
illness
image
imitate
immense
immune
impact
impose
improve
impulse
inch
include
income
increase
index
indicate
indoor
industry
infant
inflict
inform
inhale
inherit
initial
inject
injury
inmate
inner
innocent
input
inquiry
insane
insect
inside
inspire
install
intact
interest
into
invest
invite
involve
iron
island
isolate
issue
item
ivory
jacket
jaguar
jar
jazz
jealous
jeans
jelly
jewel
job
join
joke
journey
joy
judge
juice
jump
jungle
junior
junk
just
kangaroo
keen
keep
ketchup
key
kick
kid
kidney
kind
kingdom
kiss
kit
kitchen
kite
kitten
kiwi
knee
knife
knock
know
lab
label
labor
ladder
lady
lake
lamp
language
laptop
large
later
latin
laugh
laundry
lava
law
lawn
lawsuit
layer
lazy
leader
leaf
learn
leave
lecture
left
leg
legal
legend
leisure
lemon
lend
length
lens
leopard
lesson
letter
level
liar
liberty
library
license
life
lift
light
like
limb
limit
link
lion
liquid
list
little
live
lizard
load
loan
lobster
local
lock
logic
lonely
long
loop
lottery
loud
lounge
love
loyal
lucky
luggage
lumber
lunar
lunch
luxury
lyrics
machine
mad
magic
magnet
maid
mail
main
major
make
mammal
man
manage
mandate
mango
mansion
manual
maple
marble
march
margin
marine
market
marriage
mask
mass
master
match
material
math
matrix
matter
maximum
maze
meadow
mean
measure
meat
mechanic
medal
media
melody
melt
member
memory
mention
menu
mercy
merge
merit
merry
mesh
message
metal
method
middle
midnight
milk
million
mimic
mind
minimum
minor
minute
miracle
mirror
misery
miss
mistake
mix
mixed
mixture
mobile
model
modify
mom
moment
monitor
monkey
monster
month
moon
moral
more
morning
mosquito
mother
motion
motor
mountain
mouse
move
movie
much
muffin
mule
multiply
muscle
museum
mushroom
music
must
mutual
myself
mystery
myth
naive
name
napkin
narrow
nasty
nation
nature
near
neck
need
negative
neglect
neither
nephew
nerve
nest
net
network
neutral
never
news
next
nice
night
noble
noise
nominee
noodle
normal
north
nose
notable
note
nothing
notice
novel
now
nuclear
number
nurse
nut
oak
obey
object
oblige
obscure
observe
obtain
obvious
occur
ocean
october
odor
off
offer
office
often
oil
okay
old
olive
olympic
omit
once
one
onion
online
only
open
opera
opinion
oppose
option
orange
orbit
orchard
order
ordinary
organ
orient
original
orphan
ostrich
other
outdoor
outer
output
outside
oval
oven
over
own
owner
oxygen
oyster
ozone
pact
paddle
page
pair
palace
palm
panda
panel
panic
panther
paper
parade
parent
park
parrot
party
pass
patch
path
patient
patrol
pattern
pause
pave
payment
peace
peanut
pear
peasant
pelican
pen
penalty
pencil
people
pepper
perfect
permit
person
pet
phone
photo
phrase
physical
piano
picnic
picture
piece
pig
pigeon
pill
pilot
pink
pioneer
pipe
pistol
pitch
pizza
place
planet
plastic
plate
play
please
pledge
pluck
plug
plunge
poem
poet
point
polar
pole
police
pond
pony
pool
popular
portion
position
possible
post
potato
pottery
poverty
powder
power
practice
praise
predict
prefer
prepare
present
pretty
prevent
price
pride
primary
print
priority
prison
private
prize
problem
process
produce
profit
program
project
promote
proof
property
prosper
protect
proud
provide
public
pudding
pull
pulp
pulse
pumpkin
punch
pupil
puppy
purchase
purity
purpose
purse
push
put
puzzle
pyramid
quality
quantum
quarter
question
quick
quit
quiz
quote
rabbit
raccoon
race
rack
radar
radio
rail
rain
raise
rally
ramp
ranch
random
range
rapid
rare
rate
rather
raven
raw
razor
ready
real
reason
rebel
rebuild
recall
receive
recipe
record
recycle
reduce
reflect
reform
refuse
region
regret
regular
reject
relax
release
relief
rely
remain
remember
remind
remove
render
renew
rent
reopen
repair
repeat
replace
report
require
rescue
resemble
resist
resource
response
result
retire
retreat
return
reunion
reveal
review
reward
rhythm
rib
ribbon
rice
rich
ride
ridge
rifle
right
rigid
ring
riot
ripple
risk
ritual
rival
river
road
roast
robot
robust
rocket
romance
roof
rookie
room
rose
rotate
rough
round
route
royal
rubber
rude
rug
rule
run
runway
rural
sad
saddle
sadness
safe
sail
salad
salmon
salon
salt
salute
same
sample
sand
satisfy
satoshi
sauce
sausage
save
say
scale
scan
scare
scatter
scene
scheme
school
science
scissors
scorpion
scout
scrap
screen
script
scrub
sea
search
season
seat
second
secret
section
security
seed
seek
segment
select
sell
seminar
senior
sense
sentence
series
service
session
settle
setup
seven
shadow
shaft
shallow
share
shed
shell
sheriff
shield
shift
shine
ship
shiver
shock
shoe
shoot
shop
short
shoulder
shove
shrimp
shrug
shuffle
shy
sibling
sick
side
siege
sight
sign
silent
silk
silly
silver
similar
simple
since
sing
siren
sister
situate
six
size
skate
sketch
ski
skill
skin
skirt
skull
slab
slam
sleep
slender
slice
slide
slight
slim
slogan
slot
slow
slush
small
smart
smile
smoke
smooth
snack
snake
snap
sniff
snow
soap
soccer
social
sock
soda
soft
solar
soldier
solid
solution
solve
someone
song
soon
sorry
sort
soul
sound
soup
source
south
space
spare
spatial
spawn
speak
special
speed
spell
spend
sphere
spice
spider
spike
spin
spirit
split
spoil
sponsor
spoon
sport
spot
spray
spread
spring
spy
square
squeeze
squirrel
stable
stadium
staff
stage
stairs
stamp
stand
start
state
stay
steak
steel
stem
step
stereo
stick
still
sting
stock
stomach
stone
stool
story
stove
strategy
street
strike
strong
struggle
student
stuff
stumble
style
subject
submit
subway
success
such
sudden
suffer
sugar
suggest
suit
summer
sun
sunny
sunset
super
supply
supreme
sure
surface
surge
surprise
surround
survey
suspect
sustain
swallow
swamp
swap
swarm
swear
sweet
swift
swim
swing
switch
sword
symbol
symptom
syrup
system
table
tackle
tag
tail
talent
talk
tank
tape
target
task
taste
tattoo
taxi
teach
team
tell
ten
tenant
tennis
tent
term
test
text
thank
that
theme
then
theory
there
they
thing
this
thought
three
thrive
throw
thumb
thunder
ticket
tide
tiger
tilt
timber
time
tiny
tip
tired
tissue
title
toast
tobacco
today
toddler
toe
together
toilet
token
tomato
tomorrow
tone
tongue
tonight
tool
tooth
top
topic
topple
torch
tornado
tortoise
toss
total
tourist
toward
tower
town
toy
track
trade
traffic
tragic
train
transfer
trap
trash
travel
tray
treat
tree
trend
trial
tribe
trick
trigger
trim
trip
trophy
trouble
truck
true
truly
trumpet
trust
truth
try
tube
tuition
tumble
tuna
tunnel
turkey
turn
turtle
twelve
twenty
twice
twin
twist
two
type
typical
ugly
umbrella
unable
unaware
uncle
uncover
under
undo
unfair
unfold
unhappy
uniform
unique
unit
universe
unknown
unlock
until
unusual
unveil
update
upgrade
uphold
upon
upper
upset
urban
urge
usage
use
used
useful
useless
usual
utility
vacant
vacuum
vague
valid
valley
valve
van
vanish
vapor
various
vast
vault
vehicle
velvet
vendor
venture
venue
verb
verify
version
very
vessel
veteran
viable
vibrant
vicious
victory
video
view
village
vintage
violin
virtual
virus
visa
visit
visual
vital
vivid
vocal
voice
void
volcano
volume
vote
voyage
wage
wagon
wait
walk
wall
walnut
want
warfare
warm
warrior
wash
wasp
waste
water
wave
way
wealth
weapon
wear
weasel
weather
web
wedding
weekend
weird
welcome
west
wet
whale
what
wheat
wheel
when
where
whip
whisper
wide
width
wife
wild
will
win
window
wine
wing
wink
winner
winter
wire
wisdom
wise
wish
witness
wolf
woman
wonder
wood
wool
word
work
world
worry
worth
wrap
wreck
wrestle
wrist
write
wrong
yard
year
yellow
you
young
youth
zebra
zero
zone
zoo`.split("\n");

/**
 * Provider Capabilities
 *
 * These can be imported directly with:
 * ```js
 * import * as Provider from '@web3-storage/capabilities/provider'
 * ```
 *
 * @module
 */

// e.g. did:web:web3.storage or did:web:staging.web3.storage
const Provider$1 = match$1({ method: "web" });

/**
 * Capability can be invoked by an agent to add a provider to a space.
 */
const add$8 = capability({
  can: "provider/add",
  with: AccountDID$1,
  nb: struct({
    provider: Provider$1,
    consumer: SpaceDID$1,
  }),
  derives: (child, parent) => {
    return (
      and$2(equalWith(child, parent)) ||
      and$2(equal(child.nb.provider, parent.nb.provider, "provider")) ||
      and$2(equal(child.nb.consumer, parent.nb.consumer, "consumer")) ||
      ok({})
    );
  },
});

var Provider$2 = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  AccountDID: AccountDID$1,
  Provider: Provider$1,
  add: add$8,
});

const { Provider: ProviderDID$3, AccountDID } = Provider$2;
/**
 * Provisions specified `space` with the specified `account`. It is expected
 * that delegation from the account authorizing agent is either stored in the
 * agent proofs or provided explicitly.
 *
 * @template {Record<string, any>} [S=API.Service]
 * @param {API.Agent<S>} agent
 * @param {object} input
 * @param {API.AccountDID} input.account - Account provisioning the space.
 * @param {API.SpaceDID} input.consumer - Space been provisioned.
 * @param {API.ProviderDID} [input.provider] - Provider been provisioned.
 * @param {API.Delegation[]} [input.proofs] - Delegation from the account
 * authorizing agent to call `provider/add` capability.
 */
const add$7 = async (
  agent,
  {
    account,
    consumer,
    provider = /** @type {API.ProviderDID} */ (agent.connection.id.did()),
    proofs,
  }
) => {
  if (!ProviderDID$3.is(provider)) {
    throw new Error(
      `Unable to determine provider from agent.connection.id did ${provider}. expected a did:web:`
    );
  }
  const { out } = await agent.invokeAndExecute(add$8, {
    with: account,
    nb: {
      provider,
      consumer,
    },
    proofs,
  });
  return out;
};

/**
 * Data model for the (owned) space.
 *
 * @typedef {object} Model
 * @property {ED25519.EdSigner} signer
 * @property {string} name
 * @property {API.Agent} [agent]
 */
/**
 * Generates a new space.
 *
 * @param {object} options
 * @param {string} options.name
 * @param {API.Agent} [options.agent]
 */
const generate$1 = async ({ name, agent }) => {
  const { signer } = await generate$2();
  return new OwnedSpace({ signer, name, agent });
};
/**
 * Recovers space from the saved mnemonic.
 *
 * @param {string} mnemonic
 * @param {object} options
 * @param {string} options.name - Name to give to the recovered space.
 * @param {API.Agent} [options.agent]
 */
const fromMnemonic = async (mnemonic, { name, agent }) => {
  const secret = mnemonicToEntropy(mnemonic, wordlist);
  const signer = await derive(secret);
  return new OwnedSpace({ signer, name, agent });
};
/**
 * Turns (owned) space into a BIP39 mnemonic that later can be used to recover
 * the space using `fromMnemonic` function.
 *
 * @param {object} space
 * @param {ED25519.EdSigner} space.signer
 */
const toMnemonic = ({ signer }) => {
  /** @type {Uint8Array} */
  // @ts-expect-error - Field is defined but not in the interface
  const secret = signer.secret;
  return entropyToMnemonic(secret, wordlist);
};
/**
 * Creates a (UCAN) delegation that gives full access to the space to the
 * specified `account`. At the moment we only allow `did:mailto` principal
 * to be used as an `account`.
 *
 * @param {Model} space
 * @param {API.AccountDID} account
 */
const createRecovery = (space, account) =>
  createAuthorization(space, {
    audience: parse$1(account),
    access: accountAccess$1,
    expiration: Infinity,
  });
// Default authorization session is valid for 1 year
const SESSION_LIFETIME = 60 * 60 * 24 * 365;
/**
 * Creates (UCAN) delegation that gives specified `agent` an access to
 * specified ability (passed as `access.can` field) on this space.
 * Optionally, you can specify `access.expiration` field to set the
 * expiration time for the authorization. By default the authorization
 * is valid for 1 year and gives access to all capabilities on the space
 * that are needed to use the space.
 *
 * @param {Model} space
 * @param {object} options
 * @param {API.Principal} options.audience
 * @param {API.Access} [options.access]
 * @param {API.UTCUnixTimestamp} [options.expiration]
 */
const createAuthorization = async (
  { signer, name },
  {
    audience,
    access: access$1 = spaceAccess,
    expiration = now() + SESSION_LIFETIME,
  }
) => {
  return await delegate$3({
    issuer: signer,
    audience: audience,
    capabilities: toCapabilities({
      [signer.did()]: access$1,
    }),
    ...(expiration ? { expiration } : {}),
    facts: [{ space: { name } }],
  });
};
/**
 * @param {Record<API.Resource, API.Access>} allow
 * @returns {API.Capabilities}
 */
const toCapabilities = (allow) => {
  const capabilities = [];
  for (const [subject, access] of Object.entries(allow)) {
    const entries = /** @type {[API.Ability, API.Unit][]} */ (
      Object.entries(access)
    );
    for (const [can, details] of entries) {
      if (details) {
        capabilities.push({ can, with: subject });
      }
    }
  }
  return /** @type {API.Capabilities} */ (capabilities);
};
/**
 * Represents an owned space, meaning a space for which we have a private key
 * and consequently have full authority over.
 */
class OwnedSpace {
  /**
   * @param {Model} model
   */
  constructor(model) {
    this.model = model;
  }
  get signer() {
    return this.model.signer;
  }
  get name() {
    return this.model.name;
  }
  did() {
    return this.signer.did();
  }
  /**
   * Creates a renamed version of this space.
   *
   * @param {string} name
   */
  withName(name) {
    return new OwnedSpace({ signer: this.signer, name });
  }
  /**
   * Saves account in the agent store so it can be accessed across sessions.
   *
   * @param {object} input
   * @param {API.Agent} [input.agent]
   * @returns {Promise<API.Result<API.Unit, Error>>}
   */
  async save({ agent = this.model.agent } = {}) {
    if (!agent) {
      return fail("Please provide an agent to save the space into");
    }
    const proof = await createAuthorization(this, { audience: agent });
    await agent.importSpaceFromDelegation(proof);
    await agent.setCurrentSpace(this.did());
    return { ok: {} };
  }
  /**
   * @param {Authorization} authorization
   * @param {object} options
   * @param {API.Agent} [options.agent]
   */
  provision({ proofs }, { agent = this.model.agent } = {}) {
    if (!agent) {
      return fail("Please provide an agent to save the space into");
    }
    return provision(this, { proofs, agent });
  }
  /**
   * Creates a (UCAN) delegation that gives full access to the space to the
   * specified `account`. At the moment we only allow `did:mailto` principal
   * to be used as an `account`.
   *
   * @param {API.AccountDID} account
   */
  async createRecovery(account) {
    return createRecovery(this, account);
  }
  /**
   * Creates (UCAN) delegation that gives specified `agent` an access to
   * specified ability (passed as `access.can` field) on the this space.
   * Optionally, you can specify `access.expiration` field to set the
   *
   * @param {API.Principal} principal
   * @param {object} [input]
   * @param {API.Access} [input.access]
   * @param {API.UCAN.UTCUnixTimestamp} [input.expiration]
   */
  createAuthorization(principal, input) {
    return createAuthorization(this, { ...input, audience: principal });
  }
  /**
   * Derives BIP39 mnemonic that can be used to recover the space.
   *
   * @returns {string}
   */
  toMnemonic() {
    return toMnemonic(this);
  }
}
const SpaceDID = match$1({ method: "key" });
/**
 * Creates a (shared) space from given delegation.
 *
 * @param {API.Delegation} delegation
 */
const fromDelegation = (delegation) => {
  const result = SpaceDID.read(delegation.capabilities[0].with);
  if (result.error) {
    throw Object.assign(
      new Error(
        `Invalid delegation, expected capabilities[0].with to be DID, ${result.error}`
      ),
      {
        cause: result.error,
      }
    );
  }
  /** @type {{name?:string}} */
  const meta = delegation.facts[0]?.space ?? {};
  return new SharedSpace({ id: result.ok, delegation, meta });
};
/**
 * @typedef {object} Authorization
 * @property {API.Delegation[]} proofs
 *
 * @typedef {object} Space
 * @property {() => API.SpaceDID} did
 */
/**
 * @param {Space} space
 * @param {object} options
 * @param {API.Delegation[]} options.proofs
 * @param {API.Agent} options.agent
 */
const provision = async (space, { proofs, agent }) => {
  const [capability] = proofs[0].capabilities;
  const { ok: account, error: reason } = AccountDID.read(capability.with);
  if (reason) {
    return error$1(reason);
  }
  return await add$7(agent, {
    consumer: space.did(),
    account,
    proofs,
  });
};
/**
 * Represents a shared space, meaning a space for which we have a delegation
 * and consequently have limited authority over.
 */
class SharedSpace {
  /**
   * @typedef {object} SharedSpaceModel
   * @property {API.SpaceDID} id
   * @property {API.Delegation} delegation
   * @property {{name?:string}} meta
   * @property {API.Agent} [agent]
   *
   * @param {SharedSpaceModel} model
   */
  constructor(model) {
    this.model = model;
  }
  get delegation() {
    return this.model.delegation;
  }
  get meta() {
    return this.model.meta;
  }
  get name() {
    return this.meta.name ?? "";
  }
  did() {
    return this.model.id;
  }
  /**
   * @param {string} name
   */
  withName(name) {
    return new SharedSpace({
      ...this.model,
      meta: { ...this.meta, name },
    });
  }
}

/**
 *
 * @param {API.Delegation} delegation
 */
function isExpired(delegation) {
  if (
    delegation.expiration === undefined ||
    delegation.expiration <= Math.floor(Date.now() / 1000)
  ) {
    return true;
  }
  return false;
}
/**
 *
 * @param {API.Delegation} delegation
 */
function isTooEarly(delegation) {
  if (!delegation.notBefore) {
    return false;
  }
  return delegation.notBefore > Math.floor(Date.now() / 1000);
}
/**
 * Returns true if the delegation includes capability been queried.
 *
 * @param {API.Delegation} delegation
 * @param {API.CapabilityQuery} capability
 */
function canDelegateCapability(delegation, capability) {
  const allowsCapabilities = allows(delegation);
  for (const [uri, abilities] of Object.entries(allowsCapabilities)) {
    if (matchResource(/** @type {API.Resource} */ (uri), capability.with)) {
      const cans = /** @type {API.Ability[]} */ (Object.keys(abilities));
      for (const can of cans) {
        if (canDelegateAbility(can, capability.can)) {
          return true;
        }
      }
    }
  }
  return false;
}
/**
 * Returns true if given `resource` matches the resource query per UCAN
 * specification.
 *
 * @param {API.Resource} resource
 * @param {API.ResourceQuery} query
 */
const matchResource = (resource, query) => {
  if (query === "ucan:*") {
    return true;
  } else if (typeof query === "string") {
    return resource === query;
  } else {
    return query.test(resource);
  }
};

const _globalReference = globalThis || window || self;

const webcrypto = _globalReference.crypto;

/**
 *
 * @param {number} code
 * @param {Uint8Array} bytes
 */
const tagWith = (code, bytes) => {
  const offset = encodingLength$3(code);
  const multiformat = new Uint8Array(bytes.byteLength + offset);
  encodeTo$3(code, multiformat, 0);
  multiformat.set(bytes, offset);

  return multiformat;
};

/**
 * @param {number} code
 * @param {Uint8Array} source
 * @param {number} byteOffset
 * @returns
 */
const untagWith = (code, source, byteOffset = 0) => {
  const bytes = byteOffset !== 0 ? source.subarray(byteOffset) : source;
  const [tag, size] = decode$A(bytes);
  if (tag !== code) {
    throw new Error(
      `Expected multiformat with 0x${code.toString(
        16
      )} tag instead got 0x${tag.toString(16)}`
    );
  } else {
    return new Uint8Array(bytes.buffer, bytes.byteOffset + size);
  }
};

/**
 * ASN1 Tags as per https://luca.ntop.org/Teaching/Appunti/asn1.html
 */
const TAG_SIZE$1 = 1;
const INT_TAG = 0x02;
const BITSTRING_TAG = 0x03;
const OCTET_STRING_TAG = 0x04;
const SEQUENCE_TAG = 0x30;

const UNUSED_BIT_PAD = 0x00;

/**
 * @param {number} length
 * @returns {Uint8Array}
 */
const encodeDERLength = (length) => {
  if (length <= 127) {
    return new Uint8Array([length]);
  }

  /** @type {number[]} */
  const octets = [];
  while (length !== 0) {
    octets.push(length & 0xff);
    length = length >>> 8;
  }
  octets.reverse();
  return new Uint8Array([0x80 | (octets.length & 0xff), ...octets]);
};

/**
 * @param {Uint8Array} bytes
 * @param {number} offset
 * @returns {{number: number, consumed: number}}
 */
const readDERLength = (bytes, offset = 0) => {
  if ((bytes[offset] & 0x80) === 0) {
    return { number: bytes[offset], consumed: 1 };
  }

  const numberBytes = bytes[offset] & 0x7f;
  /* c8 ignore next 5 */
  if (bytes.length < numberBytes + 1) {
    throw new Error(
      `ASN parsing error: Too few bytes. Expected encoded length's length to be at least ${numberBytes}`
    );
  }

  let length = 0;
  for (let i = 0; i < numberBytes; i++) {
    length = length << 8;
    length = length | bytes[offset + i + 1];
  }

  return { number: length, consumed: numberBytes + 1 };
};

/**
 * @param {Uint8Array} input
 * @param {number} expectedTag
 * @param {number} position
 * @returns {number}
 */
const skip = (input, expectedTag, position) => {
  const parsed = into(input, expectedTag, position);
  return parsed.position + parsed.length;
};

/**
 * @param {Uint8Array} input
 * @param {number} expectedTag
 * @param {number} offset
 * @returns {{ position: number, length: number }}
 */
const into = (input, expectedTag, offset) => {
  const actualTag = input[offset];
  /* c8 ignore next 7 */
  if (actualTag !== expectedTag) {
    throw new Error(
      `ASN parsing error: Expected tag 0x${expectedTag.toString(
        16
      )} at position ${offset}, but got 0x${actualTag.toString(16)}.`
    );
  }

  // length
  const length = readDERLength(input, offset + TAG_SIZE$1);
  const position = offset + TAG_SIZE$1 + length.consumed;

  // content
  return { position, length: length.number };
};

/**
 * @param {Uint8Array} input
 */
const encodeBitString = (input) => {
  // encode input length + 1 for unused bit pad
  const length = encodeDERLength(input.byteLength + 1);
  // allocate a buffer of desired size
  const bytes = new Uint8Array(
    TAG_SIZE$1 + // ASN_BITSTRING_TAG
      length.byteLength +
      1 + // amount of unused bits at the end of our bitstring
      input.byteLength
  );

  let byteOffset = 0;
  // write bytestring tag
  bytes[byteOffset] = BITSTRING_TAG;
  byteOffset += TAG_SIZE$1;

  // write length of the bytestring
  bytes.set(length, byteOffset);
  byteOffset += length.byteLength;

  // write unused bits at the end of our bitstring
  bytes[byteOffset] = UNUSED_BIT_PAD;
  byteOffset += 1;

  // write actual data into bitstring
  bytes.set(input, byteOffset);

  return bytes;
};

/**
 * @param {Uint8Array} input
 */
const encodeOctetString = (input) => {
  // encode input length
  const length = encodeDERLength(input.byteLength);
  // allocate a buffer of desired size
  const bytes = new Uint8Array(
    TAG_SIZE$1 + length.byteLength + input.byteLength
  );

  let byteOffset = 0;
  // write octet string tag
  bytes[byteOffset] = OCTET_STRING_TAG;
  byteOffset += TAG_SIZE$1;

  // write octet string length
  bytes.set(length, byteOffset);
  byteOffset += length.byteLength;

  // write actual data into bitstring
  bytes.set(input, byteOffset);

  return bytes;
};

/**
 * @param {Uint8Array[]} sequence
 */
const encodeSequence = (sequence) => {
  // calculate bytelength for all the parts
  let byteLength = 0;
  for (const item of sequence) {
    byteLength += item.byteLength;
  }

  // encode sequence byte length
  const length = encodeDERLength(byteLength);

  // allocate the buffer to write sequence into
  const bytes = new Uint8Array(TAG_SIZE$1 + length.byteLength + byteLength);

  let byteOffset = 0;

  // write the sequence tag
  bytes[byteOffset] = SEQUENCE_TAG;
  byteOffset += TAG_SIZE$1;

  // write sequence length
  bytes.set(length, byteOffset);
  byteOffset += length.byteLength;

  // write each item in the sequence
  for (const item of sequence) {
    bytes.set(item, byteOffset);
    byteOffset += item.byteLength;
  }

  return bytes;
};

/**
 * @param {Uint8Array} bytes
 * @param {number} offset
 */
const readSequence = (bytes, offset = 0) => {
  const { position, length } = into(bytes, SEQUENCE_TAG, offset);

  return new Uint8Array(bytes.buffer, bytes.byteOffset + position, length);
};

/**
 * @param {Uint8Array} input
 */
const encodeInt = (input) => {
  const extra = input.byteLength === 0 || input[0] & 0x80 ? 1 : 0;

  // encode input length
  const length = encodeDERLength(input.byteLength + extra);
  // allocate a buffer of desired size
  const bytes = new Uint8Array(
    TAG_SIZE$1 + // INT_TAG
      length.byteLength +
      input.byteLength +
      extra
  );

  let byteOffset = 0;
  // write octet string tag
  bytes[byteOffset] = INT_TAG;
  byteOffset += TAG_SIZE$1;

  // write int length
  bytes.set(length, byteOffset);
  byteOffset += length.byteLength;

  // add 0 if the most-significant bit is set
  if (extra > 0) {
    bytes[byteOffset] = UNUSED_BIT_PAD;
    byteOffset += extra;
  }

  // write actual data into bitstring
  bytes.set(input, byteOffset);

  return bytes;
};

/**
 * @param {Uint8Array} bytes
 * @param {number} offset
 * @returns {number}
 */

const enterSequence = (bytes, offset = 0) =>
  into(bytes, SEQUENCE_TAG, offset).position;

/**
 * @param {Uint8Array} bytes
 * @param {number} offset
 * @returns {number}
 */
const skipSequence = (bytes, offset = 0) => skip(bytes, SEQUENCE_TAG, offset);

/**
 * @param {Uint8Array} bytes
 * @param {number} offset
 * @returns {number}
 */
const skipInt = (bytes, offset = 0) => skip(bytes, INT_TAG, offset);

/**
 * @param {Uint8Array} bytes
 * @param {number} offset
 * @returns {Uint8Array}
 */
const readBitString = (bytes, offset = 0) => {
  const { position, length } = into(bytes, BITSTRING_TAG, offset);
  const tag = bytes[position];
  /* c8 ignore next 5 */
  if (tag !== UNUSED_BIT_PAD) {
    throw new Error(
      `Can not read bitstring, expected length to be multiple of 8, but got ${tag} unused bits in last byte.`
    );
  }

  return new Uint8Array(
    bytes.buffer,
    bytes.byteOffset + position + 1,
    length - 1
  );
};

/**
 * @param {Uint8Array} bytes
 * @param {number} byteOffset
 * @returns {Uint8Array}
 */
const readInt$1 = (bytes, byteOffset = 0) => {
  const { position, length } = into(bytes, INT_TAG, byteOffset);
  let delta = 0;

  // drop leading 0s
  while (bytes[position + delta] === 0) {
    delta++;
  }

  return new Uint8Array(
    bytes.buffer,
    bytes.byteOffset + position + delta,
    length - delta
  );
};

/**
 * @param {Uint8Array} bytes
 * @param {number} offset
 * @returns {Uint8Array}
 */
const readOctetString = (bytes, offset = 0) => {
  const { position, length } = into(bytes, OCTET_STRING_TAG, offset);

  return new Uint8Array(bytes.buffer, bytes.byteOffset + position, length);
};

/**
 * @typedef {(bytes:Uint8Array, offset:number) => Uint8Array} Reader
 * @param {[Reader, ...Reader[]]} readers
 * @param {Uint8Array} source
 * @param {number} byteOffset
 */
const readSequenceWith = (readers, source, byteOffset = 0) => {
  const results = [];
  const sequence = readSequence(source, byteOffset);
  let offset = 0;
  for (const read of readers) {
    const chunk = read(sequence, offset);
    results.push(chunk);
    offset = chunk.byteOffset + chunk.byteLength - sequence.byteOffset;
  }
  return results;
};

/**
 * @typedef {import('./public-key.js').RSAPublicKey} RSAPublicKey
 */
/**
 * Described in RFC 5208 Section 4.1: https://tools.ietf.org/html/rfc5280#section-4.1
 * ```
 * SubjectPublicKeyInfo  ::=  SEQUENCE  {
 *    algorithm            AlgorithmIdentifier,
 *    subjectPublicKey     BIT STRING  }
 * ```
 *
 * @typedef {object} SubjectPublicKeyInfo
 * @property {API.ByteView<AlgorithmIdentifier>} algorithm
 * @property {API.ByteView<RSAPublicKey>} subjectPublicKey
 * @typedef {import('./pkcs8.js').AlgorithmIdentifier} AlgorithmIdentifier
 */

/**
 * The ASN.1 DER encoded header that needs to be added to an
 * ASN.1 DER encoded RSAPublicKey to make it a SubjectPublicKeyInfo.
 *
 * This byte sequence is always the same.
 *
 * A human-readable version of this as part of a dumpasn1 dump:
 *
 *     SEQUENCE {
 *       OBJECT IDENTIFIER rsaEncryption (1 2 840 113549 1 1 1)
 *       NULL
 *     }
 *
 * See https://github.com/ucan-wg/ts-ucan/issues/30
 */
const SPKI_PARAMS_ENCODED = new Uint8Array([
  48, 13, 6, 9, 42, 134, 72, 134, 247, 13, 1, 1, 1, 5, 0,
]);

/**
 * @param {API.ByteView<RSAPublicKey>} key
 * @returns {API.ByteView<SubjectPublicKeyInfo>}
 */
const encode$d = (key) =>
  encodeSequence([SPKI_PARAMS_ENCODED, encodeBitString(key)]);

/**
 *
 * @param {API.ByteView<SubjectPublicKeyInfo>} info
 * @returns {API.ByteView<RSAPublicKey>}
 */
const decode$h = (info) => {
  // go into the top-level SEQUENCE
  const offset = enterSequence(info, 0);
  // skip the header we expect (SKPI_PARAMS_ENCODED)
  const keyOffset = skipSequence(info, offset);

  // we expect the bitstring next
  return readBitString(info, keyOffset);
};

const PKSC8_HEADER = new Uint8Array([
  // version
  2, 1, 0,
  // privateKeyAlgorithm
  48, 13, 6, 9, 42, 134, 72, 134, 247, 13, 1, 1, 1, 5, 0,
]);
/**
 * @typedef {import('./private-key').RSAPrivateKey} RSAPrivateKey
 * @typedef {object} AlgorithmIdentifier
 * @property {Uint8Array} version
 * @property {Uint8Array} parameters
 *
 * @see https://datatracker.ietf.org/doc/html/rfc5208#section-5
 * @typedef {object} PrivateKeyInfo
 * @property {API.ByteView<number>} version
 * @property {API.ByteView<AlgorithmIdentifier>} privateKeyAlgorithm
 * @property {API.ByteView<RSAPrivateKey>} privateKey
 * @property {API.ByteView<unknown>} [attributes]
 */

/**
 * @param {API.ByteView<PrivateKeyInfo>} info
 * @returns {API.ByteView<RSAPrivateKey>}
 */
const decode$g = (info) => {
  let offset = 0;
  // go into the top-level SEQUENCE
  offset = enterSequence(info, offset);
  offset = skipInt(info, offset);
  offset = skipSequence(info, offset);

  // we expect the bitstring next
  return readOctetString(info, offset);
};

/**
 * @param {API.ByteView<RSAPrivateKey>} key
 * @returns {API.ByteView<PrivateKeyInfo>}
 */
const encode$c = (key) =>
  encodeSequence([PKSC8_HEADER, encodeOctetString(key)]);

/**
 * @param {RSAPublicKey} key
 * @returns {API.ByteView<RSAPublicKey>}
 */
const encode$b = ({ n, e }) => encodeSequence([encodeInt(n), encodeInt(e)]);

/**
 * @see https://datatracker.ietf.org/doc/html/rfc3447#appendix-A.1.2
 * @typedef {object} RSAPrivateKey
 * @property {Uint8Array} v
 * @property {Uint8Array} n
 * @property {Uint8Array} e
 * @property {Uint8Array} d
 * @property {Uint8Array} p
 * @property {Uint8Array} q
 * @property {Uint8Array} dp
 * @property {Uint8Array} dq
 * @property {Uint8Array} qi
 */

/**
 * Takes private-key information in [Private-Key Information Syntax](https://datatracker.ietf.org/doc/html/rfc5208#section-5)
 * and extracts all the fields as per [RSA private key syntax](https://datatracker.ietf.org/doc/html/rfc3447#appendix-A.1.2)
 *
 *
 * @param {API.ByteView<RSAPrivateKey>} source
 * @param {number} byteOffset
 * @returns {RSAPrivateKey}
 */
const decode$f = (source, byteOffset = 0) => {
  const [v, n, e, d, p, q, dp, dq, qi] = readSequenceWith(
    [
      readInt$1,
      readInt$1,
      readInt$1,
      readInt$1,
      readInt$1,
      readInt$1,
      readInt$1,
      readInt$1,
      readInt$1,
    ],
    source,
    byteOffset
  );

  return { v, n, e, d, p, q, dp, dq, qi };
};

const name$6 = "RSA";

/** @type {API.RSASigner['code']} */
const code$8 = 0x1305;

/** @type {API.RSAVerifier['code']} */
const verifierCode = 0x1205;

/** @type {API.SigAlg} */
const signatureCode = RS256;
const signatureAlgorithm = "RS256";

const ALG = "RSASSA-PKCS1-v1_5";
const HASH_ALG = "SHA-256";
const KEY_SIZE = 2048;
const SALT_LENGTH = 128;
const IMPORT_PARAMS = {
  name: ALG,
  hash: { name: HASH_ALG },
};

/**
 * @param {object} options
 * @param {number} [options.size]
 * @param {boolean} [options.extractable]
 * @returns {Promise<API.RSASigner>}
 */
const generate = async ({ size = KEY_SIZE, extractable = false } = {}) => {
  // We start by generate an RSA keypair using web crypto API.
  const { publicKey, privateKey } = await webcrypto.subtle.generateKey(
    {
      name: ALG,
      modulusLength: size,
      publicExponent: new Uint8Array([0x01, 0x00, 0x01]),
      hash: { name: HASH_ALG },
    },

    extractable,
    ["sign", "verify"]
  );

  // Next we need to encode public key, because `RSAVerifier` uses it to
  // for implementing a `did()` method. To do this we first export
  // Subject Public Key Info (SPKI) using web crypto API.
  const spki = await webcrypto.subtle.exportKey("spki", publicKey);
  // Then we extract public key from the SPKI and tag it with RSA public key
  // multicode
  const publicBytes = tagWith(verifierCode, decode$h(new Uint8Array(spki)));
  // Now that we have publicKey and it's multiformat representation we can
  // create a verifier.
  const verifier = new RSAVerifier({ bytes: publicBytes, publicKey });

  // If we generated non extractable key we just wrap actual keys and verifier
  // in the RSASigner view.
  if (!extractable) {
    return new UnextractableRSASigner({
      privateKey,
      verifier,
    });
  }
  // Otherwise we export key in Private Key Cryptography Standards (PKCS)
  // format and extract a bytes corresponding to the private key, which
  // we tag with RSA private key multiformat code. With both binary and actual
  // key representation we create a RSASigner view.
  // Please note that do key export flow during generation so that we can:
  // 1. Guarantee that it will be exportable.
  // 2. Make `export` method sync.
  else {
    const pkcs8 = await webcrypto.subtle.exportKey("pkcs8", privateKey);
    const bytes = tagWith(code$8, decode$g(new Uint8Array(pkcs8)));
    return new ExtractableRSASigner({
      privateKey,
      bytes,
      verifier,
    });
  }
};

/**
 * @param {API.SignerArchive<API.DID, typeof signatureCode>} archive
 * @returns {API.RSASigner}
 */
const from$a = ({ id, keys }) => {
  if (id.startsWith("did:key:")) {
    const did = /** @type {API.DIDKey} */ (id);
    const key = keys[did];
    if (key instanceof Uint8Array) {
      return decode$e(key);
    } else {
      return new UnextractableRSASigner({
        privateKey: key,
        verifier: RSAVerifier.parse(did),
      });
    }
  } else {
    throw new TypeError(
      `RSA can not import from ${id} archive, try generic Signer instead`
    );
  }
};

/**
 * @template {API.SignerImporter} Other
 * @param {Other} other
 */
const or$4 = (other) => or$6({ from: from$a }, other);

/**
 * @param {EncodedSigner} bytes
 * @returns {API.RSASigner}
 */
const decode$e = (bytes) => {
  // First we decode RSA key data from the private key with multicode tag.
  const rsa = decode$f(untagWith(code$8, bytes));
  // Then we encode RSA key data as public key with multicode tag.
  const publicBytes = tagWith(verifierCode, encode$b(rsa));

  return new ExtractableRSASigner({
    bytes,
    privateKey: webcrypto.subtle.importKey(
      "pkcs8",
      encode$c(untagWith(code$8, bytes)),
      IMPORT_PARAMS,
      true,
      ["sign"]
    ),

    verifier: RSAVerifier.decode(publicBytes),
  });
};

/**
 * @implements {API.RSAVerifier}
 */
class RSAVerifier {
  /**
   * @param {object} options
   * @param {API.Await<CryptoKey>} options.publicKey
   * @param {API.ByteView<API.RSAVerifier>} options.bytes
   */
  constructor({ publicKey, bytes }) {
    /** @private */
    this.publicKey = publicKey;
    /** @private */
    this.bytes = bytes;
  }

  /**
   * @template {API.DID} ID
   * @param {ID} id
   * @returns {API.Verifier<ID, typeof signatureCode>}
   */
  withDID(id) {
    return withDID$1(this, id);
  }

  toDIDKey() {
    return this.did();
  }

  /**
   * @param {API.ByteView<API.RSAVerifier>} bytes
   * @returns {API.RSAVerifier}
   */
  static decode(bytes) {
    return new this({
      bytes,
      publicKey: webcrypto.subtle.importKey(
        "spki",
        encode$d(untagWith(verifierCode, bytes)),
        IMPORT_PARAMS,
        true,
        ["verify"]
      ),
    });
  }
  /**
   * @param {API.DIDKey} did
   * @returns {API.RSAVerifier}
   */
  static parse(did) {
    return RSAVerifier.decode(/** @type {Uint8Array} */ (parse$1(did)));
  }

  /**
   * @param {API.PrincipalParser} other
   */
  static or(other) {
    return or$7(this, other);
  }

  /** @type {typeof verifierCode} */
  get code() {
    return verifierCode;
  }
  /**
   * @type {typeof signatureCode}
   */
  get signatureCode() {
    return signatureCode;
  }
  /**
   * @type {typeof signatureAlgorithm}
   */
  get signatureAlgorithm() {
    return signatureAlgorithm;
  }
  /**
   * DID of the Principal in `did:key` format.
   * @returns {API.DID<"key">}
   */
  did() {
    return `did:key:${base58btc$2.encode(this.bytes)}`;
  }

  /**
   * @template T
   * @param {API.ByteView<T>} payload
   * @param {API.Signature<T, typeof this.signatureCode>} signature
   * @returns {Promise<boolean>}
   */
  async verify(payload, signature) {
    // if signature code does not match RS256 it's not signed by corresponding
    // signer.
    if (signature.code !== signatureCode) {
      return false;
    }

    return webcrypto.subtle.verify(
      { name: ALG, hash: { name: HASH_ALG } },
      await this.publicKey,
      signature.raw,
      payload
    );
  }
}

/**
 * @typedef {API.ByteView<API.Signer<API.DID<'key'>, typeof signatureCode> & CryptoKey>} EncodedSigner
 */

class RSASigner {
  /**
   * @param {object} options
   * @param {API.Await<CryptoKey>} options.privateKey
   * @param {API.RSAVerifier} options.verifier
   */
  constructor({ privateKey, verifier }) {
    /** @readonly */
    this.verifier = verifier;
    /** @protected */
    this.privateKey = privateKey;
  }
  get signer() {
    return this;
  }

  /**
   * @type {typeof code}
   */
  get code() {
    return code$8;
  }
  /**
   * @type {typeof signatureCode}
   */
  get signatureCode() {
    return signatureCode;
  }
  /**
   * @type {typeof signatureAlgorithm}
   */
  get signatureAlgorithm() {
    return signatureAlgorithm;
  }

  did() {
    return this.verifier.did();
  }

  toDIDKey() {
    return this.verifier.toDIDKey();
  }

  /**
   * @template T
   * @param {API.ByteView<T>} payload
   * @param {API.Signature<T, typeof this.signatureCode>} signature
   */
  verify(payload, signature) {
    return this.verifier.verify(payload, signature);
  }
  /**
   * @template T
   * @param {API.ByteView<T>} payload
   * @returns {Promise<API.SignatureView<T, typeof signatureCode>>}
   */
  async sign(payload) {
    const buffer = await webcrypto.subtle.sign(
      { name: ALG, saltLength: SALT_LENGTH },
      await this.privateKey,
      payload
    );

    return create$f(signatureCode, new Uint8Array(buffer));
  }
}

/**
 * @implements {API.RSASigner}
 */
class ExtractableRSASigner extends RSASigner {
  /**
   * @param {object} options
   * @param {API.Await<CryptoKey>} options.privateKey
   * @param {EncodedSigner} options.bytes
   * @param {API.RSAVerifier} options.verifier
   */
  constructor(options) {
    super(options);
    this.bytes = options.bytes;
  }

  /**
   * @template {API.DID} ID
   * @param {ID} id
   * @returns {API.Signer<ID, typeof signatureCode>}
   */
  withDID(id) {
    return withDID(this, id);
  }

  toArchive() {
    const id = this.did();
    return {
      id,
      keys: { [id]: this.bytes },
    };
  }
}

/**
 * @implements {API.RSASigner}
 */
class UnextractableRSASigner extends RSASigner {
  /**
   * @param {object} options
   * @param {CryptoKey} options.privateKey
   * @param {API.RSAVerifier} options.verifier
   */
  constructor(options) {
    super(options);
    this.privateKey = options.privateKey;
  }

  /**
   * @template {API.DID} ID
   * @param {ID} id
   * @returns {API.Signer<ID, typeof signatureCode>}
   */
  withDID(id) {
    return withDID(this, id);
  }

  toArchive() {
    const id = this.did();
    return {
      id,
      keys: { [id]: this.privateKey },
    };
  }
}

var RSA = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  Verifier: RSAVerifier,
  code: code$8,
  decode: decode$e,
  from: from$a,
  generate: generate,
  name: name$6,
  or: or$4,
  signatureAlgorithm: signatureAlgorithm,
  signatureCode: signatureCode,
});

const Signer = or$5(RSA);

// e.g. did:web:web3.storage or did:web:staging.web3.storage
const ProviderDID$2 = match$1({ method: "web" });

/**
 * Capability can be invoked by a provider to check if it has given space as
 * a consumer.
 */
const has$1 = capability({
  can: "consumer/has",
  with: ProviderDID$2,
  nb: struct({
    consumer: SpaceDID$1,
  }),
  derives: (child, parent) => {
    return (
      and$2(equalWith(child, parent)) ||
      and$2(equal(child.nb.consumer, parent.nb.consumer, "consumer")) ||
      ok({})
    );
  },
});

/**
 * Capability can be invoked by a provider to get information about a consumer.
 */
const get$f = capability({
  can: "consumer/get",
  with: ProviderDID$2,
  nb: struct({
    consumer: SpaceDID$1,
  }),
  derives: (child, parent) => {
    return (
      and$2(equalWith(child, parent)) ||
      and$2(equal(child.nb.consumer, parent.nb.consumer, "consumer")) ||
      ok({})
    );
  },
});

// e.g. did:web:web3.storage or did:web:staging.web3.storage
const ProviderDID$1 = match$1({ method: "web" });

/**
 * Capability can be invoked by a provider to get information about the
 * customer.
 */
const get$e = capability({
  can: "customer/get",
  with: ProviderDID$1,
  nb: struct({
    customer: AccountDID$1,
  }),
  derives: (child, parent) => {
    return (
      and$2(equalWith(child, parent)) ||
      and$2(equal(child.nb.customer, parent.nb.customer, "customer")) ||
      ok({})
    );
  },
});

capability({
  can: "console/*",
  with: match$1(),
  derives: equalWith,
});

/**
 * Capability that succeeds with the `nb.value` value.
 */
capability({
  can: "console/log",
  with: match$1(),
  nb: struct({
    value: unknown(),
  }),
  derives: equalWith,
});

/**
 * Capability that fails with an error provided to `nb.error` field.
 */
capability({
  can: "console/error",
  with: match$1(),
  nb: struct({
    error: unknown(),
  }),
  derives: equalWith,
});

/**
 * Rate Limit Capabilities
 *
 * These can be imported directly with:
 * ```js
 * import * as RateLimit from '@web3-storage/capabilities/rate-limit'
 * ```
 *
 * @module
 */

// e.g. did:web:web3.storage or did:web:staging.web3.storage
const Provider = DID;

/**
 * Capability can be invoked by the provider or an authorized delegate to add a rate limit to a subject.
 */
const add$6 = capability({
  can: "rate-limit/add",
  with: Provider,
  nb: struct({
    subject: string(),
    rate: number(),
  }),
  derives: (child, parent) => {
    return (
      and$2(equalWith(child, parent)) ||
      and$2(equal(child.nb.subject, parent.nb.subject, "subject")) ||
      and$2(equal(child.nb.rate, parent.nb.rate, "rate")) ||
      ok({})
    );
  },
});

/**
 * Capability can be invoked by the provider are an authorized delegate to remove rate limits from a subject.
 */
const remove$6 = capability({
  can: "rate-limit/remove",
  with: Provider,
  nb: struct({
    id: string(),
  }),
  derives: (child, parent) => {
    return (
      and$2(equalWith(child, parent)) ||
      and$2(equal(child.nb.id, parent.nb.id, "id")) ||
      ok({})
    );
  },
});

/**
 * Capability can be invoked by the provider or an authorized delegate to list rate limits on the given subject
 */
const list$7 = capability({
  can: "rate-limit/list",
  with: Provider,
  nb: struct({
    subject: string(),
  }),
  derives: (child, parent) => {
    return (
      and$2(equalWith(child, parent)) ||
      and$2(equal(child.nb.subject, parent.nb.subject, "subject")) ||
      ok({})
    );
  },
});

const admin = capability({
  can: "admin/*",
  with: ProviderDID$4,
  derives: equalWith,
});

const upload = {
  /**
   * Capability can be invoked by a provider to get information about a content CID.
   */
  inspect: capability({
    can: "admin/upload/inspect",
    with: ProviderDID$4,
    nb: struct({
      root: Link,
    }),
    derives: (child, parent) => {
      return (
        and$2(equalWith(child, parent)) ||
        and$2(equal(child.nb.root, parent.nb.root, "root")) ||
        ok({})
      );
    },
  }),
};

const store = {
  /**
   * Capability can be invoked by a provider to get information an upload shard CID.
   */
  inspect: capability({
    can: "admin/store/inspect",
    with: ProviderDID$4,
    nb: struct({
      link: Link,
    }),
    derives: (child, parent) => {
      return (
        and$2(equalWith(child, parent)) ||
        and$2(equal(child.nb.link, parent.nb.link, "link")) ||
        ok({})
      );
    },
  }),
};

// e.g. did:web:web3.storage or did:web:staging.web3.storage
const ProviderDID = match$1({ method: "web" });

/**
 * Capability can be invoked by a provider to get information about a subscription.
 */
const get$d = capability({
  can: "subscription/get",
  with: ProviderDID,
  nb: struct({
    subscription: string(),
  }),
  derives: (child, parent) => {
    return (
      and$2(equalWith(child, parent)) ||
      and$2(equal(child.nb.subscription, parent.nb.subscription, "consumer")) ||
      ok({})
    );
  },
});

/**
 * Capability can be invoked to retrieve the list of subscriptions for an
 * account.
 */
const list$6 = capability({
  can: "subscription/list",
  with: AccountDID$1,
  derives: equalWith,
});

/**
 * @see https://github.com/filecoin-project/FIPs/pull/758/files
 */
const FR32_SHA2_256_TRUNC254_PADDED_BINARY_TREE = /** @type {const} */ (0x1011);
/**
 * @see https://github.com/filecoin-project/FIPs/pull/758/files
 */
const RAW_CODE = /** @type {const} */ (0x55);

const PieceLink = /** @type {import('../types.js').PieceLinkSchema} */ (
  match$2({
    code: RAW_CODE,
    version: 1,
    multihash: {
      code: FR32_SHA2_256_TRUNC254_PADDED_BINARY_TREE,
    },
  })
);

/**
 * Filecoin Storefront Capabilities
 *
 * These can be imported directly with:
 * ```js
 * import * as Storefront from '@web3-storage/capabilities/filecoin/storefront'
 * ```
 *
 * @module
 */

/**
 * Top-level capability for Filecoin operations.
 */
capability({
  can: "filecoin/*",
  /**
   * DID of the space the content is stored in.
   */
  with: match$1(),
  derives: equalWith,
});

/**
 * Capability allowing an agent to _request_ storing a content piece in
 * Filecoin.
 */
const filecoinOffer$1 = capability({
  can: "filecoin/offer",
  /**
   * DID of the space the content is stored in.
   */
  with: match$1(),
  nb: struct({
    /**
     * CID of the content that resulted in Filecoin piece.
     */
    content: match$2(),
    /**
     * CID of the piece.
     */
    piece: PieceLink,
  }),
  derives: (claim, from) => {
    return (
      and$2(equalWith(claim, from)) ||
      and$2(checkLink(claim.nb.content, from.nb.content, "nb.content")) ||
      and$2(checkLink(claim.nb.piece, from.nb.piece, "nb.piece")) ||
      ok({})
    );
  },
});

/**
 * Capability allowing a Storefront to signal that an offered piece has been
 * submitted to the filecoin storage pipeline.
 */
const filecoinSubmit = capability({
  can: "filecoin/submit",
  /**
   * DID of the Storefront.
   */
  with: match$1(),
  nb: struct({
    /**
     * CID of the content that resulted in Filecoin piece.
     */
    content: match$2(),
    /**
     * CID of the piece.
     *
     * @see https://github.com/filecoin-project/FIPs/pull/758/files
     */
    piece: PieceLink,
  }),
  derives: (claim, from) => {
    return (
      and$2(equalWith(claim, from)) ||
      and$2(checkLink(claim.nb.content, from.nb.content, "nb.content")) ||
      and$2(checkLink(claim.nb.piece, from.nb.piece, "nb.piece")) ||
      ok({})
    );
  },
});

/**
 * Capability allowing a Storefront to signal that a submitted piece has been
 * accepted in a Filecoin deal. The receipt contains the proof.
 */
const filecoinAccept = capability({
  can: "filecoin/accept",
  /**
   * DID of the Storefront.
   */
  with: match$1(),
  nb: struct({
    /**
     * CID of the content that resulted in Filecoin piece.
     */
    content: match$2(),
    /**
     * CID of the piece.
     *
     * @see https://github.com/filecoin-project/FIPs/pull/758/files
     */
    piece: PieceLink,
  }),
  derives: (claim, from) => {
    return (
      and$2(equalWith(claim, from)) ||
      and$2(checkLink(claim.nb.content, from.nb.content, "nb.content")) ||
      and$2(checkLink(claim.nb.piece, from.nb.piece, "nb.piece")) ||
      ok({})
    );
  },
});

/**
 * Capability allowing an agent to _request_ info about a content piece in
 * Filecoin deals.
 */
const filecoinInfo$1 = capability({
  can: "filecoin/info",
  /**
   * DID of the space the content is stored in.
   */
  with: match$1(),
  nb: struct({
    /**
     * CID of the piece.
     *
     * @see https://github.com/filecoin-project/FIPs/pull/758/files
     */
    piece: PieceLink,
  }),
  derives: (claim, from) => {
    return (
      and$2(equalWith(claim, from)) ||
      and$2(checkLink(claim.nb.piece, from.nb.piece, "nb.piece")) ||
      ok({})
    );
  },
});

/**
 * Filecoin Aggregator Capabilities
 *
 * These can be imported directly with:
 * ```js
 * import * as Aggregator from '@web3-storage/capabilities/filecoin/aggregator'
 * ```
 *
 * @module
 */

/**
 * Capability that allows a Storefront to request that a piece be aggregated
 * for inclusion in an upcoming an Filecoin deal.
 */
const pieceOffer = capability({
  can: "piece/offer",
  /**
   * DID of an authorized Storefront.
   */
  with: match$1(),
  nb: struct({
    /**
     * CID of the piece.
     */
    piece: PieceLink,
    /**
     * Grouping of joining segments into an aggregate.
     */
    group: match(),
  }),
  derives: (claim, from) => {
    return (
      and$2(equalWith(claim, from)) ||
      and$2(checkLink(claim.nb.piece, from.nb.piece, "nb.piece")) ||
      and$2(equal(claim.nb.group, from.nb.group, "nb.group")) ||
      ok({})
    );
  },
});

/**
 * Capability that allows an Aggregator to signal a piece has been accepted
 * or rejected for inclusion in an aggregate.
 */
const pieceAccept = capability({
  can: "piece/accept",
  /**
   * DID of the Aggregator.
   */
  with: match$1(),
  nb: struct({
    /**
     * CID of the piece.
     *
     * @see https://github.com/filecoin-project/FIPs/pull/758/files
     */
    piece: PieceLink,
    /**
     * Grouping of joining segments into an aggregate.
     */
    group: match(),
  }),
  derives: (claim, from) => {
    return (
      and$2(equalWith(claim, from)) ||
      and$2(checkLink(claim.nb.piece, from.nb.piece, "nb.piece")) ||
      and$2(equal(claim.nb.group, from.nb.group, "nb.group")) ||
      ok({})
    );
  },
});

/**
 * Filecoin Dealer Capabilities
 *
 * These can be imported directly with:
 * ```js
 * import * as Dealer from '@web3-storage/capabilities/filecoin/dealer'
 * ```
 *
 * @module
 */

/**
 * Capability allowing an Aggregator to request an aggregate to be added to a
 * deal with a Storage Provider.
 */
const aggregateOffer = capability({
  can: "aggregate/offer",
  /**
   * DID of an authorized Storefront.
   */
  with: match$1(),
  nb: struct({
    /**
     * Commitment proof for the aggregate being offered.
     */
    aggregate: PieceLink,
    /**
     * CID of the DAG-CBOR encoded block with offer details.
     * Service will queue given offer to be validated and handled.
     */
    pieces: match$2({ version: 1 }),
  }),
  derives: (claim, from) => {
    return (
      and$2(equalWith(claim, from)) ||
      and$2(checkLink(claim.nb.aggregate, from.nb.aggregate, "nb.aggregate")) ||
      and$2(checkLink(claim.nb.pieces, from.nb.pieces, "nb.pieces")) ||
      ok({})
    );
  },
});

/**
 * Capability that allows a Dealer to signal an aggregate has been accepted
 * for inclusion in a Filecoin deal.
 */
const aggregateAccept = capability({
  can: "aggregate/accept",
  /**
   * did:key identifier of the broker authority where offer is made available.
   */
  with: match$1(),
  nb: struct({
    /**
     * Commitment proof for the aggregate being offered.
     */
    aggregate: PieceLink,
    /**
     * CID of the DAG-CBOR encoded block with offer details.
     * Service will queue given offer to be validated and handled.
     */
    pieces: match$2(),
  }),
  derives: (claim, from) => {
    return (
      and$2(equalWith(claim, from)) ||
      and$2(checkLink(claim.nb.aggregate, from.nb.aggregate, "nb.aggregate")) ||
      and$2(checkLink(claim.nb.pieces, from.nb.pieces, "nb.pieces")) ||
      ok({})
    );
  },
});

/**
 * Filecoin Deal Tracker Capabilities
 *
 * These can be imported directly with:
 * ```js
 * import * as DealTracker from '@web3-storage/capabilities/filecoin/deal-tracker'
 * ```
 *
 * @module
 */

/**
 * Capability allowing a Storefront or Aggregator to obtain deal information
 * for a given aggregate piece.
 */
const dealInfo = capability({
  can: "deal/info",
  /**
   * DID of the Storefront.
   */
  with: match$1(),
  nb: struct({
    /**
     * CID of the piece.
     *
     * @see https://github.com/filecoin-project/FIPs/pull/758/files
     */
    piece: PieceLink,
  }),
  derives: (claim, from) => {
    return (
      and$2(equalWith(claim, from)) ||
      and$2(checkLink(claim.nb.piece, from.nb.piece, "nb.piece")) ||
      ok({})
    );
  },
});

/**
 * Index Capabilities.
 *
 * W3 Indexing protocol allows authorized agents to submit verifiable claims
 * about content-addressed data to be published on the InterPlanetary Network
 * Indexer (IPNI), making it publicly queryable.
 *
 * These can be imported directly with:
 * ```js
 * import * as Index from '@web3-storage/capabilities/index'
 * ```
 *
 * @module
 */

/**
 * Capability can only be delegated (but not invoked) allowing audience to
 * derive any `space/index/` prefixed capability for the space identified by the DID
 * in the `with` field.
 */
const index = capability({
  can: "space/index/*",
  /** DID of the space where indexed data is stored. */
  with: SpaceDID$1,
  derives: equalWith,
});

/**
 * `space/index/add` capability allows an agent to submit verifiable claims
 * about content-addressed data to be published on the InterPlanetary Network
 * Indexer (IPNI), making it publicly queryable.
 */
const add$5 = capability({
  can: "space/index/add",
  /** DID of the space where indexed data is stored. */
  with: SpaceDID$1,
  nb: struct({
    /** Content Archive (CAR) containing the `Index`. */
    index: match$2({ code: code$c, version: 1 }),
  }),
  derives: (claimed, delegated) =>
    and$2(equalWith(claimed, delegated)) ||
    and$2(equal(claimed.nb.index, delegated.nb.index, "index")) ||
    ok({}),
});

/**
 * Capability can be invoked by an account to get information about
 * the plan it is currently signed up for.
 */
const get$c = capability({
  can: "plan/get",
  with: AccountDID$1,
  derives: (child, parent) => {
    return and$2(equalWith(child, parent)) || ok({});
  },
});

/**
 * Capability can be invoked by an account to change its billing plan.
 */
const set$5 = capability({
  can: "plan/set",
  with: AccountDID$1,
  nb: struct({
    product: DID,
  }),
  derives: (child, parent) => {
    return (
      and$2(equalWith(child, parent)) ||
      and$2(equal(child.nb.product, parent.nb.product, "product")) ||
      ok({})
    );
  },
});

/**
 * Capability can be invoked by an account to generate a billing admin session.
 *
 * May not be possible with all billing providers - this is designed with
 * https://docs.stripe.com/api/customer_portal/sessions/create in mind.
 */
const createAdminSession$1 = capability({
  can: "plan/create-admin-session",
  with: AccountDID$1,
  nb: struct({
    returnURL: string(),
  }),
  derives: (child, parent) => {
    return (
      and$2(equalWith(child, parent)) ||
      and$2(equal(child.nb.returnURL, parent.nb.returnURL, "returnURL")) ||
      ok({})
    );
  },
});

/**
 * Capability can only be delegated (but not invoked) allowing audience to
 * be derived any `usage/` prefixed capability for the (memory) space identified
 * by DID in the `with` field.
 */
const usage = capability({
  can: "usage/*",
  /** DID of the (memory) space where usage is derived. */
  with: SpaceDID$1,
  derives: equalWith,
});

/**
 * Capability can be invoked by an agent to retrieve usage data for a space in
 * a given period.
 */
const report$1 = capability({
  can: "usage/report",
  with: SpaceDID$1,
  nb: struct({
    /** Period to retrieve events between. */
    period: struct({
      /** Time in seconds after Unix epoch (inclusive). */
      from: integer().greaterThan(-1),
      /** Time in seconds after Unix epoch (exclusive). */
      to: integer().greaterThan(-1),
    }),
  }),
  derives: (child, parent) => {
    return (
      and$2(equalWith(child, parent)) ||
      and$2(
        equal(child.nb.period?.from, parent.nb.period?.from, "period.from")
      ) ||
      and$2(equal(child.nb.period?.to, parent.nb.period?.to, "period.to")) ||
      ok({})
    );
  },
});

/**
 * Blob Capabilities.
 *
 * Blob is a fixed size byte array addressed by the multihash.
 * Usually blobs are used to represent set of IPLD blocks at different byte ranges.
 *
 * These can be imported directly with:
 * ```js
 * import * as Blob from '@web3-storage/capabilities/blob'
 * ```
 *
 * @module
 */

/**
 * Agent capabilities for Blob protocol
 */

/**
 * Capability can only be delegated (but not invoked) allowing audience to
 * derived any `space/blob/` prefixed capability for the (memory) space identified
 * by DID in the `with` field.
 */
const blob$1 = capability({
  can: "space/blob/*",
  /**
   * DID of the (memory) space where Blob is intended to
   * be stored.
   */
  with: SpaceDID$1,
  derives: equalWith,
});

/**
 * Blob description for being ingested by the service.
 */
const content = struct({
  /**
   * A multihash digest of the blob payload bytes, uniquely identifying blob.
   */
  digest: bytes(),
  /**
   * Number of bytes contained by this blob. Service will provision write target
   * for this exact size. Attempt to write a larger Blob file will fail.
   */
  size: integer(),
});

/**
 * `space/blob/add` capability allows agent to store a Blob into a (memory) space
 * identified by did:key in the `with` field. Agent should compute blob multihash
 * and size and provide it under `nb.blob` field, allowing a service to provision
 * a write location for the agent to PUT desired Blob into.
 */
const add$4 = capability({
  can: "space/blob/add",
  /**
   * DID of the (memory) space where Blob is intended to
   * be stored.
   */
  with: SpaceDID$1,
  nb: struct({
    /**
     * Blob to be added on the space.
     */
    blob: content,
  }),
  derives: equalBlob,
});

/**
 * Capability can be used to remove the stored Blob from the (memory)
 * space identified by `with` field.
 */
const remove$5 = capability({
  can: "space/blob/remove",
  /**
   * DID of the (memory) space where Blob is stored.
   */
  with: SpaceDID$1,
  nb: struct({
    /**
     * A multihash digest of the blob payload bytes, uniquely identifying blob.
     */
    digest: bytes(),
  }),
  derives: (claimed, delegated) => {
    if (claimed.with !== delegated.with) {
      return fail(
        `Expected 'with: "${delegated.with}"' instead got '${claimed.with}'`
      );
    } else if (
      delegated.nb.digest &&
      !equals$4(delegated.nb.digest, claimed.nb.digest)
    ) {
      return fail(
        `Link ${
          claimed.nb.digest ? `${claimed.nb.digest}` : ""
        } violates imposed ${delegated.nb.digest} constraint.`
      );
    }
    return ok({});
  },
});

/**
 * Capability can be invoked to request a list of stored Blobs in the
 * (memory) space identified by `with` field.
 */
const list$5 = capability({
  can: "space/blob/list",
  /**
   * DID of the (memory) space where Blobs to be listed are stored.
   */
  with: SpaceDID$1,
  nb: struct({
    /**
     * A pointer that can be moved back and forth on the list.
     * It can be used to paginate a list for instance.
     */
    cursor: string().optional(),
    /**
     * Maximum number of items per page.
     */
    size: integer().optional(),
  }),
  derives: (claimed, delegated) => {
    if (claimed.with !== delegated.with) {
      return fail(
        `Expected 'with: "${delegated.with}"' instead got '${claimed.with}'`
      );
    }
    return ok({});
  },
});

/**
 * Capability can be used to get the stored Blob from the (memory)
 * space identified by `with` field.
 */
const get$b = capability({
  can: "space/blob/get/0/1",
  /**
   * DID of the (memory) space where Blob is stored.
   */
  with: SpaceDID$1,
  nb: struct({
    /**
     * A multihash digest of the blob payload bytes, uniquely identifying blob.
     */
    digest: bytes(),
  }),
  derives: (claimed, delegated) => {
    if (claimed.with !== delegated.with) {
      return fail(
        `Expected 'with: "${delegated.with}"' instead got '${claimed.with}'`
      );
    } else if (
      delegated.nb.digest &&
      !equals$4(delegated.nb.digest, claimed.nb.digest)
    ) {
      return fail(
        `Link ${
          claimed.nb.digest ? `${claimed.nb.digest}` : ""
        } violates imposed ${delegated.nb.digest} constraint.`
      );
    }
    return ok({});
  },
});

/**
 * Service capabilities for Blob protocol
 */
/**
 * Capability can only be delegated (but not invoked) allowing audience to
 * derived any `web3.storage/blob/` prefixed capability for the (memory) space identified
 * by DID in the `with` field.
 */
const blob = capability({
  can: "web3.storage/blob/*",
  /**
   * DID of the (memory) space where Blob is intended to
   * be stored.
   */
  with: SpaceDID$1,
  derives: equalWith,
});

/**
 * `web3.storage/blob//allocate` capability can be invoked to create a memory
 * address where blob content can be written via HTTP PUT request.
 */
const allocate = capability({
  can: "web3.storage/blob/allocate",
  /**
   * Provider DID.
   */
  with: match$1(),
  nb: struct({
    /**
     * Blob to allocate on the space.
     */
    blob: content,
    /**
     * The Link for an Add Blob task, that caused an allocation
     */
    cause: Link,
    /**
     * DID of the user space where allocation takes place
     */
    space: SpaceDID$1,
  }),
  derives: (claim, from) => {
    return (
      and$2(equalWith(claim, from)) ||
      and$2(equalBlob(claim, from)) ||
      and$2(checkLink(claim.nb.cause, from.nb.cause, "cause")) ||
      and$2(equal(claim.nb.space, from.nb.space, "space")) ||
      ok({})
    );
  },
});

/**
 * `blob/accept` capability invocation should either succeed when content is
 * delivered on allocated address or fail if no content is allocation expires
 * without content being delivered.
 */
const accept = capability({
  can: "web3.storage/blob/accept",
  /**
   * Provider DID.
   */
  with: match$1(),
  nb: struct({
    /**
     * Blob to accept.
     */
    blob: content,
    /**
     * Content location commitment time to live, which will be encoded as expiry of the issued location claim.
     */
    ttl: integer().optional(),
    /**
     * DID of the user space where allocation took place
     */
    space: SpaceDID$1,
    /**
     * This task is blocked on `http/put` receipt available
     */
    _put: Await,
  }),
  derives: (claim, from) => {
    return (
      and$2(equalWith(claim, from)) ||
      and$2(equalBlob(claim, from)) ||
      and$2(equal(claim.nb.ttl, from.nb.ttl, "ttl")) ||
      and$2(equal(claim.nb.space, from.nb.space, "space")) ||
      ok({})
    );
  },
});

/**
 * HTTP Capabilities.
 *
 * These can be imported directly with:
 * ```js
 * import * as HTTP from '@web3-storage/capabilities/http'
 * ```
 *
 * @module
 */

/**
 * `http/put` capability invocation MAY be performed by any authorized agent on behalf of the subject
 * as long as they have referenced `body` content to do so.
 */
const put = capability({
  can: "http/put",
  /**
   * DID of the (memory) space where Blob is intended to
   * be stored.
   */
  with: SpaceDID$1,
  nb: struct({
    /**
     * Description of body to send (digest/size).
     */
    body: content,
    /**
     * HTTP(S) location that can receive blob content via HTTP PUT request.
     */
    url: string().or(Await),
    /**
     * HTTP headers.
     */
    headers: dictionary({ value: string() }).or(Await),
  }),
  derives: (claim, from) => {
    return (
      and$2(equalWith(claim, from)) ||
      and$2(equalBody(claim, from)) ||
      and$2(equal(claim.nb.url, from.nb, "url")) ||
      and$2(equal(claim.nb.headers, from.nb, "headers")) ||
      ok({})
    );
  },
});

/** @type {import('./types.js').ServiceAbility[]} */
const abilitiesAsStrings = [
  top.can,
  add$8.can,
  space.can,
  info.can,
  upload$1.can,
  add$9.can,
  get$g.can,
  remove$7.can,
  list$8.can,
  store$1.can,
  add$a.can,
  get$h.can,
  remove$8.can,
  list$9.can,
  access$1.can,
  authorize.can,
  attest.can,
  conclude$1.can,
  get$e.can,
  has$1.can,
  get$f.can,
  get$d.can,
  list$6.can,
  add$6.can,
  remove$6.can,
  list$7.can,
  filecoinOffer$1.can,
  filecoinSubmit.can,
  filecoinAccept.can,
  filecoinInfo$1.can,
  pieceOffer.can,
  pieceAccept.can,
  aggregateOffer.can,
  aggregateAccept.can,
  dealInfo.can,
  admin.can,
  upload.inspect.can,
  store.inspect.can,
  get$c.can,
  set$5.can,
  createAdminSession$1.can,
  usage.can,
  report$1.can,
  blob$1.can,
  add$4.can,
  remove$5.can,
  list$5.can,
  blob.can,
  allocate.can,
  accept.can,
  put.can,
  index.can,
  add$5.can,
];

/**
 * Convert a Uint8Array to an ArrayBuffer, taking into account
 * that we may be looking at a "data view".
 * thanks, https://stackoverflow.com/a/54646864
 *
 * If we aren't looking at a data view, simply returns the underlying ArrayBuffer
 * directly.
 *
 * @param {Uint8Array} array
 * @returns ArrayBuffer
 */
function uint8ArrayToArrayBuffer(array) {
  if (array.byteOffset === 0 && array.byteLength === array.buffer.byteLength) {
    return array.buffer;
  } else {
    return array.buffer.slice(
      array.byteOffset,
      array.byteLength + array.byteOffset
    );
  }
}

/** @typedef {import('./types.js').AgentDataModel} AgentDataModel */
/** @implements {AgentDataModel} */
class AgentData {
  /** @type {(data: import('./types.js').AgentDataExport) => Promise<void> | void} */
  #save;
  /**
   * @param {import('./types.js').AgentDataModel} data
   * @param {import('./types.js').AgentDataOptions} [options]
   */
  constructor(data, options = {}) {
    this.meta = data.meta;
    this.principal = data.principal;
    this.spaces = data.spaces;
    this.delegations = data.delegations;
    this.currentSpace = data.currentSpace;
    this.#save = (data) =>
      options.store ? options.store.save(data) : undefined;
  }
  /**
   * Create a new AgentData instance from the passed initialization data.
   *
   * @param {Partial<import('./types.js').AgentDataModel>} [init]
   * @param {import('./types.js').AgentDataOptions} [options]
   */
  static async create(init = {}, options = {}) {
    const agentData = new AgentData(
      {
        meta: { name: "agent", type: "device", ...init.meta },
        principal: init.principal ?? (await generate$2()),
        spaces: init.spaces ?? new Map(),
        delegations: init.delegations ?? new Map(),
        currentSpace: init.currentSpace,
      },
      options
    );
    if (options.store) {
      await options.store.save(agentData.export());
    }
    return agentData;
  }
  /**
   * Instantiate AgentData from previously exported data.
   *
   * @param {import('./types.js').AgentDataExport} raw
   * @param {import('./types.js').AgentDataOptions} [options]
   */
  static fromExport(raw, options) {
    /** @type {import('./types.js').AgentDataModel['delegations']} */
    const dels = new Map();
    for (const [key, value] of raw.delegations) {
      dels.set(key, {
        delegation: importDAG(
          value.delegation.map((d) => ({
            cid: CID$2.parse(d.cid).toV1(),
            bytes:
              d.bytes instanceof Uint8Array ? d.bytes : new Uint8Array(d.bytes),
          }))
        ),
        meta: value.meta,
      });
    }
    return new AgentData(
      {
        meta: raw.meta,
        // @ts-expect-error for some reason TS thinks this is a EdSigner
        principal: Signer.from(raw.principal),
        currentSpace: raw.currentSpace,
        spaces: raw.spaces,
        delegations: dels,
      },
      options
    );
  }
  /**
   * Export data in a format safe to pass to `structuredClone()`.
   */
  export() {
    /** @type {import('./types.js').AgentDataExport} */
    const raw = {
      meta: this.meta,
      principal: this.principal.toArchive(),
      currentSpace: this.currentSpace,
      spaces: this.spaces,
      delegations: new Map(),
    };
    for (const [key, value] of this.delegations) {
      raw.delegations.set(key, {
        meta: value.meta,
        delegation: [...value.delegation.export()].map((b) => ({
          cid: b.cid.toString(),
          bytes: uint8ArrayToArrayBuffer(b.bytes),
        })),
      });
    }
    return raw;
  }
  /**
   * @param {import('@ucanto/interface').DID} did
   * @param {import('./types.js').SpaceMeta} meta
   * @param {import('@ucanto/interface').Delegation} [proof]
   */
  async addSpace(did, meta, proof) {
    this.spaces.set(did, meta);
    await (proof ? this.addDelegation(proof) : this.#save(this.export()));
  }
  /**
   * @deprecated
   * @param {import('@ucanto/interface').DID<'key'>} did
   */
  async setCurrentSpace(did) {
    this.currentSpace = did;
    await this.#save(this.export());
  }
  /**
   * @param {import('@ucanto/interface').Delegation} delegation
   * @param {import('./types.js').DelegationMeta} [meta]
   */
  async addDelegation(delegation, meta) {
    this.delegations.set(delegation.cid.toString(), {
      delegation,
      meta: meta ?? {},
    });
    await this.#save(this.export());
  }
  /**
   * @param {import('@ucanto/interface').UCANLink} cid
   */
  async removeDelegation(cid) {
    this.delegations.delete(cid.toString());
    await this.#save(this.export());
  }
}
/**
 * Is the given capability a session attestation?
 *
 * @param {Ucanto.Capability} cap
 * @returns {boolean}
 */
const isSessionCapability = (cap) => cap.can === attest.can;
/**
 * Is the given delegation a session proof?
 *
 * @param {Ucanto.Delegation} delegation
 * @returns {delegation is Ucanto.Delegation<[import('./types.js').UCANAttest]>}
 */
const isSessionProof = (delegation) =>
  delegation.capabilities.some((cap) => isSessionCapability(cap));
/**
 * @typedef {string} SessionProofAuthorizationCid - the nb.proof CID of the ucan/attest in the session proof
 * @typedef {Ucanto.DID} SessionProofIssuer - issuer of ucan/attest session proof
 * @typedef {Record<SessionProofAuthorizationCid, Record<SessionProofIssuer, [Ucanto.Delegation, ...Ucanto.Delegation[]]>>} SessionProofIndexedByAuthorizationAndIssuer
 */
/**
 * Get a map from CIDs to the session proofs that reference them
 *
 * @param {AgentData} data
 * @returns {SessionProofIndexedByAuthorizationAndIssuer}
 */
function getSessionProofs(data) {
  /** @type {SessionProofIndexedByAuthorizationAndIssuer} */
  const proofs = {};
  for (const { delegation } of data.delegations.values()) {
    if (isSessionProof(delegation)) {
      const cap = delegation.capabilities[0];
      if (cap && !isExpired(delegation)) {
        const proof = cap.nb.proof;
        if (proof) {
          const proofCid = proof.toString();
          const issuerDid = delegation.issuer.did();
          proofs[proofCid] = proofs[proofCid] ?? {};
          proofs[proofCid][issuerDid] = proofs[proofCid][issuerDid] ?? [];
          proofs[proofCid][issuerDid].push(delegation);
        }
      }
    }
  }
  return proofs;
}

match$1({ method: "web" });

const HOST = "https://up.web3.storage";
const PRINCIPAL = parse$1("did:web:web3.storage");
/**
 * Keeps track of AgentData for all Agents constructed.
 * Used by addSpacesFromDelegations - so it can only accept Agent as param, but
 * still mutate corresponding AgentData
 *
 * @deprecated - remove this when deprecated addSpacesFromDelegations is removed
 */
/** @type {WeakMap<Agent<Record<string, any>>, AgentData>} */
const agentToData = new WeakMap();
/**
 * @typedef {API.Service} Service
 * @typedef {API.Receipt<any, any>} Receipt
 */
/**
 * Creates a Ucanto connection for the w3access API
 *
 * Usage:
 *
 * ```js
 * import { connection } from '@web3-storage/access/agent'
 * ```
 *
 * @template {API.DID} T - DID method
 * @template {Record<string, any>} [S=Service]
 * @param {object} [options]
 * @param {API.Principal<T>} [options.principal] - w3access API Principal
 * @param {URL} [options.url] - w3access API URL
 * @param {API.Transport.Channel<S>} [options.channel] - Ucanto channel to use
 * @param {typeof fetch} [options.fetch] - Fetch implementation to use
 * @returns {API.ConnectionView<S>}
 */
function connection$2(options = {}) {
  return connect({
    id: options.principal ?? PRINCIPAL,
    codec: outbound,
    channel:
      options.channel ??
      open$2({
        url: options.url ?? new URL(HOST),
        method: "POST",
        fetch: options.fetch ?? globalThis.fetch.bind(globalThis),
      }),
  });
}
/**
 * Agent
 *
 * Usage:
 *
 * ```js
 * import { Agent } from '@web3-storage/access/agent'
 * ```
 *
 * @template {Record<string, any>} [S=Service]
 */
class Agent {
  /** @type {import('./agent-data.js').AgentData} */
  #data;
  /**
   * @param {import('./agent-data.js').AgentData} data - Agent data
   * @param {import('./types.js').AgentOptions<S>} [options]
   */
  constructor(data, options = {}) {
    /** @type { Client.Channel<S> & { url?: URL } | undefined } */
    const channel = options.connection?.channel;
    this.url = options.url ?? channel?.url ?? new URL(HOST);
    this.connection =
      options.connection ??
      connection$2({
        principal: options.servicePrincipal,
        url: this.url,
      });
    this.#data = data;
    agentToData.set(this, this.#data);
  }
  /**
   * Create a new Agent instance, optionally with the passed initialization data.
   *
   * @template {Record<string, any>} [R=Service]
   * @param {Partial<import('./types.js').AgentDataModel>} [init]
   * @param {import('./types.js').AgentOptions<R> & import('./types.js').AgentDataOptions} [options]
   */
  static async create(init, options = {}) {
    const data = await AgentData.create(init, options);
    return new Agent(data, options);
  }
  /**
   * Instantiate an Agent from pre-exported agent data.
   *
   * @template {Record<string, any>} [R=Service]
   * @param {import('./types.js').AgentDataExport} raw
   * @param {import('./types.js').AgentOptions<R> & import('./types.js').AgentDataOptions} [options]
   */
  static from(raw, options = {}) {
    const data = AgentData.fromExport(raw, options);
    return new Agent(data, options);
  }
  get issuer() {
    return this.#data.principal;
  }
  get meta() {
    return this.#data.meta;
  }
  get spaces() {
    return this.#data.spaces;
  }
  did() {
    return this.#data.principal.did();
  }
  /**
   * Add a proof to the agent store.
   *
   * @param {API.Delegation} delegation
   */
  async addProof(delegation) {
    return await this.addProofs([delegation]);
  }
  /**
   * Adds set of proofs to the agent store.
   *
   * @param {Iterable<API.Delegation>} delegations
   */
  async addProofs(delegations) {
    for (const proof of delegations) {
      await this.#data.addDelegation(proof, { audience: this.meta });
    }
    await this.removeExpiredDelegations();
    return {};
  }
  /**
   * Query the delegations store for all the delegations matching the capabilities provided.
   *
   * @param {API.CapabilityQuery[]} [caps]
   */
  #delegations(caps) {
    const _caps = new Set(caps);
    /** @type {Array<{ delegation: API.Delegation, meta: API.DelegationMeta }>} */
    const values = [];
    for (const [, value] of this.#data.delegations) {
      // check expiration
      if (
        !isExpired(value.delegation) && // check if delegation can be used
        !isTooEarly(value.delegation)
      ) {
        // check if we need to filter for caps
        if (Array.isArray(caps) && caps.length > 0) {
          for (const cap of _caps) {
            if (canDelegateCapability(value.delegation, cap)) {
              values.push(value);
            }
          }
        } else {
          values.push(value);
        }
      }
    }
    return values;
  }
  /**
   * Clean up any expired delegations.
   */
  async removeExpiredDelegations() {
    for (const [, value] of this.#data.delegations) {
      if (isExpired(value.delegation)) {
        await this.#data.removeDelegation(value.delegation.cid);
      }
    }
  }
  /**
   * Revoke a delegation by CID.
   *
   * If the delegation was issued by this agent (and therefore is stored in the
   * delegation store) you can just pass the CID. If not, or if the current agent's
   * delegation store no longer contains the delegation, you MUST pass a chain of
   * proofs that proves your authority to revoke this delegation as `options.proofs`.
   *
   * @param {API.UCANLink} delegationCID
   * @param {object} [options]
   * @param {API.Delegation[]} [options.proofs]
   */
  async revoke(delegationCID, options = {}) {
    const additionalProofs = options.proofs ?? [];
    // look for the identified delegation in the delegation store and the passed proofs
    const delegation = [...this.delegations(), ...additionalProofs].find(
      (delegation) => delegation.cid.equals(delegationCID)
    );
    if (!delegation) {
      return {
        error: new Error(
          `could not find delegation ${delegationCID.toString()} - please include the delegation in options.proofs`
        ),
      };
    }
    const receipt = await this.invokeAndExecute(revoke, {
      // per https://github.com/storacha/w3up/blob/main/packages/capabilities/src/ucan.js#L38C6-L38C6 the resource here should be
      // the current issuer - using the space DID here works for simple cases but falls apart when a delegee tries to revoke a delegation
      // they have re-delegated, since they don't have "ucan/revoke" capabilities on the space
      with: this.issuer.did(),
      nb: {
        ucan: delegation.cid,
      },
      proofs: [delegation, ...additionalProofs],
    });
    return receipt.out;
  }
  /**
   * Get all the proofs matching the capabilities.
   *
   * Proofs are delegations with an audience matching agent DID, or with an
   * audience matching the session DID.
   *
   * Proof of session will also be included in the returned proofs if any
   * proofs matching the passed capabilities require it.
   *
   * @param {API.CapabilityQuery[]} [caps] - Capabilities to filter by. Empty or undefined caps with return all the proofs.
   * @param {object} [options]
   * @param {API.DID} [options.sessionProofIssuer] - only include session proofs for this issuer
   */
  proofs(caps, options) {
    const authorizations = [];
    for (const { delegation } of this.#delegations(caps)) {
      if (delegation.audience.did() === this.issuer.did()) {
        authorizations.push(delegation);
      }
    }
    // now let's add any session proofs that refer to those authorizations
    const sessions = getSessionProofs(this.#data);
    for (const proof of authorizations) {
      const proofsByIssuer = sessions[proof.asCID.toString()] ?? {};
      const sessionProofs = options?.sessionProofIssuer
        ? proofsByIssuer[options.sessionProofIssuer] ?? []
        : Object.values(proofsByIssuer).flat();
      if (sessionProofs.length) {
        authorizations.push(...sessionProofs);
      }
    }
    return authorizations;
  }
  /**
   * Get delegations created by the agent for others.
   *
   * @param {API.CapabilityQuery[]} [caps] - Capabilities to filter by. Empty or undefined caps with return all the delegations.
   */
  delegations(caps) {
    const arr = [];
    for (const { delegation } of this.delegationsWithMeta(caps)) {
      arr.push(delegation);
    }
    return arr;
  }
  /**
   * Get delegations created by the agent for others and their metadata.
   *
   * @param {API.CapabilityQuery[]} [caps] - Capabilities to filter by. Empty or undefined caps with return all the delegations.
   */
  delegationsWithMeta(caps) {
    const arr = [];
    for (const value of this.#delegations(caps)) {
      const { delegation } = value;
      const isSession = delegation.capabilities.some(
        (c) => c.can === attest.can
      );
      if (!isSession && delegation.audience.did() !== this.issuer.did()) {
        arr.push(value);
      }
    }
    return arr;
  }
  /**
   * Creates a space signer and a delegation to the agent
   *
   * @param {string} name
   */
  async createSpace(name) {
    return await generate$1({ name, agent: this });
  }
  /**
   * @param {string} secret
   * @param {object} options
   * @param {string} options.name
   */
  async recoverSpace(secret, { name }) {
    return await fromMnemonic(secret, { name, agent: this });
  }
  /**
   * Import a space from a delegation.
   *
   * @param {API.Delegation} delegation
   * @param {object} options
   * @param {string} [options.name]
   */
  async importSpaceFromDelegation(delegation, { name = "" } = {}) {
    const space =
      name === ""
        ? fromDelegation(delegation)
        : fromDelegation(delegation).withName(name);
    this.#data.spaces.set(space.did(), { ...space.meta, name: space.name });
    await this.addProof(space.delegation);
    // if we do not have a current space, make this one current
    if (!this.currentSpace()) {
      await this.setCurrentSpace(space.did());
    }
    return space;
  }
  /**
   * Sets the current selected space
   *
   * Other methods will default to use the current space if no resource is defined
   *
   * @param {API.SpaceDID} space
   */
  async setCurrentSpace(space) {
    if (!this.#data.spaces.has(space)) {
      throw new Error(`Agent has no proofs for ${space}.`);
    }
    await this.#data.setCurrentSpace(space);
    return space;
  }
  /**
   * Get current space DID
   */
  currentSpace() {
    return this.#data.currentSpace;
  }
  /**
   * Get current space DID, proofs and abilities
   */
  currentSpaceWithMeta() {
    if (!this.#data.currentSpace) {
      return;
    }
    const proofs = this.proofs([
      {
        can: "space/info",
        with: this.#data.currentSpace,
      },
    ]);
    const caps = new Set();
    for (const p of proofs) {
      for (const cap of p.capabilities) {
        caps.add(cap.can);
      }
    }
    return {
      did: this.#data.currentSpace,
      proofs,
      capabilities: [...caps],
      meta: this.#data.spaces.get(this.#data.currentSpace),
    };
  }
  /**
   *
   * @param {import('./types.js').DelegationOptions} options
   */
  async delegate(options) {
    const space = this.currentSpaceWithMeta();
    if (!space) {
      throw new Error("no space selected.");
    }
    const caps = /** @type {API.Capabilities} */ (
      options.abilities.map((a) => {
        return {
          with: space.did,
          can: a,
        };
      })
    );
    // Verify agent can provide proofs for each requested capability
    for (const cap of caps) {
      if (!this.proofs([cap]).length) {
        throw new Error(
          `cannot delegate capability ${cap.can} with ${cap.with}`
        );
      }
    }
    const delegation = await delegate$3({
      issuer: this.issuer,
      capabilities: caps,
      proofs: this.proofs(caps),
      facts: [{ space: space.meta ?? {} }],
      ...options,
    });
    await this.#data.addDelegation(delegation, {
      audience: options.audienceMeta,
    });
    await this.removeExpiredDelegations();
    return delegation;
  }
  /**
   * Invoke and execute the given capability on the Access service connection
   *
   * ```js
   *
   * await agent.invokeAndExecute(Space.recover, {
   *   nb: {
   *     identity: 'mailto: email@gmail.com',
   *   },
   * })
   *
   * // sugar for
   * const recoverInvocation = await agent.invoke(Space.recover, {
   *   nb: {
   *     identity: 'mailto: email@gmail.com',
   *   },
   * })
   *
   * await recoverInvocation.execute(agent.connection)
   * ```
   *
   * @template {API.Ability} A
   * @template {API.URI} R
   * @template {API.Caveats} C
   * @param {API.TheCapabilityParser<API.CapabilityMatch<A, R, C>>} cap
   * @param {API.InvokeOptions<A, R, API.TheCapabilityParser<API.CapabilityMatch<A, R, C>>>} options
   * @returns {Promise<API.InferReceipt<API.Capability<A, R, C>, S>>}
   */
  async invokeAndExecute(cap, options) {
    const inv = await this.invoke(cap, options);
    const out = inv.execute(/** @type {*} */ (this.connection));
    return /** @type {*} */ (out);
  }
  /**
   * Execute invocations on the agent's connection
   *
   * @example
   * ```js
   * const i1 = await agent.invoke(Space.info, {})
   * const i2 = await agent.invoke(Space.recover, {
   *   nb: {
   *     identity: 'mailto:hello@web3.storage',
   *   },
   * })
   *
   * const results = await agent.execute2(i1, i2)
   *
   * ```
   * @template {API.Capability} C
   * @template {API.Tuple<API.ServiceInvocation<C, S>>} I
   * @param {I} invocations
   */
  execute(...invocations) {
    return this.connection.execute(...invocations);
  }
  /**
   * Creates an invocation for the given capability with Agent's proofs, service, issuer and space.
   *
   * @example
   * ```js
   * const recoverInvocation = await agent.invoke(Space.recover, {
   *   nb: {
   *     identity: 'mailto: email@gmail.com',
   *   },
   * })
   *
   * await recoverInvocation.execute(agent.connection)
   * // or
   * await agent.execute(recoverInvocation)
   * ```
   *
   * @template {API.Ability} A
   * @template {API.URI} R
   * @template {API.TheCapabilityParser<API.CapabilityMatch<A, R, C>>} CAP
   * @template {API.Caveats} [C={}]
   * @param {CAP} cap
   * @param {import('./types.js').InvokeOptions<A, R, CAP>} options
   */
  async invoke(cap, options) {
    const audience = options.audience || this.connection.id;
    const space = options.with || this.currentSpace();
    if (!space) {
      throw new Error(
        "No space or resource selected, you need pass a resource."
      );
    }
    const proofs = [
      ...(options.proofs || []),
      ...this.proofs(
        [
          {
            with: space,
            can: cap.can,
          },
        ],
        { sessionProofIssuer: audience.did() }
      ),
    ];
    if (proofs.length === 0 && options.with !== this.did()) {
      throw new Error(
        `no proofs available for resource ${space} and ability ${cap.can}`
      );
    }
    const inv = invoke({
      ...options,
      audience,
      // @ts-ignore
      capability: cap.create({
        with: space,
        nb: options.nb,
      }),
      issuer: this.issuer,
      proofs: [...proofs],
      nonce: options.nonce,
    });
    return /** @type {API.IssuedInvocationView<API.InferInvokedCapability<CAP>>} */ (
      inv
    );
  }
  /**
   * Get Space information from Access service
   *
   * @param {API.URI<"did:">} [space]
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async getSpaceInfo(space, options) {
    const _space = space || this.currentSpace();
    if (!_space) {
      throw new Error("No space selected, you need pass a resource.");
    }
    const inv = await this.invokeAndExecute(info, {
      ...options,
      with: _space,
    });
    if (inv.out.error) {
      throw inv.out.error;
    }
    return /** @type {import('./types.js').SpaceInfoResult} */ (inv.out.ok);
  }
}
/**
 * Given a list of delegations, add to agent data spaces list.
 *
 * @deprecated - trying to remove explicit space tracking from Agent/AgentData
 * in favor of functions that derive the space set from access.delegations
 *
 * @template {Record<string, any>} [S=Service]
 * @param {Agent<S>} agent
 * @param {API.Delegation[]} delegations
 */
async function addSpacesFromDelegations(agent, delegations) {
  const data = agentToData.get(agent);
  if (!data) {
    throw Object.assign(new Error(`cannot determine AgentData for Agent`), {
      agent: agent,
    });
  }
  // spaces we find along the way.
  const spaces = new Map();
  // only consider ucans with this agent as the audience
  const ours = delegations.filter((x) => x.audience.did() === agent.did());
  // space names are stored as facts in proofs in the special `ucan:*` delegation from email to agent.
  const ucanStars = ours.filter(
    (x) => x.capabilities[0].can === "*" && x.capabilities[0].with === "ucan:*"
  );
  for (const delegation of ucanStars) {
    for (const proof of delegation.proofs) {
      if (
        !isDelegation(proof) ||
        !proof.capabilities[0].with.startsWith("did:key")
      ) {
        continue;
      }
      const space = fromDelegation(proof);
      spaces.set(space.did(), space.meta);
    }
  }
  // Find any other spaces the user may have access to
  for (const delegation of ours) {
    // TODO: we need a more robust way to determine which spaces a user has access to
    // it may or may not involve look at delegations
    const allows$1 = allows(delegation);
    for (const [resource, value] of Object.entries(allows$1)) {
      // If we discovered a delegation to any DID, we add it to the spaces list.
      if (resource.startsWith("did:key") && Object.keys(value).length > 0) {
        if (!spaces.has(resource)) {
          spaces.set(resource, {});
        }
      }
    }
  }
  for (const [did, meta] of spaces) {
    await data.addSpace(did, meta);
  }
}
/**
 * Stores given delegations in the agent's data store and adds discovered spaces
 * to the agent's space list.
 *
 * @param {Agent} agent
 * @param {object} authorization
 * @param {API.Delegation[]} authorization.proofs
 * @returns {Promise<API.Result<API.Unit, Error>>}
 */
const importAuthorization = async (agent, { proofs }) => {
  try {
    await agent.addProofs(proofs);
    await addSpacesFromDelegations(agent, proofs);
    return { ok: {} };
  } catch (error) {
    return /** @type {{error:Error}} */ ({ error });
  }
};

function pDefer() {
  const deferred = {};

  deferred.promise = new Promise((resolve, reject) => {
    deferred.resolve = resolve;
    deferred.reject = reject;
  });

  return deferred;
}

/**
 * @template T
 * @typedef {import('./types.js').Driver<T>} Driver
 */
const STORE_NAME = "AccessStore";
const DATA_ID = 1;
/**
 * Driver implementation for the browser.
 *
 * Usage:
 *
 * ```js
 * import { IndexedDBDriver } from '@web3-storage/access/drivers/indexeddb'
 * ```
 *
 * @template T
 * @implements {Driver<T>}
 */
class IndexedDBDriver {
  /** @type {string} */
  #dbName;
  /** @type {number|undefined} */
  #dbVersion;
  /** @type {string} */
  #dbStoreName;
  /** @type {IDBDatabase|undefined} */
  #db;
  /** @type {boolean} */
  #autoOpen;
  /**
   * @param {string} dbName
   * @param {object} [options]
   * @param {number} [options.dbVersion]
   * @param {string} [options.dbStoreName]
   * @param {boolean} [options.autoOpen]
   */
  constructor(dbName, options = {}) {
    this.#dbName = dbName;
    this.#dbVersion = options.dbVersion;
    this.#dbStoreName = options.dbStoreName ?? STORE_NAME;
    this.#autoOpen = options.autoOpen ?? true;
  }
  /** @returns {Promise<IDBDatabase>} */
  async #getOpenDB() {
    if (!this.#db) {
      if (!this.#autoOpen) throw new Error("Store is not open");
      await this.open();
    }
    // @ts-expect-error open sets this.#db
    return this.#db;
  }
  async open() {
    const db = this.#db;
    if (db) return;
    /** @type {import('p-defer').DeferredPromise<void>} */
    const { resolve, reject, promise } = pDefer();
    const openReq = indexedDB.open(this.#dbName, this.#dbVersion);
    openReq.addEventListener("upgradeneeded", () => {
      const db = openReq.result;
      db.createObjectStore(this.#dbStoreName, { keyPath: "id" });
    });
    openReq.addEventListener("success", () => {
      this.#db = openReq.result;
      resolve();
    });
    openReq.addEventListener("error", () => reject(openReq.error));
    return promise;
  }
  async close() {
    const db = this.#db;
    if (!db) throw new Error("Store is not open");
    db.close();
    this.#db = undefined;
  }
  /** @param {T} data */
  async save(data) {
    const db = await this.#getOpenDB();
    const putData = withObjectStore(
      db,
      "readwrite",
      this.#dbStoreName,
      async (store) => {
        /** @type {import('p-defer').DeferredPromise<void>} */
        const { resolve, reject, promise } = pDefer();
        const putReq = store.put({ id: DATA_ID, ...data });
        putReq.addEventListener("success", () => resolve());
        putReq.addEventListener("error", () =>
          reject(new Error("failed to query DB", { cause: putReq.error }))
        );
        return promise;
      }
    );
    return await putData();
  }
  async load() {
    const db = await this.#getOpenDB();
    const getData = withObjectStore(
      db,
      "readonly",
      this.#dbStoreName,
      async (store) => {
        /** @type {import('p-defer').DeferredPromise<T|undefined>} */
        const { resolve, reject, promise } = pDefer();
        const getReq = store.get(DATA_ID);
        getReq.addEventListener("success", () => resolve(getReq.result));
        getReq.addEventListener("error", () =>
          reject(new Error("failed to query DB", { cause: getReq.error }))
        );
        return promise;
      }
    );
    return await getData();
  }
  async reset() {
    const db = await this.#getOpenDB();
    const clear = withObjectStore(db, "readwrite", this.#dbStoreName, (s) => {
      /** @type {import('p-defer').DeferredPromise<void>} */
      const { resolve, reject, promise } = pDefer();
      const req = s.clear();
      req.addEventListener("success", () => {
        resolve();
      });
      req.addEventListener("error", () =>
        reject(new Error("failed to query DB", { cause: req.error }))
      );
      return promise;
    });
    await clear();
  }
}
/**
 * @template T
 * @param {IDBDatabase} db
 * @param {IDBTransactionMode} txnMode
 * @param {string} storeName
 * @param {(s: IDBObjectStore) => Promise<T>} fn
 * @returns
 */
function withObjectStore(db, txnMode, storeName, fn) {
  return async () => {
    const tx = db.transaction(storeName, txnMode);
    /** @type {import('p-defer').DeferredPromise<T>} */
    const { resolve, reject, promise } = pDefer();
    /** @type {T} */
    let result;
    tx.addEventListener("complete", () => resolve(result));
    tx.addEventListener("abort", () =>
      reject(tx.error || new Error("transaction aborted"))
    );
    tx.addEventListener("error", () =>
      reject(new Error("transaction error", { cause: tx.error }))
    );
    try {
      result = await fn(tx.objectStore(storeName));
      tx.commit();
    } catch (error) {
      reject(error);
      tx.abort();
    }
    return promise;
  };
}

/**
 * Store implementation for the browser.
 *
 * Usage:
 *
 * ```js
 * import { StoreIndexedDB } from '@web3-storage/access/stores/store-indexeddb'
 * ```
 *
 * @extends {IndexedDBDriver<import('../types.js').AgentDataExport>}
 */
class StoreIndexedDB extends IndexedDBDriver {}

/**
 * Number of bits per byte
 */
const BITS_PER_BYTE = 8;

/**
 * The number of Frs per Block.
 */
const FRS_PER_QUAD = 4;

/** @type {4n} */ (BigInt(FRS_PER_QUAD));

/**
 * The amount of bits in an Fr when not padded.
 */
const IN_BITS_FR = 254;
/**
 * The amount of bits in an Fr when padded.
 */
const OUT_BITS_FR = 256;

const IN_BYTES_PER_QUAD =
  /** @type {127} */
  ((FRS_PER_QUAD * IN_BITS_FR) / BITS_PER_BYTE);

const OUT_BYTES_PER_QUAD =
  /** @type {128} */
  ((FRS_PER_QUAD * OUT_BITS_FR) / BITS_PER_BYTE);

const PADDED_BYTES_PER_QUAD = /** @type {127n} */ (BigInt(IN_BYTES_PER_QUAD));

/** @type {128n} */ (BigInt(OUT_BYTES_PER_QUAD));

const FR_RATIO = IN_BITS_FR / OUT_BITS_FR;

/**
 * Size of a node in the merkle tree.
 */
const NODE_SIZE =
  /** @type {32} */
  (OUT_BYTES_PER_QUAD / FRS_PER_QUAD);

const EXPANDED_BYTES_PER_NODE = /** @type {32n} */ (BigInt(NODE_SIZE));

/**
 * The smallest amount of data for which FR32 padding has a defined result.
 * Silently upgrading 2 leaves to 4 would break the symmetry so we require
 * an extra byte and the rest can be 0 padded to expand to 4 leaves.
 */
const MIN_PAYLOAD_SIZE = 2 * NODE_SIZE + 1;

/**
 * @param {Iterable<number>} bytes
 * @returns {API.MerkleTreeNode}
 */
const from$9 = (bytes) => {
  /* c8 ignore next 7 */
  if (bytes instanceof Uint8Array) {
    if (bytes.length > NODE_SIZE) {
      return bytes.subarray(0, NODE_SIZE);
    } else if (bytes.length == NODE_SIZE) {
      return bytes;
    }
  }

  const node = new Uint8Array(NODE_SIZE);
  node.set([...bytes]);
  return node;
};

const empty$3 = () => EMPTY$4;

const EMPTY$4 = from$9(new Uint8Array(NODE_SIZE).fill(0));
Object.freeze(EMPTY$4.buffer);

const name$5 = "sha2-256";
const code$7 = 0x12;
const size$2 = 32;

const prefix = new Uint8Array([18, 32]);

let Digest$4 = class Digest {
  /**
   * @param {Uint8Array} bytes
   */
  constructor(bytes) {
    /** @type {typeof code} */
    this.code = code$7;
    /** @type {typeof name} */
    this.name = name$5;
    this.bytes = bytes;
    /** @type {typeof size} */
    this.size = size$2;
    this.digest = bytes.subarray(2);
  }
};

/* c8 ignore next */

/**
 * @param {Uint8Array} payload
 * @returns {import('multiformats').MultihashDigest<typeof code>}
 */
const digest$2 = (payload) => {
  const digest = new Uint8Array(prefix.length + size$2);
  digest.set(prefix, 0);
  digest.set(sha256$3(payload), prefix.length);

  return new Digest$4(digest);
};

/**
 * @type {API.MulticodecCode<typeof SHA256.code, typeof SHA256.name>}
 */
const code$6 = code$7;

var SHA256 = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  code: code$6,
  digest: digest$2,
  name: name$5,
  size: size$2,
});

/**
 * @param {Uint8Array} payload
 * @param {object} [options]
 * @param {API.SyncMultihashHasher<API.SHA256_CODE>} [options.hasher]
 * @returns {API.MerkleTreeNode}
 */
function truncatedHash(payload, options = {}) {
  const hasher = options.hasher || SHA256;
  const { digest } = hasher.digest(payload);
  return truncate(digest);
}

/**
 * @param {API.MerkleTreeNode} left
 * @param {API.MerkleTreeNode} right
 * @param {object} [options]
 * @param {API.SyncMultihashHasher<API.SHA256_CODE>} [options.hasher]
 * @returns {API.MerkleTreeNode}
 */
const computeNode = (left, right, options) => {
  const payload = new Uint8Array(left.length + right.length);
  payload.set(left, 0);
  payload.set(right, left.length);
  return truncatedHash(payload, options);
};

/**
 * @param {API.MerkleTreeNode} node
 * @returns {API.MerkleTreeNode}
 */
function truncate(node) {
  node[NODE_SIZE - 1] &= 0b00111111;
  return node;
}

const MAX_LEVEL = 64;

/**
 * This is a lazy zero-comm buffer which we fill up on demand.
 */
class ZeroComm {
  constructor() {
    this.bytes = new Uint8Array(MAX_LEVEL * NODE_SIZE);
    this.bytes.set(empty$3(), 0);
    /** @private */
    this.node = empty$3();
    /** @private */
    this.length = NODE_SIZE;
  }
  /**
   * @param {number} start
   * @param {number} end
   */
  slice(start, end) {
    while (this.length < end) {
      this.node = computeNode(this.node, this.node);
      this.bytes.set(this.node, this.length);
      this.length += NODE_SIZE;
    }

    return this.bytes.subarray(start, end);
  }
}
const ZERO_COMM = new ZeroComm();

/**
 * simple access by level, only levels between `0` and `64` inclusive are
 * available otherwise throws an error.
 *
 * @param {number} level
 * @returns {API.MerkleTreeNode}
 */
const fromLevel = (level) => {
  if (level < 0 || level >= MAX_LEVEL) {
    throw new Error(
      `Only levels between 0 and ${MAX_LEVEL - 1} inclusive are available`
    );
  }

  return ZERO_COMM.slice(NODE_SIZE * level, NODE_SIZE * (level + 1));
};

/**
 * @param {Uint8Array} source
 * @returns {API.MerkleTreeNode[]}
 */
const split$1 = (source) => {
  const count = source.length / NODE_SIZE;
  const chunks = new Array(count);
  for (let n = 0; n < count; n++) {
    const offset = n * NODE_SIZE;
    const chunk = source.subarray(offset, offset + NODE_SIZE);
    chunks[n] = chunk;
  }
  return chunks;
};

/**
 * Determine the additional bytes of zeroed padding to append to the
 * end of a resource of `size` length in order to fit within a pow2 piece while
 * leaving enough room for Fr32 padding (2 bits per 254).
 *
 * @param {number} payloadSize - The size of the payload.
 * @returns {number}
 */
function toZeroPaddedSize(payloadSize) {
  const size = Math.max(payloadSize, MIN_PAYLOAD_SIZE);
  const highestBit = Math.floor(Math.log2(size));

  const bound = Math.ceil(FR_RATIO * 2 ** (highestBit + 1));
  // the size is either the closest pow2 number, or the next pow2 number if we
  // don't have space for padding
  return size <= bound ? bound : Math.ceil(FR_RATIO * 2 ** (highestBit + 2));
}

/**
 * Derives fr32 padded size from the source content size (that MUST be
 * multiples of {@link IN_BYTES_PER_QUAD}) in bytes.
 *
 * @param {number} size
 */
const toPieceSize = (size) => toZeroPaddedSize(size) / FR_RATIO;

/**
 * Takes source bytes that returns fr32 padded bytes.
 *
 * @param {Uint8Array} source
 * @param {Uint8Array} output
 * @returns {API.Fr23Padded}
 */
const pad = (source, output = new Uint8Array(toPieceSize(source.length))) => {
  const size = toZeroPaddedSize(source.byteLength);
  // Calculate number of quads in the given source
  const quadCount = size / IN_BYTES_PER_QUAD;

  // Cycle over four(4) 31-byte groups, leaving 1 byte in between:
  // 31 + 1 + 31 + 1 + 31 + 1 + 31 = 127
  for (let n = 0; n < quadCount; n++) {
    const readOffset = n * IN_BYTES_PER_QUAD;
    const writeOffset = n * OUT_BYTES_PER_QUAD;

    // First 31 bytes + 6 bits are taken as-is (trimmed later)
    output.set(source.subarray(readOffset, readOffset + 32), writeOffset);

    // first 2-bit "shim" forced into the otherwise identical output
    output[writeOffset + 31] &= 0b00111111;

    // copy next Fr32 preceded with the last two bits of the previous Fr32
    for (let i = 32; i < 64; i++) {
      output[writeOffset + i] =
        (source[readOffset + i] << 2) | (source[readOffset + i - 1] >> 6);
    }

    // next 2-bit shim
    output[writeOffset + 63] &= 0b00111111;

    for (let i = 64; i < 96; i++) {
      output[writeOffset + i] =
        (source[readOffset + i] << 4) | (source[readOffset + i - 1] >> 4);
    }

    // next 2-bit shim
    output[writeOffset + 95] &= 0b00111111;

    for (let i = 96; i < 127; i++) {
      output[writeOffset + i] =
        (source[readOffset + i] << 6) | (source[readOffset + i - 1] >> 2);
    }

    // we shim last 2-bits by shifting the last byte by two bits
    output[writeOffset + 127] = source[readOffset + 126] >> 2;
  }

  return output;
};

/**
 * Returns the base 2 logarithm of the given `n`, rounded down.
 *
 * @param {API.uint64} n
 * @returns {number}
 */
const log2Floor = (n) => {
  let result = 0n;
  while ((n >>= 1n)) result++;
  return Number(result);
};

/**
 * Return the integer logarithm with ceiling for 64 bit unsigned ints.
 *
 * @param {API.uint64} n
 */
const log2Ceil = (n) => (n <= 1n ? 0 : log2Floor(BigInt(n) - 1n) + 1);

/**
 * Takes arbitrary payload size and calculates 0-padding required to
 * produce a {@link API.PaddedSize}.
 *
 * @param {API.uint64} size
 */
const toPadding = (size) => toPadded(size) - size;

/**
 * Takes arbitrary payload size and calculates size after required 0-padding.
 *
 * @param {API.uint64} size
 * @returns {API.PaddedSize}
 */
const toPadded = (size) => toQauds(size) * PADDED_BYTES_PER_QUAD;

/**
 * Takes arbitrary payload size and calculates number of quads that will be
 * required to represent it.
 *
 * @param {API.uint64} size
 */
const toQauds = (size) => {
  // Number of quads required to fit given payload size.
  // Since bigint division truncates we add another quads shy of 1 number of
  // bytes to round up.
  const quadCount = (size + PADDED_BYTES_PER_QUAD - 1n) / PADDED_BYTES_PER_QUAD;
  // Next we we log2 then pow2 with some rounding to ensure that result
  // is 2 ^ n.
  return 2n ** BigInt(log2Ceil(quadCount));
};

/**
 * Calculates the {@link API.PieceSize} for the given height of the piece tree.
 *
 * @param {number} height
 * @returns {API.PieceSize}
 */
const fromHeight = (height) => fromWidth(2n ** BigInt(height));

/**
 * Takes piece tree width (leaf count) and returns corresponding
 * {@link API.PieceSize}.
 *
 * @param {API.uint64} width
 * @returns {API.PieceSize}
 */
const fromWidth = (width) => width * EXPANDED_BYTES_PER_NODE;

/**
 * @see https://github.com/multiformats/multicodec/pull/331/files
 */
const name$4 = /** @type {const} */ (
  "fr32-sha2-256-trunc254-padded-binary-tree"
);

/**
 * @type {API.MulticodecCode<0x1011, typeof name>}
 * @see https://github.com/multiformats/multicodec/pull/331/files
 */
const code$5 = 0x1011;

/**
 * Varint is used to encode the tree height which is limited to 9 bytes.
 *
 * @see https://github.com/multiformats/unsigned-varint#practical-maximum-of-9-bytes-for-security
 */
const MAX_PADDING_SIZE = 9;
/**
 * One byte is used to store the tree height.
 */
const HEIGHT_SIZE = 1;

/**
 * Amount of bytes used to store the tree root.
 */
const ROOT_SIZE = size$2;

/**
 * Size of the multihash digest in bytes.
 */
const MAX_DIGEST_SIZE = MAX_PADDING_SIZE + HEIGHT_SIZE + size$2;

const TAG_SIZE = encodingLength$3(code$5);

/**
 * Max size of the multihash in bytes
 */
const MAX_SIZE = TAG_SIZE + encodingLength$3(MAX_DIGEST_SIZE) + MAX_DIGEST_SIZE;

/**
 * Since first byte in the digest is the tree height, the maximum height is 255.
 *
 * @type {255}
 */
const MAX_HEIGHT$1 = 255;

/**
 * Max payload is determined by the maximum height of the tree, which is limited
 * by the int we could store in one byte. We calculate the max piece size
 * and derive max payload size that can would produce it after FR32 padding.
 */
const MAX_PAYLOAD_SIZE$1 =
  (fromHeight(MAX_HEIGHT$1) * BigInt(IN_BITS_FR)) / BigInt(OUT_BITS_FR);

/**
 * @param {API.Piece} piece
 * @returns {API.PieceDigest}
 */
const fromPiece = ({ padding, height, root }) => {
  const paddingLength = encodingLength$3(Number(padding));
  const size = paddingLength + HEIGHT_SIZE + ROOT_SIZE;
  const sizeLength = encodingLength$3(size);

  const multihashLength = TAG_SIZE + sizeLength + size;

  let offset = 0;
  const bytes = new Uint8Array(multihashLength);
  encodeTo$3(code$5, bytes, offset);
  offset += TAG_SIZE;

  encodeTo$3(size, bytes, offset);
  offset += sizeLength;

  encodeTo$3(Number(padding), bytes, offset);
  offset += paddingLength;

  bytes[offset] = height;
  offset += HEIGHT_SIZE;

  bytes.set(root, offset);

  return new Digest$3(bytes);
};

/**
 * @param {Uint8Array} bytes
 * @returns {API.PieceDigest}
 */
const fromBytes$3 = (bytes) => new Digest$3(bytes);

/**
 * @param {object} input
 * @param {Uint8Array} input.digest
 */
const toBytes$2 = ({ digest }) => {
  const SIZE_BYTE_LENGTH = encodingLength$3(digest.length);

  // number of bytes prefix will take up
  const prefixByteLength = SIZE_BYTE_LENGTH + TAG_SIZE;

  // if digest is view within a buffer that has enough bytes in front to
  // fit the prefix it may be already include a prefix in which case we
  // will simply use a larger slice.
  if (digest.byteOffset >= prefixByteLength) {
    const bytes = new Uint8Array(
      digest.buffer,
      digest.byteOffset - prefixByteLength,
      digest.byteOffset + digest.length
    );

    // if the prefix matches our bytes represent a multihash
    const [tag, offset] = decode$A(bytes);
    if (tag === code$5 && decode$A(bytes, offset)[0] === digest.length) {
      return bytes;
    }
  }

  const bytes = new Uint8Array(digest.length + prefixByteLength);
  encodeTo$3(code$5, bytes);
  encodeTo$3(digest.length, bytes, TAG_SIZE);
  bytes.set(digest, prefixByteLength);

  return bytes;
};

/**
 * @param {object} input
 * @param {Uint8Array} input.digest
 */
const height = ({ digest }) => {
  const [, offset] = decode$A(digest);
  return digest[offset];
};

/**
 * @param {object} input
 * @param {Uint8Array} input.digest
 */
const padding = ({ digest }) => {
  const [padding] = decode$A(digest);
  return BigInt(padding);
};

/**
 * @param {object} input
 * @param {Uint8Array} input.digest
 */
const root = ({ digest }) => {
  const [, offset] = decode$A(digest);
  return digest.subarray(offset + HEIGHT_SIZE, offset + HEIGHT_SIZE + size$2);
};

/**
 * @implements {API.PieceDigest}
 */
let Digest$3 = class Digest {
  /**
   * @param {Uint8Array} bytes
   */
  constructor(bytes) {
    this.bytes = bytes;
    const [tag] = decode$A(bytes);
    if (tag !== code$5) {
      throw new RangeError(`Expected multihash with code ${code$5}`);
    }

    let offset = TAG_SIZE;
    const [size, length] = decode$A(bytes, offset);
    offset += length;
    const digest = bytes.subarray(offset);

    if (digest.length !== size) {
      throw new RangeError(
        `Invalid multihash size expected ${offset + size} bytes, got ${
          bytes.length
        } bytes`
      );
    }

    this.digest = digest;
  }
  get name() {
    return name$4;
  }
  get code() {
    return code$5;
  }
  get size() {
    return this.digest.length;
  }
  get padding() {
    return padding(this);
  }
  get height() {
    return height(this);
  }
  get root() {
    return root(this);
  }
};

var digest$1 = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  HEIGHT_SIZE: HEIGHT_SIZE,
  MAX_DIGEST_SIZE: MAX_DIGEST_SIZE,
  MAX_HEIGHT: MAX_HEIGHT$1,
  MAX_PAYLOAD_SIZE: MAX_PAYLOAD_SIZE$1,
  MAX_SIZE: MAX_SIZE,
  ROOT_SIZE: ROOT_SIZE,
  TAG_SIZE: TAG_SIZE,
  code: code$5,
  fromBytes: fromBytes$3,
  fromPiece: fromPiece,
  height: height,
  name: name$4,
  padding: padding,
  root: root,
  toBytes: toBytes$2,
});

/**
 * @see https://github.com/multiformats/multicodec/pull/331/files
 */
const name$3 = /** @type {const} */ (
  "fr32-sha2-256-trunc254-padded-binary-tree"
);

/**
 * @type {API.MulticodecCode<0x1011, typeof name>}
 * @see https://github.com/multiformats/multicodec/pull/331/files
 */
const code$4 = 0x1011;

/**
 * Since first byte in the digest is the tree height, the maximum height is 255.
 *
 * @type {255}
 */
const MAX_HEIGHT = 255;

/**
 * Max payload is determined by the maximum height of the tree, which is limited
 * by the int we could store in one byte. We calculate the max piece size
 * and derive max payload size that can would produce it after FR32 padding.
 */
const MAX_PAYLOAD_SIZE =
  (fromHeight(MAX_HEIGHT) * BigInt(IN_BITS_FR)) / BigInt(OUT_BITS_FR);

/**
 * Computes the digest of the given payload.
 *
 * @param {Uint8Array} payload
 * @returns {API.PieceDigest}
 */
const digest = (payload) => {
  const hasher = new Hasher$3();
  hasher.write(payload);
  return hasher.digest();
};

/**
 * Creates a streaming hasher that can be used to consumer larger streams
 * of data than it would be practical to load into memory all at once.
 *
 * @returns {API.StreamingHasher<typeof code, number, API.PieceDigest>}
 */
const create$b = () => new Hasher$3();

/**
 * @typedef {[API.MerkleTreeNode[], ...API.MerkleTreeNode[][]]} Layers
 *
 * @implements {API.StreamingHasher<typeof code, number, API.PieceDigest>}
 */
let Hasher$3 = class Hasher {
  constructor() {
    /**
     * The number of bytes consumed by the hasher.
     *
     * @private
     */
    this.bytesWritten = 0n;

    /**
     * This buffer is used to accumulate bytes until we have enough to fill a
     * quad.
     *
     * ⚠️ Note that you should never read bytes past {@link offset} as those
     * are considered dirty and may contain garbage.
     *
     * @protected
     */
    this.buffer = new Uint8Array(IN_BYTES_PER_QUAD);

    /**
     * Offset is the number of bytes in we have written into the buffer. If
     * offset is 0 it means that the buffer is effectively empty. When `offset`
     * is equal to `this.buffer.length` we have a quad that can be processed.
     *
     * @protected
     */
    this.offset = 0;

    /**
     * The layers of the tree. Each layer will contain either 0 or 1 nodes
     * between writes. When we write into a hasher, if we have enough nodes
     * leaves will be created and pushed into the `layers[0]` array, after
     * which we flush and combine every two leafs into a node which is moved
     * to the next layer. This process is repeated until we reach the top
     * layer, leaving each layer either empty or with a single node.
     *
     * @type {Layers}
     */
    this.layers = [[]];
  }

  /**
   * Return the total number of bytes written into the hasher. Calling
   * {@link reset} will reset the hasher and the count will be reset to 0.
   *
   * @returns {bigint}
   */
  count() {
    return this.bytesWritten;
  }

  /**
   * Computes the digest of all the data that has been written into this hasher.
   * This method does not have side-effects, meaning that you can continue
   * writing and call this method again to compute digest of all the data
   * written from the very beginning.
   */
  digest() {
    const bytes = new Uint8Array(MAX_SIZE);
    const count = this.digestInto(bytes, 0, true);
    return fromBytes$3(bytes.subarray(0, count));
  }

  /**
   * Computes the digest and writes into the given buffer. You can provide
   * optional `byteOffset` to write digest at that offset in the buffer. By
   * default the multihash prefix will be written into the buffer, but you can
   * opt-out by passing `false` as the `asMultihash` argument.
   *
   * @param {Uint8Array} output
   * @param {number} [byteOffset]
   * @param {boolean} asMultihash
   */
  digestInto(output, byteOffset = 0, asMultihash = true) {
    const { buffer, layers, offset, bytesWritten } = this;

    // We do not want to mutate the layers, so we create a shallow copy of it
    // which we will use to compute the root.
    let [leaves, ...nodes] = layers;

    // If we have some bytes in the buffer we fill rest with zeros and compute
    // leaves from it. Note that it is safe to mutate the buffer here as bytes
    // past `offset` are considered dirty and should not be read.
    if (offset > 0 || bytesWritten === 0n) {
      leaves = [...leaves, ...split$1(pad(buffer.fill(0, offset)))];
    }

    const tree = build([leaves, ...nodes]);
    const height = tree.length - 1;
    const [root] = tree[height];
    const padding = Number(toPadding(this.bytesWritten));

    const paddingLength = encodingLength$3(
      /** @type {number & bigint} */ (padding)
    );

    let endOffset = byteOffset;
    // Write the multihash prefix if requested
    if (asMultihash) {
      encodeTo$3(code$4, output, endOffset);
      endOffset += TAG_SIZE;

      const size = paddingLength + HEIGHT_SIZE + ROOT_SIZE;
      const sizeLength = encodingLength$3(size);
      encodeTo$3(size, output, endOffset);
      endOffset += sizeLength;
    }

    encodeTo$3(padding, output, endOffset);
    endOffset += paddingLength;

    // Write the tree height as the first byte of the digest
    output[endOffset] = height;
    endOffset += 1;

    // Write the root as the remaining 32 bytes of the digest
    output.set(root, endOffset);
    endOffset += root.length;

    // Return number of bytes written
    return endOffset - byteOffset;
  }
  /**
   * @param {Uint8Array} bytes
   */
  write(bytes) {
    const { buffer, offset, layers } = this;
    const leaves = layers[0];
    const { length } = bytes;
    // If we got no bytes there is nothing to do here
    if (length === 0) {
      return this;
      /* c8 ignore next 5 */
    } else if (this.bytesWritten + BigInt(length) > MAX_PAYLOAD_SIZE) {
      throw new RangeError(
        `Writing ${length} bytes exceeds max payload size of ${MAX_PAYLOAD_SIZE}`
      );
    }
    // If we do not have enough bytes to form a quad, just add append new bytes
    // to the buffer and return.
    else if (offset + length < buffer.length) {
      buffer.set(bytes, offset);
      this.offset += length;
      this.bytesWritten += BigInt(length);
      return this;
    }
    // Otherwise we first fill the buffer to form a quad and create some leaves.
    // Then we slice remaining bytes into quads sized chunks and create leaves
    // from them. If we have some bytes left we copy them into the buffer and
    // flush to combining node pairs and propagate them up the tree.
    else {
      // Number of bytes required to fill the quad buffer
      const bytesRequired = buffer.length - offset;
      // copy required bytes into the buffer and turn them into leaves
      // which we push into the leaf layer.
      buffer.set(bytes.subarray(0, bytesRequired), offset);
      leaves.push(...split$1(pad(buffer)));

      // Now we slice remaining bytes into quads, create leaves from them
      // and push them into the leaf layer.
      let readOffset = bytesRequired;
      while (readOffset + IN_BYTES_PER_QUAD < length) {
        const quad = bytes.subarray(readOffset, readOffset + IN_BYTES_PER_QUAD);
        leaves.push(...split$1(pad(quad)));
        readOffset += IN_BYTES_PER_QUAD;
      }

      // Whatever byte were left are copied into the buffer and we update
      // the offset to reflect that.
      this.buffer.set(bytes.subarray(readOffset), 0);
      this.offset = length - readOffset;

      // We also update the total number of bytes written.
      this.bytesWritten += BigInt(length);

      // Now prune the layers to propagate all the new leaves up the tree.
      prune(this.layers);

      return this;
    }
  }

  /**
   * Resets this hasher to its initial state so it could be recycled as new
   * instance.
   */
  reset() {
    this.offset = 0;
    this.bytesWritten = 0n;
    this.layers.length = 1;
    this.layers[0].length = 0;
    return this;
  }

  /* c8 ignore next 3 */
  dispose() {
    this.reset();
  }
  get code() {
    return code$4;
  }
  get name() {
    return name$3;
  }
};

/**
 * Prunes layers by combining node pairs into nodes in the next layer and
 * removing them from the layer that they were in. After pruning each layer
 * will end up with at most one node. New layers may be created in the process
 * when nodes from the top layer are combined.
 *
 * @param {Layers} layers
 */
const prune = (layers) => flush$1(layers, false);

/**
 * Flushes all the nodes in layers by combining node pairs into nodes in the
 * next layer. Layers with only one node are combined with zero padded nodes
 * (corresponding to the level of the layer). Unlike {@link prune} combined
 * nodes are not removed and layers are copied instead of been mutated.
 *
 * @param {Layers} layers
 */
const build = (layers) => flush$1([...layers], true);

/**
 * @param {Layers} layers
 * @param {boolean} build
 * @returns {Layers}
 */
const flush$1 = (layers, build) => {
  // Note it is important that we do not mutate any of the layers otherwise
  // writing more data into the hasher and computing the digest will produce
  // wrong results.
  let level = 0;
  // We will walk up the tree until we reach the top layer. However, we may end
  // up with creating new layers in the process, so we will keep track of the
  while (level < layers.length) {
    let next = layers[level + 1];
    const layer = layers[level];

    // If we have the odd number of nodes and we have not reached the top
    // layer, we push a zero padding node corresponding to the current level.
    if (build && layer.length % 2 > 0 && next) {
      layer.push(fromLevel(level));
    }

    level += 1;

    // If we have 0 nodes in the current layer we just move to the next one.

    // If we have a next layer and we are building  will combine nodes from the current layer
    next = next ? (build ? [...next] : next) : [];
    let index = 0;
    // Note that we have checked that we have an even number of nodes so
    // we will never end up with an extra node when consuming two at a time.
    while (index + 1 < layer.length) {
      const node = computeNode(layer[index], layer[index + 1]);

      // we proactively delete nodes in order to free up a memory used.
      delete layer[index];
      delete layer[index + 1];

      next.push(node);
      index += 2;
    }

    if (next.length) {
      layers[level] = next;
    }

    // we remove nodes that we have combined from the current layer to reduce
    // memory overhead and move to the next layer.
    layer.splice(0, index);
  }

  return layers;
};

var PieceHasher = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  Digest: digest$1,
  MAX_HEIGHT: MAX_HEIGHT,
  MAX_PAYLOAD_SIZE: MAX_PAYLOAD_SIZE,
  code: code$4,
  create: create$b,
  digest: digest,
  name: name$3,
});

new TextEncoder();
new TextDecoder();

const contentType$2 = "application/cbor";

const HEADERS = Object.freeze({
  "content-type": contentType$2,
});

/**
 * Encodes `AgentMessage` into a legacy CBOR representation.
 *
 * @template {API.AgentMessage} Message
 * @param {Message} message
 * @param {API.EncodeOptions} [options]
 * @returns {API.HTTPResponse<Message>}
 */
const encode$a = (message, options) => {
  const legacyResults = [];
  for (const receipt of message.receipts.values()) {
    const result = receipt.out;
    if (result.ok) {
      legacyResults.push(result.ok);
    } else {
      legacyResults.push({
        ...result.error,
        error: true,
      });
    }
  }

  /** @type {Uint8Array} */
  const body = encode$i(legacyResults);

  return /** @type {API.HTTPResponse<Message>} */ ({
    headers: HEADERS,
    body,
  });
};

var response = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  contentType: contentType$2,
  encode: encode$a,
});

const contentType$1 = "application/car";

/**
 * @template {API.AgentMessage} Message
 * @param {API.HTTPRequest<Message>} request
 */
const decode$d = async ({ body }) => {
  const { roots, blocks } = decode$o(/** @type {Uint8Array} */ (body));
  /** @type {API.IssuedInvocation[]} */
  const run = [];
  for (const { cid } of roots) {
    // We don't have a way to know if the root matches a ucan link.
    const invocation = view$2({
      root: /** @type {API.Link} */ (cid),
      blocks,
    });
    run.push(invocation);
  }

  const message = await build$1({
    invocations: /** @type {API.Tuple<API.IssuedInvocation>} */ (run),
  });

  return /** @type {Message} */ (message);
};

var request$1 = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  contentType: contentType$1,
  decode: decode$d,
});

const { contentType } = request$1;

/**
 * This is an inbound codec designed to support legacy clients and encode
 * responses in a legacy (CBOR) format.
 */
inbound({
  decoders: {
    [contentType]: request$1,
    [contentType$3]: request$3,
  },
  encoders: {
    // Here we configure encoders such that if accept header is `*/*` (which is
    // the default if omitted) we will encode the response in CBOR. If
    // `application/vnd.ipld.car` is set we will encode the response in current
    // format.
    // Here we exploit the fact that legacy clients do not send an accept header
    // and therefore will get response in legacy format. New clients on the other
    // hand will send `application/vnd.ipld.car` and consequently get response
    // in current format.
    "*/*;q=0.1": response,
    [contentType$3]: response$1,
  },
});

/**
 * @typedef {import('./types.js').SERVICE} Service
 * @typedef {import('./types.js').ServiceConfig} ServiceConfig
 */
/**
 * @type {Record<Service, ServiceConfig>}
 */
const services = {
  STOREFRONT: {
    url: new URL("https://up.web3.storage"),
    principal: parse$1("did:web:web3.storage"),
  },
  AGGREGATOR: {
    url: new URL("https://aggregator.web3.storage"),
    principal: parse$1("did:web:web3.storage"),
  },
  DEALER: {
    url: new URL("https://dealer.web3.storage"),
    principal: parse$1("did:web:web3.storage"),
  },
  DEAL_TRACKER: {
    url: new URL("https://tracker.web3.storage"),
    principal: parse$1("did:web:web3.storage"),
  },
};

/**
 * @typedef {import('./types.js').StorefrontService} StorefrontService
 * @typedef {import('@ucanto/interface').ConnectionView<StorefrontService>} ConnectionView
 */
/** @type {ConnectionView} */
const connection$1 = connect({
  id: services.STOREFRONT.principal,
  codec: outbound,
  channel: open$2({
    url: services.STOREFRONT.url,
    method: "POST",
  }),
});
/**
 * The `filecoin/offer` task can be executed to request storing a content piece
 * in Filecoin. It issues a signed receipt of the execution result.
 *
 * A receipt for successful execution will contain an effect, linking to a
 * `filecoin/submit` task that will complete asynchronously.
 *
 * Otherwise the task is failed and the receipt will contain details of the
 * reason behind the failure.
 *
 * @see https://github.com/storacha/specs/blob/main/w3-filecoin.md#filecoinoffer
 *
 * @param {import('./types.js').InvocationConfig} conf - Configuration
 * @param {import('multiformats').UnknownLink} content
 * @param {import('@web3-storage/data-segment').PieceLink} piece
 * @param {import('./types.js').RequestOptions<StorefrontService>} [options]
 */
async function filecoinOffer(
  { issuer, with: resource, proofs, audience },
  content,
  piece,
  options = {}
) {
  /* c8 ignore next */
  const conn = options.connection ?? connection$1;
  const invocation = filecoinOffer$1.invoke({
    issuer,
    /* c8 ignore next */
    audience: audience ?? services.STOREFRONT.principal,
    with: resource,
    nb: {
      content,
      piece,
    },
    proofs,
    expiration: Infinity,
  });
  return await invocation.execute(conn);
}
/**
 * The `filecoin/info` task can be executed to request info about a content piece
 * in Filecoin. It issues a signed receipt of the execution result.
 *
 * @param {import('./types.js').InvocationConfig} conf - Configuration
 * @param {import('@web3-storage/data-segment').PieceLink} piece
 * @param {import('./types.js').RequestOptions<StorefrontService>} [options]
 */
async function filecoinInfo(
  { issuer, with: resource, proofs, audience },
  piece,
  options = {}
) {
  /* c8 ignore next */
  const conn = options.connection ?? connection$1;
  const invocation = filecoinInfo$1.invoke({
    issuer,
    /* c8 ignore next */
    audience: audience ?? services.STOREFRONT.principal,
    with: resource,
    nb: {
      piece,
    },
    proofs,
  });
  return await invocation.execute(conn);
}

/**
 * @typedef {import('./types.js').AggregatorService} AggregatorService
 * @typedef {import('@ucanto/interface').ConnectionView<AggregatorService>} ConnectionView
 */
/** @type {ConnectionView} */
connect({
  id: services.AGGREGATOR.principal,
  codec: outbound,
  channel: open$2({
    url: services.AGGREGATOR.url,
    method: "POST",
  }),
});

/**
 * @typedef {import('./types.js').DealerService} DealerService
 * @typedef {import('@ucanto/interface').ConnectionView<DealerService>} ConnectionView
 */
/** @type {ConnectionView} */
connect({
  id: services.DEALER.principal,
  codec: outbound,
  channel: open$2({
    url: services.DEALER.url,
    method: "POST",
  }),
});

/**
 * @typedef {import('./types.js').DealTrackerService} DealTrackerService
 * @typedef {import('@ucanto/interface').ConnectionView<DealTrackerService>} ConnectionView
 */
/** @type {ConnectionView} */
connect({
  id: services.DEAL_TRACKER.principal,
  codec: outbound,
  channel: open$2({
    url: services.DEAL_TRACKER.url,
    method: "POST",
  }),
});

// base-x encoding / decoding
// Copyright (c) 2018 base-x contributors
// Copyright (c) 2014-2018 The Bitcoin Core developers (base58.cpp)
// Distributed under the MIT software license, see the accompanying
// file LICENSE or http://www.opensource.org/licenses/mit-license.php.
function base$1(ALPHABET, name) {
  if (ALPHABET.length >= 255) {
    throw new TypeError("Alphabet too long");
  }
  var BASE_MAP = new Uint8Array(256);
  for (var j = 0; j < BASE_MAP.length; j++) {
    BASE_MAP[j] = 255;
  }
  for (var i = 0; i < ALPHABET.length; i++) {
    var x = ALPHABET.charAt(i);
    var xc = x.charCodeAt(0);
    if (BASE_MAP[xc] !== 255) {
      throw new TypeError(x + " is ambiguous");
    }
    BASE_MAP[xc] = i;
  }
  var BASE = ALPHABET.length;
  var LEADER = ALPHABET.charAt(0);
  var FACTOR = Math.log(BASE) / Math.log(256); // log(BASE) / log(256), rounded up
  var iFACTOR = Math.log(256) / Math.log(BASE); // log(256) / log(BASE), rounded up
  function encode(source) {
    if (source instanceof Uint8Array);
    else if (ArrayBuffer.isView(source)) {
      source = new Uint8Array(
        source.buffer,
        source.byteOffset,
        source.byteLength
      );
    } else if (Array.isArray(source)) {
      source = Uint8Array.from(source);
    }
    if (!(source instanceof Uint8Array)) {
      throw new TypeError("Expected Uint8Array");
    }
    if (source.length === 0) {
      return "";
    }
    // Skip & count leading zeroes.
    var zeroes = 0;
    var length = 0;
    var pbegin = 0;
    var pend = source.length;
    while (pbegin !== pend && source[pbegin] === 0) {
      pbegin++;
      zeroes++;
    }
    // Allocate enough space in big-endian base58 representation.
    var size = ((pend - pbegin) * iFACTOR + 1) >>> 0;
    var b58 = new Uint8Array(size);
    // Process the bytes.
    while (pbegin !== pend) {
      var carry = source[pbegin];
      // Apply "b58 = b58 * 256 + ch".
      var i = 0;
      for (
        var it1 = size - 1;
        (carry !== 0 || i < length) && it1 !== -1;
        it1--, i++
      ) {
        carry += (256 * b58[it1]) >>> 0;
        b58[it1] = carry % BASE >>> 0;
        carry = (carry / BASE) >>> 0;
      }
      if (carry !== 0) {
        throw new Error("Non-zero carry");
      }
      length = i;
      pbegin++;
    }
    // Skip leading zeroes in base58 result.
    var it2 = size - length;
    while (it2 !== size && b58[it2] === 0) {
      it2++;
    }
    // Translate the result into a string.
    var str = LEADER.repeat(zeroes);
    for (; it2 < size; ++it2) {
      str += ALPHABET.charAt(b58[it2]);
    }
    return str;
  }
  function decodeUnsafe(source) {
    if (typeof source !== "string") {
      throw new TypeError("Expected String");
    }
    if (source.length === 0) {
      return new Uint8Array();
    }
    var psz = 0;
    // Skip leading spaces.
    if (source[psz] === " ") {
      return;
    }
    // Skip and count leading '1's.
    var zeroes = 0;
    var length = 0;
    while (source[psz] === LEADER) {
      zeroes++;
      psz++;
    }
    // Allocate enough space in big-endian base256 representation.
    var size = ((source.length - psz) * FACTOR + 1) >>> 0; // log(58) / log(256), rounded up.
    var b256 = new Uint8Array(size);
    // Process the characters.
    while (source[psz]) {
      // Decode character
      var carry = BASE_MAP[source.charCodeAt(psz)];
      // Invalid character
      if (carry === 255) {
        return;
      }
      var i = 0;
      for (
        var it3 = size - 1;
        (carry !== 0 || i < length) && it3 !== -1;
        it3--, i++
      ) {
        carry += (BASE * b256[it3]) >>> 0;
        b256[it3] = carry % 256 >>> 0;
        carry = (carry / 256) >>> 0;
      }
      if (carry !== 0) {
        throw new Error("Non-zero carry");
      }
      length = i;
      psz++;
    }
    // Skip trailing spaces.
    if (source[psz] === " ") {
      return;
    }
    // Skip leading zeroes in b256.
    var it4 = size - length;
    while (it4 !== size && b256[it4] === 0) {
      it4++;
    }
    var vch = new Uint8Array(zeroes + (size - it4));
    var j = zeroes;
    while (it4 !== size) {
      vch[j++] = b256[it4++];
    }
    return vch;
  }
  function decode(string) {
    var buffer = decodeUnsafe(string);
    if (buffer) {
      return buffer;
    }
    throw new Error(`Non-${name} character`);
  }
  return {
    encode: encode,
    decodeUnsafe: decodeUnsafe,
    decode: decode,
  };
}
var src$1 = base$1;

var _brrp__multiformats_scope_baseX$1 = src$1;

/**
 * @param {Uint8Array} aa
 * @param {Uint8Array} bb
 */
const equals$3 = (aa, bb) => {
  if (aa === bb) return true;
  if (aa.byteLength !== bb.byteLength) {
    return false;
  }

  for (let ii = 0; ii < aa.byteLength; ii++) {
    if (aa[ii] !== bb[ii]) {
      return false;
    }
  }

  return true;
};

/**
 * @param {ArrayBufferView|ArrayBuffer|Uint8Array} o
 * @returns {Uint8Array}
 */
const coerce$1 = (o) => {
  if (o instanceof Uint8Array && o.constructor.name === "Uint8Array") return o;
  if (o instanceof ArrayBuffer) return new Uint8Array(o);
  if (ArrayBuffer.isView(o)) {
    return new Uint8Array(o.buffer, o.byteOffset, o.byteLength);
  }
  throw new Error("Unknown type, must be binary type");
};

/**
 * Class represents both BaseEncoder and MultibaseEncoder meaning it
 * can be used to encode to multibase or base encode without multibase
 * prefix.
 *
 * @class
 * @template {string} Base
 * @template {string} Prefix
 * @implements {API.MultibaseEncoder<Prefix>}
 * @implements {API.BaseEncoder}
 */
let Encoder$1 = class Encoder {
  /**
   * @param {Base} name
   * @param {Prefix} prefix
   * @param {(bytes:Uint8Array) => string} baseEncode
   */
  constructor(name, prefix, baseEncode) {
    this.name = name;
    this.prefix = prefix;
    this.baseEncode = baseEncode;
  }

  /**
   * @param {Uint8Array} bytes
   * @returns {API.Multibase<Prefix>}
   */
  encode(bytes) {
    if (bytes instanceof Uint8Array) {
      return `${this.prefix}${this.baseEncode(bytes)}`;
    } else {
      throw Error("Unknown type, must be binary type");
    }
  }
};

/**
 * @template {string} Prefix
 */
/**
 * Class represents both BaseDecoder and MultibaseDecoder so it could be used
 * to decode multibases (with matching prefix) or just base decode strings
 * with corresponding base encoding.
 *
 * @class
 * @template {string} Base
 * @template {string} Prefix
 * @implements {API.MultibaseDecoder<Prefix>}
 * @implements {API.UnibaseDecoder<Prefix>}
 * @implements {API.BaseDecoder}
 */
let Decoder$1 = class Decoder {
  /**
   * @param {Base} name
   * @param {Prefix} prefix
   * @param {(text:string) => Uint8Array} baseDecode
   */
  constructor(name, prefix, baseDecode) {
    this.name = name;
    this.prefix = prefix;
    /* c8 ignore next 3 */
    if (prefix.codePointAt(0) === undefined) {
      throw new Error("Invalid prefix character");
    }
    /** @private */
    this.prefixCodePoint = /** @type {number} */ (prefix.codePointAt(0));
    this.baseDecode = baseDecode;
  }

  /**
   * @param {string} text
   */
  decode(text) {
    if (typeof text === "string") {
      if (text.codePointAt(0) !== this.prefixCodePoint) {
        throw Error(
          `Unable to decode multibase string ${JSON.stringify(text)}, ${
            this.name
          } decoder only supports inputs prefixed with ${this.prefix}`
        );
      }
      return this.baseDecode(text.slice(this.prefix.length));
    } else {
      throw Error("Can only multibase decode strings");
    }
  }

  /**
   * @template {string} OtherPrefix
   * @param {API.UnibaseDecoder<OtherPrefix>|ComposedDecoder<OtherPrefix>} decoder
   * @returns {ComposedDecoder<Prefix|OtherPrefix>}
   */
  or(decoder) {
    return or$3(this, decoder);
  }
};

/**
 * @template {string} Prefix
 * @typedef {Record<Prefix, API.UnibaseDecoder<Prefix>>} Decoders
 */

/**
 * @template {string} Prefix
 * @implements {API.MultibaseDecoder<Prefix>}
 * @implements {API.CombobaseDecoder<Prefix>}
 */
let ComposedDecoder$1 = class ComposedDecoder {
  /**
   * @param {Decoders<Prefix>} decoders
   */
  constructor(decoders) {
    this.decoders = decoders;
  }

  /**
   * @template {string} OtherPrefix
   * @param {API.UnibaseDecoder<OtherPrefix>|ComposedDecoder<OtherPrefix>} decoder
   * @returns {ComposedDecoder<Prefix|OtherPrefix>}
   */
  or(decoder) {
    return or$3(this, decoder);
  }

  /**
   * @param {string} input
   * @returns {Uint8Array}
   */
  decode(input) {
    const prefix = /** @type {Prefix} */ (input[0]);
    const decoder = this.decoders[prefix];
    if (decoder) {
      return decoder.decode(input);
    } else {
      throw RangeError(
        `Unable to decode multibase string ${JSON.stringify(
          input
        )}, only inputs prefixed with ${Object.keys(
          this.decoders
        )} are supported`
      );
    }
  }
};

/**
 * @template {string} L
 * @template {string} R
 * @param {API.UnibaseDecoder<L>|API.CombobaseDecoder<L>} left
 * @param {API.UnibaseDecoder<R>|API.CombobaseDecoder<R>} right
 * @returns {ComposedDecoder<L|R>}
 */
const or$3 = (left, right) =>
  new ComposedDecoder$1(
    /** @type {Decoders<L|R>} */ ({
      ...(left.decoders || {
        [/** @type API.UnibaseDecoder<L> */ (left).prefix]: left,
      }),
      ...(right.decoders || {
        [/** @type API.UnibaseDecoder<R> */ (right).prefix]: right,
      }),
    })
  );

/**
 * @class
 * @template {string} Base
 * @template {string} Prefix
 * @implements {API.MultibaseCodec<Prefix>}
 * @implements {API.MultibaseEncoder<Prefix>}
 * @implements {API.MultibaseDecoder<Prefix>}
 * @implements {API.BaseCodec}
 * @implements {API.BaseEncoder}
 * @implements {API.BaseDecoder}
 */
let Codec$1 = class Codec {
  /**
   * @param {Base} name
   * @param {Prefix} prefix
   * @param {(bytes:Uint8Array) => string} baseEncode
   * @param {(text:string) => Uint8Array} baseDecode
   */
  constructor(name, prefix, baseEncode, baseDecode) {
    this.name = name;
    this.prefix = prefix;
    this.baseEncode = baseEncode;
    this.baseDecode = baseDecode;
    this.encoder = new Encoder$1(name, prefix, baseEncode);
    this.decoder = new Decoder$1(name, prefix, baseDecode);
  }

  /**
   * @param {Uint8Array} input
   */
  encode(input) {
    return this.encoder.encode(input);
  }

  /**
   * @param {string} input
   */
  decode(input) {
    return this.decoder.decode(input);
  }
};

/**
 * @template {string} Base
 * @template {string} Prefix
 * @param {object} options
 * @param {Base} options.name
 * @param {Prefix} options.prefix
 * @param {(bytes:Uint8Array) => string} options.encode
 * @param {(input:string) => Uint8Array} options.decode
 * @returns {Codec<Base, Prefix>}
 */
const from$8 = ({ name, prefix, encode, decode }) =>
  new Codec$1(name, prefix, encode, decode);

/**
 * @template {string} Base
 * @template {string} Prefix
 * @param {object} options
 * @param {Base} options.name
 * @param {Prefix} options.prefix
 * @param {string} options.alphabet
 * @returns {Codec<Base, Prefix>}
 */
const baseX$1 = ({ prefix, name, alphabet }) => {
  const { encode, decode } = _brrp__multiformats_scope_baseX$1(alphabet, name);
  return from$8({
    prefix,
    name,
    encode,
    /**
     * @param {string} text
     */
    decode: (text) => coerce$1(decode(text)),
  });
};

/**
 * @param {string} string
 * @param {string} alphabet
 * @param {number} bitsPerChar
 * @param {string} name
 * @returns {Uint8Array}
 */
const decode$c = (string, alphabet, bitsPerChar, name) => {
  // Build the character lookup table:
  /** @type {Record<string, number>} */
  const codes = {};
  for (let i = 0; i < alphabet.length; ++i) {
    codes[alphabet[i]] = i;
  }

  // Count the padding bytes:
  let end = string.length;
  while (string[end - 1] === "=") {
    --end;
  }

  // Allocate the output:
  const out = new Uint8Array(((end * bitsPerChar) / 8) | 0);

  // Parse the data:
  let bits = 0; // Number of bits currently in the buffer
  let buffer = 0; // Bits waiting to be written out, MSB first
  let written = 0; // Next byte to write
  for (let i = 0; i < end; ++i) {
    // Read one character from the string:
    const value = codes[string[i]];
    if (value === undefined) {
      throw new SyntaxError(`Non-${name} character`);
    }

    // Append the bits to the buffer:
    buffer = (buffer << bitsPerChar) | value;
    bits += bitsPerChar;

    // Write out some bits if the buffer has a byte's worth:
    if (bits >= 8) {
      bits -= 8;
      out[written++] = 0xff & (buffer >> bits);
    }
  }

  // Verify that we have received just enough bits:
  if (bits >= bitsPerChar || 0xff & (buffer << (8 - bits))) {
    throw new SyntaxError("Unexpected end of data");
  }

  return out;
};

/**
 * @param {Uint8Array} data
 * @param {string} alphabet
 * @param {number} bitsPerChar
 * @returns {string}
 */
const encode$9 = (data, alphabet, bitsPerChar) => {
  const pad = alphabet[alphabet.length - 1] === "=";
  const mask = (1 << bitsPerChar) - 1;
  let out = "";

  let bits = 0; // Number of bits currently in the buffer
  let buffer = 0; // Bits waiting to be written out, MSB first
  for (let i = 0; i < data.length; ++i) {
    // Slurp data into the buffer:
    buffer = (buffer << 8) | data[i];
    bits += 8;

    // Write out as much as we can:
    while (bits > bitsPerChar) {
      bits -= bitsPerChar;
      out += alphabet[mask & (buffer >> bits)];
    }
  }

  // Partial character:
  if (bits) {
    out += alphabet[mask & (buffer << (bitsPerChar - bits))];
  }

  // Add padding characters until we hit a byte boundary:
  if (pad) {
    while ((out.length * bitsPerChar) & 7) {
      out += "=";
    }
  }

  return out;
};

/**
 * RFC4648 Factory
 *
 * @template {string} Base
 * @template {string} Prefix
 * @param {object} options
 * @param {Base} options.name
 * @param {Prefix} options.prefix
 * @param {string} options.alphabet
 * @param {number} options.bitsPerChar
 */
const rfc4648$1 = ({ name, prefix, bitsPerChar, alphabet }) => {
  return from$8({
    prefix,
    name,
    encode(input) {
      return encode$9(input, alphabet, bitsPerChar);
    },
    decode(input) {
      return decode$c(input, alphabet, bitsPerChar, name);
    },
  });
};

const base32$1 = rfc4648$1({
  prefix: "b",
  name: "base32",
  alphabet: "abcdefghijklmnopqrstuvwxyz234567",
  bitsPerChar: 5,
});

rfc4648$1({
  prefix: "B",
  name: "base32upper",
  alphabet: "ABCDEFGHIJKLMNOPQRSTUVWXYZ234567",
  bitsPerChar: 5,
});

rfc4648$1({
  prefix: "c",
  name: "base32pad",
  alphabet: "abcdefghijklmnopqrstuvwxyz234567=",
  bitsPerChar: 5,
});

rfc4648$1({
  prefix: "C",
  name: "base32padupper",
  alphabet: "ABCDEFGHIJKLMNOPQRSTUVWXYZ234567=",
  bitsPerChar: 5,
});

rfc4648$1({
  prefix: "v",
  name: "base32hex",
  alphabet: "0123456789abcdefghijklmnopqrstuv",
  bitsPerChar: 5,
});

rfc4648$1({
  prefix: "V",
  name: "base32hexupper",
  alphabet: "0123456789ABCDEFGHIJKLMNOPQRSTUV",
  bitsPerChar: 5,
});

rfc4648$1({
  prefix: "t",
  name: "base32hexpad",
  alphabet: "0123456789abcdefghijklmnopqrstuv=",
  bitsPerChar: 5,
});

rfc4648$1({
  prefix: "T",
  name: "base32hexpadupper",
  alphabet: "0123456789ABCDEFGHIJKLMNOPQRSTUV=",
  bitsPerChar: 5,
});

rfc4648$1({
  prefix: "h",
  name: "base32z",
  alphabet: "ybndrfg8ejkmcpqxot1uwisza345h769",
  bitsPerChar: 5,
});

const base58btc$1 = baseX$1({
  name: "base58btc",
  prefix: "z",
  alphabet: "123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz",
});

baseX$1({
  name: "base58flickr",
  prefix: "Z",
  alphabet: "123456789abcdefghijkmnopqrstuvwxyzABCDEFGHJKLMNPQRSTUVWXYZ",
});

var encode_1$2 = encode$8;

var MSB$4 = 0x80,
  MSBALL$3 = -128,
  INT$3 = Math.pow(2, 31);

function encode$8(num, out, offset) {
  out = out || [];
  offset = offset || 0;
  var oldOffset = offset;

  while (num >= INT$3) {
    out[offset++] = (num & 0xff) | MSB$4;
    num /= 128;
  }
  while (num & MSBALL$3) {
    out[offset++] = (num & 0xff) | MSB$4;
    num >>>= 7;
  }
  out[offset] = num | 0;

  encode$8.bytes = offset - oldOffset + 1;

  return out;
}

var decode$b = read$3;

var MSB$1$2 = 0x80,
  REST$1$2 = 0x7f;

function read$3(buf, offset) {
  var res = 0,
    offset = offset || 0,
    shift = 0,
    counter = offset,
    b,
    l = buf.length;

  do {
    if (counter >= l) {
      read$3.bytes = 0;
      throw new RangeError("Could not decode varint");
    }
    b = buf[counter++];
    res +=
      shift < 28
        ? (b & REST$1$2) << shift
        : (b & REST$1$2) * Math.pow(2, shift);
    shift += 7;
  } while (b >= MSB$1$2);

  read$3.bytes = counter - offset;

  return res;
}

var N1$2 = Math.pow(2, 7);
var N2$2 = Math.pow(2, 14);
var N3$2 = Math.pow(2, 21);
var N4$2 = Math.pow(2, 28);
var N5$2 = Math.pow(2, 35);
var N6$2 = Math.pow(2, 42);
var N7$2 = Math.pow(2, 49);
var N8$2 = Math.pow(2, 56);
var N9$2 = Math.pow(2, 63);

var length$2 = function (value) {
  return value < N1$2
    ? 1
    : value < N2$2
    ? 2
    : value < N3$2
    ? 3
    : value < N4$2
    ? 4
    : value < N5$2
    ? 5
    : value < N6$2
    ? 6
    : value < N7$2
    ? 7
    : value < N8$2
    ? 8
    : value < N9$2
    ? 9
    : 10;
};

var varint$2 = {
  encode: encode_1$2,
  decode: decode$b,
  encodingLength: length$2,
};

var _brrp_varint$2 = varint$2;

/**
 * @param {Uint8Array} data
 * @param {number} [offset=0]
 * @returns {[number, number]}
 */
const decode$a = (data, offset = 0) => {
  const code = _brrp_varint$2.decode(data, offset);
  return [code, _brrp_varint$2.decode.bytes];
};

/**
 * @param {number} int
 * @param {Uint8Array} target
 * @param {number} [offset=0]
 */
const encodeTo$2 = (int, target, offset = 0) => {
  _brrp_varint$2.encode(int, target, offset);
  return target;
};

/**
 * @param {number} int
 * @returns {number}
 */
const encodingLength$2 = (int) => {
  return _brrp_varint$2.encodingLength(int);
};

/**
 * Creates a multihash digest.
 *
 * @template {number} Code
 * @param {Code} code
 * @param {Uint8Array} digest
 */
const create$a = (code, digest) => {
  const size = digest.byteLength;
  const sizeOffset = encodingLength$2(code);
  const digestOffset = sizeOffset + encodingLength$2(size);

  const bytes = new Uint8Array(digestOffset + size);
  encodeTo$2(code, bytes, 0);
  encodeTo$2(size, bytes, sizeOffset);
  bytes.set(digest, digestOffset);

  return new Digest$2(code, size, digest, bytes);
};

/**
 * Turns bytes representation of multihash digest into an instance.
 *
 * @param {Uint8Array} multihash
 * @returns {MultihashDigest}
 */
const decode$9 = (multihash) => {
  const bytes = coerce$1(multihash);
  const [code, sizeOffset] = decode$a(bytes);
  const [size, digestOffset] = decode$a(bytes.subarray(sizeOffset));
  const digest = bytes.subarray(sizeOffset + digestOffset);

  if (digest.byteLength !== size) {
    throw new Error("Incorrect length");
  }

  return new Digest$2(code, size, digest, bytes);
};

/**
 * @param {MultihashDigest} a
 * @param {unknown} b
 * @returns {b is MultihashDigest}
 */
const equals$2 = (a, b) => {
  if (a === b) {
    return true;
  } else {
    const data = /** @type {{code?:unknown, size?:unknown, bytes?:unknown}} */ (
      b
    );

    return (
      a.code === data.code &&
      a.size === data.size &&
      data.bytes instanceof Uint8Array &&
      equals$3(a.bytes, data.bytes)
    );
  }
};

/**
 * @typedef {import('./interface.js').MultihashDigest} MultihashDigest
 */

/**
 * Represents a multihash digest which carries information about the
 * hashing algorithm and an actual hash digest.
 *
 * @template {number} Code
 * @template {number} Size
 * @class
 * @implements {MultihashDigest}
 */
let Digest$2 = class Digest {
  /**
   * Creates a multihash digest.
   *
   * @param {Code} code
   * @param {Size} size
   * @param {Uint8Array} digest
   * @param {Uint8Array} bytes
   */
  constructor(code, size, digest, bytes) {
    this.code = code;
    this.size = size;
    this.digest = digest;
    this.bytes = bytes;
  }
};

/**
 * @template {API.Link<unknown, number, number, API.Version>} T
 * @template {string} Prefix
 * @param {T} link
 * @param {API.MultibaseEncoder<Prefix>} [base]
 * @returns {API.ToString<T, Prefix>}
 */
const format$1 = (link, base) => {
  const { bytes, version } = link;
  switch (version) {
    case 0:
      return toStringV0$1(
        bytes,
        baseCache$1(link),
        /** @type {API.MultibaseEncoder<"z">} */ (base) || base58btc$1.encoder
      );
    default:
      return toStringV1$1(
        bytes,
        baseCache$1(link),
        /** @type {API.MultibaseEncoder<Prefix>} */ (base || base32$1.encoder)
      );
  }
};

/** @type {WeakMap<API.UnknownLink, Map<string, string>>} */
const cache$2 = new WeakMap();

/**
 * @param {API.UnknownLink} cid
 * @returns {Map<string, string>}
 */
const baseCache$1 = (cid) => {
  const baseCache = cache$2.get(cid);
  if (baseCache == null) {
    const baseCache = new Map();
    cache$2.set(cid, baseCache);
    return baseCache;
  }
  return baseCache;
};

/**
 * @template {unknown} [Data=unknown]
 * @template {number} [Format=number]
 * @template {number} [Alg=number]
 * @template {API.Version} [Version=API.Version]
 * @implements {API.Link<Data, Format, Alg, Version>}
 */

let CID$1 = class CID {
  /**
   * @param {Version} version - Version of the CID
   * @param {Format} code - Code of the codec content is encoded in, see https://github.com/multiformats/multicodec/blob/master/table.csv
   * @param {API.MultihashDigest<Alg>} multihash - (Multi)hash of the of the content.
   * @param {Uint8Array} bytes
   */
  constructor(version, code, multihash, bytes) {
    /** @readonly */
    this.code = code;
    /** @readonly */
    this.version = version;
    /** @readonly */
    this.multihash = multihash;
    /** @readonly */
    this.bytes = bytes;

    // flag to serializers that this is a CID and
    // should be treated specially
    /** @readonly */
    this["/"] = bytes;
  }

  /**
   * Signalling `cid.asCID === cid` has been replaced with `cid['/'] === cid.bytes`
   * please either use `CID.asCID(cid)` or switch to new signalling mechanism
   *
   * @deprecated
   */
  get asCID() {
    return this;
  }

  // ArrayBufferView
  get byteOffset() {
    return this.bytes.byteOffset;
  }

  // ArrayBufferView
  get byteLength() {
    return this.bytes.byteLength;
  }

  /**
   * @returns {CID<Data, API.DAG_PB, API.SHA_256, 0>}
   */
  toV0() {
    switch (this.version) {
      case 0: {
        return /** @type {CID<Data, API.DAG_PB, API.SHA_256, 0>} */ (this);
      }
      case 1: {
        const { code, multihash } = this;

        if (code !== DAG_PB_CODE$1) {
          throw new Error("Cannot convert a non dag-pb CID to CIDv0");
        }

        // sha2-256
        if (multihash.code !== SHA_256_CODE$1) {
          throw new Error("Cannot convert non sha2-256 multihash CID to CIDv0");
        }

        return /** @type {CID<Data, API.DAG_PB, API.SHA_256, 0>} */ (
          CID.createV0(
            /** @type {API.MultihashDigest<API.SHA_256>} */ (multihash)
          )
        );
      }
      default: {
        throw Error(
          `Can not convert CID version ${this.version} to version 0. This is a bug please report`
        );
      }
    }
  }

  /**
   * @returns {CID<Data, Format, Alg, 1>}
   */
  toV1() {
    switch (this.version) {
      case 0: {
        const { code, digest } = this.multihash;
        const multihash = create$a(code, digest);
        return /** @type {CID<Data, Format, Alg, 1>} */ (
          CID.createV1(this.code, multihash)
        );
      }
      case 1: {
        return /** @type {CID<Data, Format, Alg, 1>} */ (this);
      }
      default: {
        throw Error(
          `Can not convert CID version ${this.version} to version 1. This is a bug please report`
        );
      }
    }
  }

  /**
   * @param {unknown} other
   * @returns {other is CID<Data, Format, Alg, Version>}
   */
  equals(other) {
    return CID.equals(this, other);
  }

  /**
   * @template {unknown} Data
   * @template {number} Format
   * @template {number} Alg
   * @template {API.Version} Version
   * @param {API.Link<Data, Format, Alg, Version>} self
   * @param {unknown} other
   * @returns {other is CID}
   */
  static equals(self, other) {
    const unknown =
      /** @type {{code?:unknown, version?:unknown, multihash?:unknown}} */ (
        other
      );
    return (
      unknown &&
      self.code === unknown.code &&
      self.version === unknown.version &&
      equals$2(self.multihash, unknown.multihash)
    );
  }

  /**
   * @param {API.MultibaseEncoder<string>} [base]
   * @returns {string}
   */
  toString(base) {
    return format$1(this, base);
  }

  /**
   * @returns {API.LinkJSON<this>}
   */
  toJSON() {
    return { "/": format$1(this) };
  }

  link() {
    return this;
  }

  get [Symbol.toStringTag]() {
    return "CID";
  }

  // Legacy

  [Symbol.for("nodejs.util.inspect.custom")]() {
    return `CID(${this.toString()})`;
  }

  /**
   * Takes any input `value` and returns a `CID` instance if it was
   * a `CID` otherwise returns `null`. If `value` is instanceof `CID`
   * it will return value back. If `value` is not instance of this CID
   * class, but is compatible CID it will return new instance of this
   * `CID` class. Otherwise returns null.
   *
   * This allows two different incompatible versions of CID library to
   * co-exist and interop as long as binary interface is compatible.
   *
   * @template {unknown} Data
   * @template {number} Format
   * @template {number} Alg
   * @template {API.Version} Version
   * @template {unknown} U
   * @param {API.Link<Data, Format, Alg, Version>|U} input
   * @returns {CID<Data, Format, Alg, Version>|null}
   */
  static asCID(input) {
    if (input == null) {
      return null;
    }

    const value = /** @type {any} */ (input);
    if (value instanceof CID) {
      // If value is instance of CID then we're all set.
      return value;
    } else if (
      (value["/"] != null && value["/"] === value.bytes) ||
      value.asCID === value
    ) {
      // If value isn't instance of this CID class but `this.asCID === this` or
      // `value['/'] === value.bytes` is true it is CID instance coming from a
      // different implementation (diff version or duplicate). In that case we
      // rebase it to this `CID` implementation so caller is guaranteed to get
      // instance with expected API.
      const { version, code, multihash, bytes } = value;
      return new CID(
        version,
        code,
        /** @type {API.MultihashDigest<Alg>} */ (multihash),
        bytes || encodeCID$1(version, code, multihash.bytes)
      );
    } else if (value[cidSymbol$1] === true) {
      // If value is a CID from older implementation that used to be tagged via
      // symbol we still rebase it to the this `CID` implementation by
      // delegating that to a constructor.
      const { version, multihash, code } = value;
      const digest =
        /** @type {API.MultihashDigest<Alg>} */
        (decode$9(multihash));
      return CID.create(version, code, digest);
    } else {
      // Otherwise value is not a CID (or an incompatible version of it) in
      // which case we return `null`.
      return null;
    }
  }

  /**
   *
   * @template {unknown} Data
   * @template {number} Format
   * @template {number} Alg
   * @template {API.Version} Version
   * @param {Version} version - Version of the CID
   * @param {Format} code - Code of the codec content is encoded in, see https://github.com/multiformats/multicodec/blob/master/table.csv
   * @param {API.MultihashDigest<Alg>} digest - (Multi)hash of the of the content.
   * @returns {CID<Data, Format, Alg, Version>}
   */
  static create(version, code, digest) {
    if (typeof code !== "number") {
      throw new Error("String codecs are no longer supported");
    }

    if (!(digest.bytes instanceof Uint8Array)) {
      throw new Error("Invalid digest");
    }

    switch (version) {
      case 0: {
        if (code !== DAG_PB_CODE$1) {
          throw new Error(
            `Version 0 CID must use dag-pb (code: ${DAG_PB_CODE$1}) block encoding`
          );
        } else {
          return new CID(version, code, digest, digest.bytes);
        }
      }
      case 1: {
        const bytes = encodeCID$1(version, code, digest.bytes);
        return new CID(version, code, digest, bytes);
      }
      default: {
        throw new Error("Invalid version");
      }
    }
  }

  /**
   * Simplified version of `create` for CIDv0.
   *
   * @template {unknown} [T=unknown]
   * @param {API.MultihashDigest<typeof SHA_256_CODE>} digest - Multihash.
   * @returns {CID<T, typeof DAG_PB_CODE, typeof SHA_256_CODE, 0>}
   */
  static createV0(digest) {
    return CID.create(0, DAG_PB_CODE$1, digest);
  }

  /**
   * Simplified version of `create` for CIDv1.
   *
   * @template {unknown} Data
   * @template {number} Code
   * @template {number} Alg
   * @param {Code} code - Content encoding format code.
   * @param {API.MultihashDigest<Alg>} digest - Miltihash of the content.
   * @returns {CID<Data, Code, Alg, 1>}
   */
  static createV1(code, digest) {
    return CID.create(1, code, digest);
  }

  /**
   * Decoded a CID from its binary representation. The byte array must contain
   * only the CID with no additional bytes.
   *
   * An error will be thrown if the bytes provided do not contain a valid
   * binary representation of a CID.
   *
   * @template {unknown} Data
   * @template {number} Code
   * @template {number} Alg
   * @template {API.Version} Ver
   * @param {API.ByteView<API.Link<Data, Code, Alg, Ver>>} bytes
   * @returns {CID<Data, Code, Alg, Ver>}
   */
  static decode(bytes) {
    const [cid, remainder] = CID.decodeFirst(bytes);
    if (remainder.length) {
      throw new Error("Incorrect length");
    }
    return cid;
  }

  /**
   * Decoded a CID from its binary representation at the beginning of a byte
   * array.
   *
   * Returns an array with the first element containing the CID and the second
   * element containing the remainder of the original byte array. The remainder
   * will be a zero-length byte array if the provided bytes only contained a
   * binary CID representation.
   *
   * @template {unknown} T
   * @template {number} C
   * @template {number} A
   * @template {API.Version} V
   * @param {API.ByteView<API.Link<T, C, A, V>>} bytes
   * @returns {[CID<T, C, A, V>, Uint8Array]}
   */
  static decodeFirst(bytes) {
    const specs = CID.inspectBytes(bytes);
    const prefixSize = specs.size - specs.multihashSize;
    const multihashBytes = coerce$1(
      bytes.subarray(prefixSize, prefixSize + specs.multihashSize)
    );
    if (multihashBytes.byteLength !== specs.multihashSize) {
      throw new Error("Incorrect length");
    }
    const digestBytes = multihashBytes.subarray(
      specs.multihashSize - specs.digestSize
    );
    const digest = new Digest$2(
      specs.multihashCode,
      specs.digestSize,
      digestBytes,
      multihashBytes
    );
    const cid =
      specs.version === 0
        ? CID.createV0(/** @type {API.MultihashDigest<API.SHA_256>} */ (digest))
        : CID.createV1(specs.codec, digest);
    return [/** @type {CID<T, C, A, V>} */ (cid), bytes.subarray(specs.size)];
  }

  /**
   * Inspect the initial bytes of a CID to determine its properties.
   *
   * Involves decoding up to 4 varints. Typically this will require only 4 to 6
   * bytes but for larger multicodec code values and larger multihash digest
   * lengths these varints can be quite large. It is recommended that at least
   * 10 bytes be made available in the `initialBytes` argument for a complete
   * inspection.
   *
   * @template {unknown} T
   * @template {number} C
   * @template {number} A
   * @template {API.Version} V
   * @param {API.ByteView<API.Link<T, C, A, V>>} initialBytes
   * @returns {{ version:V, codec:C, multihashCode:A, digestSize:number, multihashSize:number, size:number }}
   */
  static inspectBytes(initialBytes) {
    let offset = 0;
    const next = () => {
      const [i, length] = decode$a(initialBytes.subarray(offset));
      offset += length;
      return i;
    };

    let version = /** @type {V} */ (next());
    let codec = /** @type {C} */ (DAG_PB_CODE$1);
    if (/** @type {number} */ (version) === 18) {
      // CIDv0
      version = /** @type {V} */ (0);
      offset = 0;
    } else {
      codec = /** @type {C} */ (next());
    }

    if (version !== 0 && version !== 1) {
      throw new RangeError(`Invalid CID version ${version}`);
    }

    const prefixSize = offset;
    const multihashCode = /** @type {A} */ (next()); // multihash code
    const digestSize = next(); // multihash length
    const size = offset + digestSize;
    const multihashSize = size - prefixSize;

    return { version, codec, multihashCode, digestSize, multihashSize, size };
  }

  /**
   * Takes cid in a string representation and creates an instance. If `base`
   * decoder is not provided will use a default from the configuration. It will
   * throw an error if encoding of the CID is not compatible with supplied (or
   * a default decoder).
   *
   * @template {string} Prefix
   * @template {unknown} Data
   * @template {number} Code
   * @template {number} Alg
   * @template {API.Version} Ver
   * @param {API.ToString<API.Link<Data, Code, Alg, Ver>, Prefix>} source
   * @param {API.MultibaseDecoder<Prefix>} [base]
   * @returns {CID<Data, Code, Alg, Ver>}
   */
  static parse(source, base) {
    const [prefix, bytes] = parseCIDtoBytes$1(source, base);

    const cid = CID.decode(bytes);

    if (cid.version === 0 && source[0] !== "Q") {
      throw Error("Version 0 CID string must not include multibase prefix");
    }

    // Cache string representation to avoid computing it on `this.toString()`
    baseCache$1(cid).set(prefix, source);

    return cid;
  }
};

/**
 * @template {string} Prefix
 * @template {unknown} Data
 * @template {number} Code
 * @template {number} Alg
 * @template {API.Version} Ver
 * @param {API.ToString<API.Link<Data, Code, Alg, Ver>, Prefix>} source
 * @param {API.MultibaseDecoder<Prefix>} [base]
 * @returns {[Prefix, API.ByteView<API.Link<Data, Code, Alg, Ver>>]}
 */
const parseCIDtoBytes$1 = (source, base) => {
  switch (source[0]) {
    // CIDv0 is parsed differently
    case "Q": {
      const decoder = base || base58btc$1;
      return [
        /** @type {Prefix} */ (base58btc$1.prefix),
        decoder.decode(`${base58btc$1.prefix}${source}`),
      ];
    }
    case base58btc$1.prefix: {
      const decoder = base || base58btc$1;
      return [
        /** @type {Prefix} */ (base58btc$1.prefix),
        decoder.decode(source),
      ];
    }
    case base32$1.prefix: {
      const decoder = base || base32$1;
      return [/** @type {Prefix} */ (base32$1.prefix), decoder.decode(source)];
    }
    default: {
      if (base == null) {
        throw Error(
          "To parse non base32 or base58btc encoded CID multibase decoder must be provided"
        );
      }
      return [/** @type {Prefix} */ (source[0]), base.decode(source)];
    }
  }
};

/**
 *
 * @param {Uint8Array} bytes
 * @param {Map<string, string>} cache
 * @param {API.MultibaseEncoder<'z'>} base
 */
const toStringV0$1 = (bytes, cache, base) => {
  const { prefix } = base;
  if (prefix !== base58btc$1.prefix) {
    throw Error(`Cannot string encode V0 in ${base.name} encoding`);
  }

  const cid = cache.get(prefix);
  if (cid == null) {
    const cid = base.encode(bytes).slice(1);
    cache.set(prefix, cid);
    return cid;
  } else {
    return cid;
  }
};

/**
 * @template {string} Prefix
 * @param {Uint8Array} bytes
 * @param {Map<string, string>} cache
 * @param {API.MultibaseEncoder<Prefix>} base
 */
const toStringV1$1 = (bytes, cache, base) => {
  const { prefix } = base;
  const cid = cache.get(prefix);
  if (cid == null) {
    const cid = base.encode(bytes);
    cache.set(prefix, cid);
    return cid;
  } else {
    return cid;
  }
};

const DAG_PB_CODE$1 = 0x70;
const SHA_256_CODE$1 = 0x12;

/**
 * @param {API.Version} version
 * @param {number} code
 * @param {Uint8Array} multihash
 * @returns {Uint8Array}
 */
const encodeCID$1 = (version, code, multihash) => {
  const codeOffset = encodingLength$2(version);
  const hashOffset = codeOffset + encodingLength$2(code);
  const bytes = new Uint8Array(hashOffset + multihash.byteLength);
  encodeTo$2(version, bytes, 0);
  encodeTo$2(code, bytes, codeOffset);
  bytes.set(multihash, hashOffset);
  return bytes;
};

const cidSymbol$1 = Symbol.for("@ipld/js-cid/CID");

/**
 * Simplified version of `create` for CIDv1.
 *
 * @template {unknown} Data
 * @template {number} Code
 * @template {number} Alg
 * @param {Code} code - Content encoding format code.
 * @param {API.MultihashDigest<Alg>} digest - Miltihash of the content.
 * @returns {API.Link<Data, Code, Alg>}
 */
const create$9 = (code, digest) => CID$1.create(1, code, digest);

// @ts-check

/**
 * @template T
 * @typedef {import('./interface.js').ByteView<T>} ByteView
 */

const name$2 = "raw";
const code$3 = 0x55;

/**
 * @param {Uint8Array} node
 * @returns {ByteView<Uint8Array>}
 */
const encode$7 = (node) => coerce$1(node);

/**
 * @param {ByteView<Uint8Array>} data
 * @returns {Uint8Array}
 */
const decode$8 = (data) => coerce$1(data);

var raw = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  code: code$3,
  decode: decode$8,
  encode: encode$7,
  name: name$2,
});

/**
 * @template {string} Name
 * @template {number} Code
 * @param {object} options
 * @param {Name} options.name
 * @param {Code} options.code
 * @param {(input: Uint8Array) => Await<Uint8Array>} options.encode
 */
const from$7 = ({ name, code, encode }) => new Hasher$2(name, code, encode);

/**
 * Hasher represents a hashing algorithm implementation that produces as
 * `MultihashDigest`.
 *
 * @template {string} Name
 * @template {number} Code
 * @class
 * @implements {MultihashHasher<Code>}
 */
let Hasher$2 = class Hasher {
  /**
   *
   * @param {Name} name
   * @param {Code} code
   * @param {(input: Uint8Array) => Await<Uint8Array>} encode
   */
  constructor(name, code, encode) {
    this.name = name;
    this.code = code;
    this.encode = encode;
  }

  /**
   * @param {Uint8Array} input
   * @returns {Await<Digest.Digest<Code, number>>}
   */
  digest(input) {
    if (input instanceof Uint8Array) {
      const result = this.encode(input);
      return result instanceof Uint8Array
        ? create$a(this.code, result)
        : /* c8 ignore next 1 */
          result.then((digest) => create$a(this.code, digest));
    } else {
      throw Error("Unknown type, must be binary type");
      /* c8 ignore next 1 */
    }
  }
};

/**
 * @template {number} Alg
 * @typedef {import('./interface.js').MultihashHasher} MultihashHasher
 */

/**
 * @template T
 * @typedef {Promise<T>|T} Await
 */

/* global crypto */

/**
 * @param {AlgorithmIdentifier} name
 */
const sha$2 =
  (name) =>
  /**
   * @param {Uint8Array} data
   */
  async (data) =>
    new Uint8Array(await crypto.subtle.digest(name, data));

const sha256$2 = from$7({
  name: "sha2-256",
  code: 0x12,
  encode: sha$2("SHA-256"),
});

var retry$2 = {};

var retry_operation;
var hasRequiredRetry_operation;

function requireRetry_operation() {
  if (hasRequiredRetry_operation) return retry_operation;
  hasRequiredRetry_operation = 1;
  function RetryOperation(timeouts, options) {
    // Compatibility for the old (timeouts, retryForever) signature
    if (typeof options === "boolean") {
      options = { forever: options };
    }

    this._originalTimeouts = JSON.parse(JSON.stringify(timeouts));
    this._timeouts = timeouts;
    this._options = options || {};
    this._maxRetryTime = (options && options.maxRetryTime) || Infinity;
    this._fn = null;
    this._errors = [];
    this._attempts = 1;
    this._operationTimeout = null;
    this._operationTimeoutCb = null;
    this._timeout = null;
    this._operationStart = null;
    this._timer = null;

    if (this._options.forever) {
      this._cachedTimeouts = this._timeouts.slice(0);
    }
  }
  retry_operation = RetryOperation;

  RetryOperation.prototype.reset = function () {
    this._attempts = 1;
    this._timeouts = this._originalTimeouts.slice(0);
  };

  RetryOperation.prototype.stop = function () {
    if (this._timeout) {
      clearTimeout(this._timeout);
    }
    if (this._timer) {
      clearTimeout(this._timer);
    }

    this._timeouts = [];
    this._cachedTimeouts = null;
  };

  RetryOperation.prototype.retry = function (err) {
    if (this._timeout) {
      clearTimeout(this._timeout);
    }

    if (!err) {
      return false;
    }
    var currentTime = new Date().getTime();
    if (err && currentTime - this._operationStart >= this._maxRetryTime) {
      this._errors.push(err);
      this._errors.unshift(new Error("RetryOperation timeout occurred"));
      return false;
    }

    this._errors.push(err);

    var timeout = this._timeouts.shift();
    if (timeout === undefined) {
      if (this._cachedTimeouts) {
        // retry forever, only keep last error
        this._errors.splice(0, this._errors.length - 1);
        timeout = this._cachedTimeouts.slice(-1);
      } else {
        return false;
      }
    }

    var self = this;
    this._timer = setTimeout(function () {
      self._attempts++;

      if (self._operationTimeoutCb) {
        self._timeout = setTimeout(function () {
          self._operationTimeoutCb(self._attempts);
        }, self._operationTimeout);

        if (self._options.unref) {
          self._timeout.unref();
        }
      }

      self._fn(self._attempts);
    }, timeout);

    if (this._options.unref) {
      this._timer.unref();
    }

    return true;
  };

  RetryOperation.prototype.attempt = function (fn, timeoutOps) {
    this._fn = fn;

    if (timeoutOps) {
      if (timeoutOps.timeout) {
        this._operationTimeout = timeoutOps.timeout;
      }
      if (timeoutOps.cb) {
        this._operationTimeoutCb = timeoutOps.cb;
      }
    }

    var self = this;
    if (this._operationTimeoutCb) {
      this._timeout = setTimeout(function () {
        self._operationTimeoutCb();
      }, self._operationTimeout);
    }

    this._operationStart = new Date().getTime();

    this._fn(this._attempts);
  };

  RetryOperation.prototype.try = function (fn) {
    console.log("Using RetryOperation.try() is deprecated");
    this.attempt(fn);
  };

  RetryOperation.prototype.start = function (fn) {
    console.log("Using RetryOperation.start() is deprecated");
    this.attempt(fn);
  };

  RetryOperation.prototype.start = RetryOperation.prototype.try;

  RetryOperation.prototype.errors = function () {
    return this._errors;
  };

  RetryOperation.prototype.attempts = function () {
    return this._attempts;
  };

  RetryOperation.prototype.mainError = function () {
    if (this._errors.length === 0) {
      return null;
    }

    var counts = {};
    var mainError = null;
    var mainErrorCount = 0;

    for (var i = 0; i < this._errors.length; i++) {
      var error = this._errors[i];
      var message = error.message;
      var count = (counts[message] || 0) + 1;

      counts[message] = count;

      if (count >= mainErrorCount) {
        mainError = error;
        mainErrorCount = count;
      }
    }

    return mainError;
  };
  return retry_operation;
}

var hasRequiredRetry$1;

function requireRetry$1() {
  if (hasRequiredRetry$1) return retry$2;
  hasRequiredRetry$1 = 1;
  (function (exports) {
    var RetryOperation = requireRetry_operation();

    exports.operation = function (options) {
      var timeouts = exports.timeouts(options);
      return new RetryOperation(timeouts, {
        forever: options && (options.forever || options.retries === Infinity),
        unref: options && options.unref,
        maxRetryTime: options && options.maxRetryTime,
      });
    };

    exports.timeouts = function (options) {
      if (options instanceof Array) {
        return [].concat(options);
      }

      var opts = {
        retries: 10,
        factor: 2,
        minTimeout: 1 * 1000,
        maxTimeout: Infinity,
        randomize: false,
      };
      for (var key in options) {
        opts[key] = options[key];
      }

      if (opts.minTimeout > opts.maxTimeout) {
        throw new Error("minTimeout is greater than maxTimeout");
      }

      var timeouts = [];
      for (var i = 0; i < opts.retries; i++) {
        timeouts.push(this.createTimeout(i, opts));
      }

      if (options && options.forever && !timeouts.length) {
        timeouts.push(this.createTimeout(i, opts));
      }

      // sort the array numerically ascending
      timeouts.sort(function (a, b) {
        return a - b;
      });

      return timeouts;
    };

    exports.createTimeout = function (attempt, opts) {
      var random = opts.randomize ? Math.random() + 1 : 1;

      var timeout = Math.round(
        random * Math.max(opts.minTimeout, 1) * Math.pow(opts.factor, attempt)
      );
      timeout = Math.min(timeout, opts.maxTimeout);

      return timeout;
    };

    exports.wrap = function (obj, options, methods) {
      if (options instanceof Array) {
        methods = options;
        options = null;
      }

      if (!methods) {
        methods = [];
        for (var key in obj) {
          if (typeof obj[key] === "function") {
            methods.push(key);
          }
        }
      }

      for (var i = 0; i < methods.length; i++) {
        var method = methods[i];
        var original = obj[method];

        obj[method] = function retryWrapper(original) {
          var op = exports.operation(options);
          var args = Array.prototype.slice.call(arguments, 1);
          var callback = args.pop();

          args.push(function (err) {
            if (op.retry(err)) {
              return;
            }
            if (err) {
              arguments[0] = op.mainError();
            }
            callback.apply(this, arguments);
          });

          op.attempt(function () {
            original.apply(obj, args);
          });
        }.bind(obj, original);
        obj[method].options = options;
      }
    };
  })(retry$2);
  return retry$2;
}

var retry$1;
var hasRequiredRetry;

function requireRetry() {
  if (hasRequiredRetry) return retry$1;
  hasRequiredRetry = 1;
  retry$1 = requireRetry$1();
  return retry$1;
}

var retryExports = requireRetry();
var retry = /*@__PURE__*/ getDefaultExportFromCjs(retryExports);

const networkErrorMsgs = new Set([
  "Failed to fetch", // Chrome
  "NetworkError when attempting to fetch resource.", // Firefox
  "The Internet connection appears to be offline.", // Safari
  "Network request failed", // `cross-fetch`
  "fetch failed", // Undici (Node.js)
]);

class AbortError extends Error {
  constructor(message) {
    super();

    if (message instanceof Error) {
      this.originalError = message;
      ({ message } = message);
    } else {
      this.originalError = new Error(message);
      this.originalError.stack = this.stack;
    }

    this.name = "AbortError";
    this.message = message;
  }
}

const decorateErrorWithCounts = (error, attemptNumber, options) => {
  // Minus 1 from attemptNumber because the first attempt does not count as a retry
  const retriesLeft = options.retries - (attemptNumber - 1);

  error.attemptNumber = attemptNumber;
  error.retriesLeft = retriesLeft;
  return error;
};

const isNetworkError = (errorMessage) => networkErrorMsgs.has(errorMessage);

const getDOMException = (errorMessage) =>
  globalThis.DOMException === undefined
    ? new Error(errorMessage)
    : new DOMException(errorMessage);

async function pRetry(input, options) {
  return new Promise((resolve, reject) => {
    options = {
      onFailedAttempt() {},
      retries: 10,
      ...options,
    };

    const operation = retry.operation(options);

    operation.attempt(async (attemptNumber) => {
      try {
        resolve(await input(attemptNumber));
      } catch (error) {
        if (!(error instanceof Error)) {
          reject(
            new TypeError(
              `Non-error was thrown: "${error}". You should only throw errors.`
            )
          );
          return;
        }

        if (error instanceof AbortError) {
          operation.stop();
          reject(error.originalError);
        } else if (
          error instanceof TypeError &&
          !isNetworkError(error.message)
        ) {
          operation.stop();
          reject(error);
        } else {
          decorateErrorWithCounts(error, attemptNumber, options);

          try {
            await options.onFailedAttempt(error);
          } catch (error) {
            reject(error);
            return;
          }

          if (!operation.retry(error)) {
            reject(operation.mainError());
          }
        }
      }
    });

    if (options.signal && !options.signal.aborted) {
      options.signal.addEventListener(
        "abort",
        () => {
          operation.stop();
          const reason =
            options.signal.reason === undefined
              ? getDOMException("The operation was aborted.")
              : options.signal.reason;
          reject(reason instanceof Error ? reason : getDOMException(reason));
        },
        {
          once: true,
        }
      );
    }
  });
}

const serviceURL = new URL("https://up.web3.storage");
const servicePrincipal = parse$1("did:web:web3.storage");
const receiptsEndpoint = "https://up.web3.storage/receipt/";
/** @type {import('@ucanto/interface').ConnectionView<import('./types.js').Service>} */
const connection = connect({
  id: servicePrincipal,
  codec: outbound,
  channel: open$2({
    url: serviceURL,
    method: "POST",
  }),
});

const REQUEST_RETRIES = 3;

/**
 *
 * @param {string} url
 * @param {import('./types.js').ProgressFn} handler
 */
function createUploadProgressHandler$1(url, handler) {
  /**
   *
   * @param {import('./types.js').ProgressStatus} status
   */
  function onUploadProgress({ total, loaded, lengthComputable }) {
    return handler({ total, loaded, lengthComputable, url });
  }
  return onUploadProgress;
}
/**
 * Store a DAG encoded as a CAR file. The issuer needs the `store/add`
 * delegated capability.
 *
 * Required delegated capability proofs: `store/add`
 *
 * @param {import('./types.js').InvocationConfig} conf Configuration
 * for the UCAN invocation. An object with `issuer`, `with` and `proofs`.
 *
 * The `issuer` is the signing authority that is issuing the UCAN
 * invocation(s). It is typically the user _agent_.
 *
 * The `with` is the resource the invocation applies to. It is typically the
 * DID of a space.
 *
 * The `proofs` are a set of capability delegations that prove the issuer
 * has the capability to perform the action.
 *
 * The issuer needs the `store/add` delegated capability.
 * @param {Blob|Uint8Array} car CAR file data.
 * @param {import('./types.js').RequestOptions} [options]
 * @returns {Promise<import('./types.js').CARLink>}
 */
async function add$3(
  { issuer, with: resource, proofs, audience },
  car,
  options = {}
) {
  // TODO: validate blob contains CAR data
  /* c8 ignore next 2 */
  const bytes =
    car instanceof Uint8Array ? car : new Uint8Array(await car.arrayBuffer());
  const link = await link$2(bytes);
  /* c8 ignore next */
  const conn = options.connection ?? connection;
  const result = await pRetry(
    async () => {
      return await add$a
        .invoke({
          issuer,
          /* c8 ignore next */
          audience: audience ?? servicePrincipal,
          with: SpaceDID$1.from(resource),
          nb: { link, size: bytes.length },
          proofs,
          nonce: options.nonce,
        })
        .execute(conn);
    },
    {
      onFailedAttempt: console.warn,
      retries: options.retries ?? REQUEST_RETRIES,
    }
  );
  if (!result.out.ok) {
    throw new Error(`failed ${add$a.can} invocation`, {
      cause: result.out.error,
    });
  }
  // Return early if it was already uploaded.
  if (result.out.ok.status === "done") {
    return link;
  }
  const responseAddUpload = result.out.ok;
  const fetchWithUploadProgress =
    options.fetchWithUploadProgress ||
    options.fetch ||
    globalThis.fetch.bind(globalThis);
  let fetchDidCallUploadProgressCb = false;
  const res = await pRetry(
    async () => {
      try {
        const res = await fetchWithUploadProgress(responseAddUpload.url, {
          method: "PUT",
          body: car,
          headers: responseAddUpload.headers,
          signal: options.signal,
          onUploadProgress: (status) => {
            fetchDidCallUploadProgressCb = true;
            if (options.onUploadProgress)
              createUploadProgressHandler$1(
                responseAddUpload.url,
                options.onUploadProgress
              )(status);
          },
          // @ts-expect-error - this is needed by recent versions of node - see https://github.com/bluesky-social/atproto/pull/470 for more info
          duplex: "half",
        });
        if (res.status >= 400 && res.status < 500) {
          throw new AbortError(`upload failed: ${res.status}`);
        }
        return res;
      } catch (err) {
        if (options.signal?.aborted === true) {
          throw new AbortError("upload aborted");
        }
        throw err;
      }
    },
    {
      retries: options.retries ?? REQUEST_RETRIES,
    }
  );
  if (!fetchDidCallUploadProgressCb && options.onUploadProgress) {
    // the fetch implementation didn't support onUploadProgress
    const carBlob = new Blob([car]);
    options.onUploadProgress({
      total: carBlob.size,
      loaded: carBlob.size,
      lengthComputable: false,
    });
  }
  if (!res.ok) {
    throw new Error(`upload failed: ${res.status}`);
  }
  return link;
}
/**
 * Get details of a stored item.
 *
 * Required delegated capability proofs: `store/get`
 *
 * @param {import('./types.js').InvocationConfig} conf Configuration
 * for the UCAN invocation. An object with `issuer`, `with` and `proofs`.
 *
 * The `issuer` is the signing authority that is issuing the UCAN
 * invocation(s). It is typically the user _agent_.
 *
 * The `with` is the resource the invocation applies to. It is typically the
 * DID of a space.
 *
 * The `proofs` are a set of capability delegations that prove the issuer
 * has the capability to perform the action.
 *
 * The issuer needs the `store/get` delegated capability.
 * @param {import('multiformats/link').Link<unknown, CAR.codec.code>} link CID of stored CAR file.
 * @param {import('./types.js').RequestOptions} [options]
 * @returns {Promise<import('./types.js').StoreGetSuccess>}
 */
async function get$a(
  { issuer, with: resource, proofs, audience },
  link,
  options = {}
) {
  /* c8 ignore next */
  const conn = options.connection ?? connection;
  const result = await pRetry(
    async () => {
      return await get$h
        .invoke({
          issuer,
          /* c8 ignore next */
          audience: audience ?? servicePrincipal,
          with: SpaceDID$1.from(resource),
          nb: { link },
          proofs,
          nonce: options.nonce,
        })
        .execute(conn);
    },
    {
      onFailedAttempt: console.warn,
      retries: options.retries ?? REQUEST_RETRIES,
    }
  );
  if (!result.out.ok) {
    throw new Error(`failed ${get$h.can} invocation`, {
      cause: result.out.error,
    });
  }
  return result.out.ok;
}
/**
 * List CAR files stored by the issuer.
 *
 * @param {import('./types.js').InvocationConfig} conf Configuration
 * for the UCAN invocation. An object with `issuer`, `with` and `proofs`.
 *
 * The `issuer` is the signing authority that is issuing the UCAN
 * invocation(s). It is typically the user _agent_.
 *
 * The `with` is the resource the invocation applies to. It is typically the
 * DID of a space.
 *
 * The `proofs` are a set of capability delegations that prove the issuer
 * has the capability to perform the action.
 *
 * The issuer needs the `store/list` delegated capability.
 * @param {import('./types.js').ListRequestOptions} [options]
 * @returns {Promise<import('./types.js').StoreListSuccess>}
 */
async function list$4(
  { issuer, with: resource, proofs, audience },
  options = {}
) {
  /* c8 ignore next */
  const conn = options.connection ?? connection;
  const result = await list$9
    .invoke({
      issuer,
      /* c8 ignore next */
      audience: audience ?? servicePrincipal,
      with: SpaceDID$1.from(resource),
      proofs,
      nb: {
        cursor: options.cursor,
        size: options.size,
        pre: options.pre,
      },
      nonce: options.nonce,
    })
    .execute(conn);
  if (!result.out.ok) {
    throw new Error(`failed ${list$9.can} invocation`, {
      cause: result.out.error,
    });
  }
  return result.out.ok;
}
/**
 * Remove a stored CAR file by CAR CID.
 *
 * @param {import('./types.js').InvocationConfig} conf Configuration
 * for the UCAN invocation. An object with `issuer`, `with` and `proofs`.
 *
 * The `issuer` is the signing authority that is issuing the UCAN
 * invocation(s). It is typically the user _agent_.
 *
 * The `with` is the resource the invocation applies to. It is typically the
 * DID of a space.
 *
 * The `proofs` are a set of capability delegations that prove the issuer
 * has the capability to perform the action.
 *
 * The issuer needs the `store/remove` delegated capability.
 * @param {import('./types.js').CARLink} link CID of CAR file to remove.
 * @param {import('./types.js').RequestOptions} [options]
 */
async function remove$4(
  { issuer, with: resource, proofs, audience },
  link,
  options = {}
) {
  /* c8 ignore next */
  const conn = options.connection ?? connection;
  const result = await remove$8
    .invoke({
      issuer,
      /* c8 ignore next */
      audience: audience ?? servicePrincipal,
      with: SpaceDID$1.from(resource),
      nb: { link },
      proofs,
      nonce: options.nonce,
    })
    .execute(conn);
  if (!result.out.ok) {
    throw new Error(`failed ${remove$8.can} invocation`, {
      cause: result.out.error,
    });
  }
  return result.out;
}

class ReceiptNotFound extends Error {
  /**
   * @param {import('multiformats').UnknownLink} taskCid
   */
  constructor(taskCid) {
    super();
    this.taskCid = taskCid;
  }
  /* c8 ignore start */
  get reason() {
    return `receipt not found for task ${this.taskCid} in the indexed workflow`;
  }
  /* c8 ignore end */
  get name() {
    return "ReceiptNotFound";
  }
}
class ReceiptMissing extends Error {
  /**
   * @param {import('multiformats').UnknownLink} taskCid
   */
  constructor(taskCid) {
    super();
    this.taskCid = taskCid;
  }
  /* c8 ignore start */
  get reason() {
    return `receipt missing for task ${this.taskCid}`;
  }
  /* c8 ignore end */
  get name() {
    return "ReceiptMissing";
  }
}
/**
 * Polls for a receipt for an executed task by its CID.
 *
 * @param {import('multiformats').UnknownLink} taskCid
 * @param {import('./types.js').RequestOptions} [options]
 * @returns {Promise<import('@ucanto/interface').Receipt>}
 */
async function poll(taskCid, options = {}) {
  return await pRetry(
    async () => {
      const res = await get$9(taskCid, options);
      if (res.error) {
        // @ts-ignore
        if (res.error.name === "ReceiptNotFound") {
          // throw an error that will cause `p-retry` to retry with
          throw res.error;
        } else {
          throw new AbortError(
            new Error("failed to fetch blob/accept receipt", {
              cause: res.error,
            })
          );
        }
      }
      return res.ok;
    },
    {
      onFailedAttempt: console.warn,
      /* c8 ignore next */
      retries: options.retries ?? REQUEST_RETRIES,
    }
  );
}
/**
 * Calculate a receipt endpoint from the URL of a channel, if it has one.
 *
 * @param {import('@ucanto/interface').Channel<Record<string, any>>} channel
 */
function receiptEndpointFromChannel(channel) {
  if ("url" in channel && channel.url instanceof URL) {
    const url = channel.url;
    return new URL("/receipt/", url.toString());
  } else {
    return null;
  }
}
/**
 * Get a receipt for an executed task by its CID.
 *
 * @param {import('multiformats').UnknownLink} taskCid
 * @param {import('./types.js').RequestOptions} [options]
 * @returns {Promise<import('@ucanto/client').Result<import('@ucanto/interface').Receipt, Error>>}
 */
async function get$9(taskCid, options = {}) {
  const channel = options.connection?.channel;
  const receiptsEndpoint$1 =
    options.receiptsEndpoint ??
    (channel && receiptEndpointFromChannel(channel)) ??
    receiptsEndpoint;
  // Fetch receipt from endpoint
  const url = new URL(taskCid.toString(), receiptsEndpoint$1);
  const fetchReceipt = options.fetch ?? globalThis.fetch.bind(globalThis);
  const workflowResponse = await fetchReceipt(url);
  /* c8 ignore start */
  if (workflowResponse.status === 404) {
    return {
      error: new ReceiptNotFound(taskCid),
    };
  }
  /* c8 ignore stop */
  // Get receipt from Message Archive
  const agentMessageBytes = new Uint8Array(
    await workflowResponse.arrayBuffer()
  );
  // Decode message
  const agentMessage = await decode$m({
    body: agentMessageBytes,
    headers: {},
  });
  // Get receipt from the potential multiple receipts in the message
  const receipt = agentMessage.receipts.get(taskCid.toString());
  if (!receipt) {
    return {
      error: new ReceiptMissing(taskCid),
    };
  }
  return {
    ok: receipt,
  };
}

/* c8 ignore next 3 */
const isCloudflareWorkers =
  typeof navigator !== "undefined" &&
  navigator?.userAgent === "Cloudflare-Workers";

/**
 * @param {string} url
 * @param {import('../types.js').ProgressFn} handler
 */
function createUploadProgressHandler(url, handler) {
  /** @param {import('../types.js').ProgressStatus} status */
  const onUploadProgress = ({ total, loaded, lengthComputable }) => {
    return handler({ total, loaded, lengthComputable, url });
  };
  return onUploadProgress;
}
// FIXME this code has been copied over from upload-api
/**
 * @param {import('@ucanto/interface').Invocation} concludeFx
 */
function getConcludeReceipt(concludeFx) {
  const receiptBlocks = new Map();
  for (const block of concludeFx.iterateIPLDBlocks()) {
    receiptBlocks.set(`${block.cid}`, block);
  }
  return view$1({
    // @ts-expect-error object of type unknown
    root: concludeFx.capabilities[0].nb.receipt,
    blocks: receiptBlocks,
  });
}
// FIXME this code has been copied over from upload-api
/**
 * @param {import('@ucanto/interface').Receipt} receipt
 */
function parseBlobAddReceiptNext(receipt) {
  // Get invocations next
  /**
   * @type {import('@ucanto/interface').Invocation[]}
   */
  // @ts-expect-error read only effect
  const forkInvocations = receipt.fx.fork;
  const allocateTask = forkInvocations.find(
    (fork) => fork.capabilities[0].can === allocate.can
  );
  const concludefxs = forkInvocations.filter(
    (fork) => fork.capabilities[0].can === conclude$1.can
  );
  const putTask = forkInvocations.find(
    (fork) => fork.capabilities[0].can === put.can
  );
  const acceptTask = forkInvocations.find(
    (fork) => fork.capabilities[0].can === accept.can
  );
  /* c8 ignore next 3 */
  if (!allocateTask || !concludefxs.length || !putTask || !acceptTask) {
    throw new Error("mandatory effects not received");
  }
  // Decode receipts available
  const nextReceipts = concludefxs.map((fx) => getConcludeReceipt(fx));
  /** @type {import('@ucanto/interface').Receipt<import('../types.js').BlobAllocateSuccess, import('../types.js').BlobAllocateFailure> | undefined} */
  // @ts-expect-error types unknown for next
  const allocateReceipt = nextReceipts.find((receipt) =>
    receipt.ran.link().equals(allocateTask.cid)
  );
  /** @type {import('@ucanto/interface').Receipt<{}, import('@ucanto/interface').Failure> | undefined} */
  // @ts-expect-error types unknown for next
  const putReceipt = nextReceipts.find((receipt) =>
    receipt.ran.link().equals(putTask.cid)
  );
  /** @type {import('@ucanto/interface').Receipt<import('../types.js').BlobAcceptSuccess, import('../types.js').BlobAcceptFailure> | undefined} */
  // @ts-expect-error types unknown for next
  const acceptReceipt = nextReceipts.find((receipt) =>
    receipt.ran.link().equals(acceptTask.cid)
  );
  /* c8 ignore next 3 */
  if (!allocateReceipt) {
    throw new Error("mandatory effects not received");
  }
  return {
    allocate: {
      task: allocateTask,
      receipt: allocateReceipt,
    },
    put: {
      task: putTask,
      receipt: putReceipt,
    },
    accept: {
      task: acceptTask,
      receipt: acceptReceipt,
    },
  };
}
// FIXME this code has been copied over from upload-api
/**
 * @param {import('@ucanto/interface').Signer} id
 * @param {import('@ucanto/interface').Principal} serviceDid
 * @param {import('@ucanto/interface').Receipt} receipt
 */
function createConcludeInvocation(id, serviceDid, receipt) {
  const receiptBlocks = [];
  const receiptCids = [];
  for (const block of receipt.iterateIPLDBlocks()) {
    receiptBlocks.push(block);
    receiptCids.push(block.cid);
  }
  const concludeAllocatefx = conclude$1.invoke({
    issuer: id,
    audience: serviceDid,
    with: id.toDIDKey(),
    nb: {
      receipt: receipt.link(),
    },
    expiration: Infinity,
    facts: [
      {
        ...receiptCids,
      },
    ],
  });
  for (const block of receiptBlocks) {
    concludeAllocatefx.attach(block);
  }
  return concludeAllocatefx;
}
/**
 * Store a blob to the service. The issuer needs the `blob/add`
 * delegated capability.
 *
 * Required delegated capability proofs: `blob/add`
 *
 * @param {import('../types.js').InvocationConfig} conf Configuration
 * for the UCAN invocation. An object with `issuer`, `with` and `proofs`.
 *
 * The `issuer` is the signing authority that is issuing the UCAN
 * invocation(s). It is typically the user _agent_.
 *
 * The `with` is the resource the invocation applies to. It is typically the
 * DID of a space.
 *
 * The `proofs` are a set of capability delegations that prove the issuer
 * has the capability to perform the action.
 *
 * The issuer needs the `blob/add` delegated capability.
 * @param {import('multiformats').MultihashDigest} digest
 * @param {Blob|Uint8Array} data Blob data.
 * @param {import('../types.js').RequestOptions} [options]
 * @returns {Promise<import('../types.js').BlobAddOk>}
 */
async function add$2(
  { issuer, with: resource, proofs, audience },
  digest,
  data,
  options = {}
) {
  /* c8 ignore next 2 */
  const bytes =
    data instanceof Uint8Array
      ? data
      : new Uint8Array(await data.arrayBuffer());
  const size = bytes.length;
  /* c8 ignore next */
  const conn = options.connection ?? connection;
  const result = await pRetry(
    async () => {
      return await add$4
        .invoke({
          issuer,
          /* c8 ignore next */
          audience: audience ?? servicePrincipal,
          with: SpaceDID$1.from(resource),
          nb: input$8(digest, size),
          proofs,
          nonce: options.nonce,
        })
        .execute(conn);
    },
    {
      onFailedAttempt: console.warn,
      retries: options.retries ?? REQUEST_RETRIES,
    }
  );
  if (!result.out.ok) {
    throw new Error(`failed ${add$4.can} invocation`, {
      cause: result.out.error,
    });
  }
  const nextTasks = parseBlobAddReceiptNext(result);
  const { receipt: allocateReceipt } = nextTasks.allocate;
  /* c8 ignore next 5 */
  if (!allocateReceipt.out.ok) {
    throw new Error(`failed ${add$4.can} invocation`, {
      cause: allocateReceipt.out.error,
    });
  }
  const { address } = allocateReceipt.out.ok;
  if (address) {
    const fetchWithUploadProgress =
      options.fetchWithUploadProgress ||
      options.fetch ||
      globalThis.fetch.bind(globalThis);
    let fetchDidCallUploadProgressCb = false;
    await pRetry(
      async () => {
        try {
          const res = await fetchWithUploadProgress(address.url, {
            method: "PUT",
            ...(!isCloudflareWorkers && { mode: "cors" }),
            body: bytes,
            headers: address.headers,
            signal: options.signal,
            onUploadProgress: (status) => {
              fetchDidCallUploadProgressCb = true;
              if (options.onUploadProgress)
                createUploadProgressHandler(
                  address.url,
                  options.onUploadProgress
                )(status);
            },
            // @ts-expect-error - this is needed by recent versions of node - see https://github.com/bluesky-social/atproto/pull/470 for more info
            duplex: "half",
          });
          // do not retry client errors
          if (res.status >= 400 && res.status < 500) {
            throw new AbortError(`upload failed: ${res.status}`);
          }
          if (!res.ok) {
            throw new Error(`upload failed: ${res.status}`);
          }
        } catch (err) {
          if (options.signal?.aborted === true) {
            throw new AbortError("upload aborted");
          }
          throw err;
        }
      },
      {
        retries: options.retries ?? REQUEST_RETRIES,
      }
    );
    if (!fetchDidCallUploadProgressCb && options.onUploadProgress) {
      // the fetch implementation didn't support onUploadProgress
      const blob = new Blob([bytes]);
      options.onUploadProgress({
        total: blob.size,
        loaded: blob.size,
        lengthComputable: false,
      });
    }
  }
  // Invoke `conclude` with `http/put` receipt
  let { receipt: httpPutReceipt } = nextTasks.put;
  if (!httpPutReceipt?.out.ok) {
    const derivedSigner = from$b(
      /** @type {import('@ucanto/interface').SignerArchive<import('@ucanto/interface').DID, typeof ed25519.signatureCode>} */
      (nextTasks.put.task.facts[0]["keys"])
    );
    httpPutReceipt = await issue$1({
      issuer: derivedSigner,
      ran: nextTasks.put.task.cid,
      result: { ok: {} },
    });
    const httpPutConcludeInvocation = createConcludeInvocation(
      issuer,
      /* c8 ignore next */
      audience ?? servicePrincipal,
      httpPutReceipt
    );
    const ucanConclude = await httpPutConcludeInvocation.execute(conn);
    if (!ucanConclude.out.ok) {
      throw new Error(`failed ${add$4.can} invocation`, {
        cause: result.out.error,
      });
    }
  }
  // Ensure the blob has been accepted
  let { receipt: acceptReceipt } = nextTasks.accept;
  if (!acceptReceipt?.out.ok) {
    acceptReceipt = await poll(nextTasks.accept.task.link(), options);
  }
  const blocks = new Map(
    [...acceptReceipt.iterateIPLDBlocks()].map((block) => [
      `${block.cid}`,
      block,
    ])
  );
  const site = view$3({
    root: /** @type {import('@ucanto/interface').UCANLink} */ (
      acceptReceipt.out.ok?.site
    ),
    blocks,
  });
  return { site };
}
/** Returns the ability used by an invocation. */
const ability$2 = add$4.can;
/**
 * Returns required input to the invocation.
 *
 * @param {import('multiformats').MultihashDigest} digest
 * @param {number} size
 */
const input$8 = (digest, size) => ({
  blob: {
    digest: digest.bytes,
    size,
  },
});

/**
 * Gets a stored Blob file by digest.
 *
 * @param {import('../types.js').InvocationConfig} conf Configuration
 * for the UCAN invocation. An object with `issuer`, `with` and `proofs`.
 *
 * The `issuer` is the signing authority that is issuing the UCAN
 * invocation(s). It is typically the user _agent_.
 *
 * The `with` is the resource the invocation applies to. It is typically the
 * DID of a space.
 *
 * The `proofs` are a set of capability delegations that prove the issuer
 * has the capability to perform the action.
 *
 * The issuer needs the `blob/get/0/1` delegated capability.
 * @param {import('multiformats').MultihashDigest} multihash of the blob
 * @param {import('../types.js').RequestOptions} [options]
 */
async function get$8(
  { issuer, with: resource, proofs, audience },
  multihash,
  options = {}
) {
  /* c8 ignore next */
  const conn = options.connection ?? connection;
  const result = await get$b
    .invoke({
      issuer,
      /* c8 ignore next */
      audience: audience ?? servicePrincipal,
      with: SpaceDID$1.from(resource),
      nb: input$7(multihash),
      proofs,
      nonce: options.nonce,
    })
    .execute(conn);
  if (!result.out.ok) {
    throw new Error(`failed ${get$b.can} invocation`, {
      cause: result.out.error,
    });
  }
  return result.out;
}
/** Returns the ability used by an invocation. */
get$b.can;
/**
 * Returns required input to the invocation.
 *
 * @param {import('multiformats').MultihashDigest} digest
 */
const input$7 = (digest) => ({ digest: digest.bytes });

/**
 * List Blobs stored in the space.
 *
 * @param {import('../types.js').InvocationConfig} conf Configuration
 * for the UCAN invocation. An object with `issuer`, `with` and `proofs`.
 *
 * The `issuer` is the signing authority that is issuing the UCAN
 * invocation(s). It is typically the user _agent_.
 *
 * The `with` is the resource the invocation applies to. It is typically the
 * DID of a space.
 *
 * The `proofs` are a set of capability delegations that prove the issuer
 * has the capability to perform the action.
 *
 * The issuer needs the `blob/list` delegated capability.
 * @param {import('../types.js').ListRequestOptions} [options]
 * @returns {Promise<import('../types.js').BlobListSuccess>}
 */
async function list$3(
  { issuer, with: resource, proofs, audience },
  options = {}
) {
  /* c8 ignore next */
  const conn = options.connection ?? connection;
  const result = await list$5
    .invoke({
      issuer,
      /* c8 ignore next */
      audience: audience ?? servicePrincipal,
      with: SpaceDID$1.from(resource),
      proofs,
      nb: input$6(options.cursor, options.size),
      nonce: options.nonce,
    })
    .execute(conn);
  if (!result.out.ok) {
    throw new Error(`failed ${list$5.can} invocation`, {
      cause: result.out.error,
    });
  }
  return result.out.ok;
}
/** Returns the ability used by an invocation. */
list$5.can;
/**
 * Returns required input to the invocation.
 *
 * @param {string} [cursor]
 * @param {number} [size]
 */
const input$6 = (cursor, size) => ({ cursor, size });

/**
 * Remove a stored Blob file by digest.
 *
 * @param {import('../types.js').InvocationConfig} conf Configuration
 * for the UCAN invocation. An object with `issuer`, `with` and `proofs`.
 *
 * The `issuer` is the signing authority that is issuing the UCAN
 * invocation(s). It is typically the user _agent_.
 *
 * The `with` is the resource the invocation applies to. It is typically the
 * DID of a space.
 *
 * The `proofs` are a set of capability delegations that prove the issuer
 * has the capability to perform the action.
 *
 * The issuer needs the `blob/remove` delegated capability.
 * @param {import('multiformats').MultihashDigest} multihash of the blob
 * @param {import('../types.js').RequestOptions} [options]
 */
async function remove$3(
  { issuer, with: resource, proofs, audience },
  multihash,
  options = {}
) {
  /* c8 ignore next */
  const conn = options.connection ?? connection;
  const result = await remove$5
    .invoke({
      issuer,
      /* c8 ignore next */
      audience: audience ?? servicePrincipal,
      with: SpaceDID$1.from(resource),
      nb: input$5(multihash),
      proofs,
      nonce: options.nonce,
    })
    .execute(conn);
  if (!result.out.ok) {
    throw new Error(`failed ${remove$5.can} invocation`, {
      cause: result.out.error,
    });
  }
  return result.out;
}
/** Returns the ability used by an invocation. */
remove$5.can;
/**
 * Returns required input to the invocation.
 *
 * @param {import('multiformats').MultihashDigest} digest
 */
const input$5 = (digest) => ({ digest: digest.bytes });

/**
 * Register an "index" with the service. The issuer needs the `index/add`
 * delegated capability.
 *
 * Required delegated capability proofs: `index/add`
 *
 * @param {import('../types.js').InvocationConfig} conf Configuration
 * for the UCAN invocation. An object with `issuer`, `with` and `proofs`.
 *
 * The `issuer` is the signing authority that is issuing the UCAN
 * invocation(s). It is typically the user _agent_.
 *
 * The `with` is the resource the invocation applies to. It is typically the
 * DID of a space.
 *
 * The `proofs` are a set of capability delegations that prove the issuer
 * has the capability to perform the action.
 *
 * The issuer needs the `index/add` delegated capability.
 * @param {import('../types.js').CARLink} index Index to store.
 * @param {import('../types.js').RequestOptions} [options]
 * @returns {Promise<import('../types.js').IndexAddSuccess>}
 */
async function add$1(
  { issuer, with: resource, proofs, audience },
  index,
  options = {}
) {
  /* c8 ignore next */
  const conn = options.connection ?? connection;
  const result = await pRetry(
    async () => {
      return await add$5
        .invoke({
          issuer,
          /* c8 ignore next */
          audience: audience ?? servicePrincipal,
          with: SpaceDID$1.from(resource),
          nb: input$4(index),
          proofs,
        })
        .execute(conn);
    },
    {
      onFailedAttempt: console.warn,
      retries: options.retries ?? REQUEST_RETRIES,
    }
  );
  if (!result.out.ok) {
    throw new Error(`failed ${add$5.can} invocation`, {
      cause: result.out.error,
    });
  }
  return result.out.ok;
}
/** Returns the ability used by an invocation. */
const ability$1 = add$5.can;
/**
 * Returns required input to the invocation.
 *
 * @param {import('../types.js').CARLink} index
 */
const input$4 = (index) => ({ index });

/**
 * Register an "upload" with the service. The issuer needs the `upload/add`
 * delegated capability.
 *
 * Required delegated capability proofs: `upload/add`
 *
 * @param {import('../types.js').InvocationConfig} conf Configuration
 * for the UCAN invocation. An object with `issuer`, `with` and `proofs`.
 *
 * The `issuer` is the signing authority that is issuing the UCAN
 * invocation(s). It is typically the user _agent_.
 *
 * The `with` is the resource the invocation applies to. It is typically the
 * DID of a space.
 *
 * The `proofs` are a set of capability delegations that prove the issuer
 * has the capability to perform the action.
 *
 * The issuer needs the `upload/add` delegated capability.
 * @param {import('multiformats/link').UnknownLink} root Root data CID for the DAG that was stored.
 * @param {import('../types.js').CARLink[]} shards CIDs of CAR files that contain the DAG.
 * @param {import('../types.js').RequestOptions} [options]
 * @returns {Promise<import('../types.js').UploadAddSuccess>}
 */
async function add(
  { issuer, with: resource, proofs, audience },
  root,
  shards,
  options = {}
) {
  /* c8 ignore next */
  const conn = options.connection ?? connection;
  const result = await pRetry(
    async () => {
      return await add$9
        .invoke({
          issuer,
          /* c8 ignore next */
          audience: audience ?? servicePrincipal,
          with: SpaceDID$1.from(resource),
          nb: input$3(root, shards),
          proofs,
          nonce: options.nonce,
        })
        .execute(conn);
    },
    {
      onFailedAttempt: console.warn,
      retries: options.retries ?? REQUEST_RETRIES,
    }
  );
  if (!result.out.ok) {
    throw new Error(`failed ${add$9.can} invocation`, {
      cause: result.out.error,
    });
  }
  return result.out.ok;
}
/** Returns the ability used by an invocation. */
const ability = add$9.can;
/**
 * Returns required input to the invocation.
 *
 * @param {import('multiformats/link').UnknownLink} root
 * @param {import('../types.js').CARLink[]} shards
 */
const input$3 = (root, shards) => ({ root, shards });

/**
 * Get details of an "upload".
 *
 * Required delegated capability proofs: `upload/get`
 *
 * @param {import('../types.js').InvocationConfig} conf Configuration
 * for the UCAN invocation. An object with `issuer`, `with` and `proofs`.
 *
 * The `issuer` is the signing authority that is issuing the UCAN
 * invocation(s). It is typically the user _agent_.
 *
 * The `with` is the resource the invocation applies to. It is typically the
 * DID of a space.
 *
 * The `proofs` are a set of capability delegations that prove the issuer
 * has the capability to perform the action.
 *
 * The issuer needs the `upload/get` delegated capability.
 * @param {import('multiformats/link').UnknownLink} root Root data CID for the DAG that was stored.
 * @param {import('../types.js').RequestOptions} [options]
 * @returns {Promise<import('../types.js').UploadGetSuccess>}
 */
async function get$7(
  { issuer, with: resource, proofs, audience },
  root,
  options = {}
) {
  /* c8 ignore next */
  const conn = options.connection ?? connection;
  const result = await pRetry(
    async () => {
      return await get$g
        .invoke({
          issuer,
          /* c8 ignore next */
          audience: audience ?? servicePrincipal,
          with: SpaceDID$1.from(resource),
          nb: input$2(root),
          proofs,
          nonce: options.nonce,
        })
        .execute(conn);
    },
    {
      onFailedAttempt: console.warn,
      retries: options.retries ?? REQUEST_RETRIES,
    }
  );
  if (!result.out.ok) {
    throw new Error(`failed ${get$g.can} invocation`, {
      cause: result.out.error,
    });
  }
  return result.out.ok;
}
/** Returns the ability used by an invocation. */
get$g.can;
/**
 * Returns required input to the invocation.
 *
 * @param {import('multiformats/link').UnknownLink} root
 */
const input$2 = (root) => ({ root });

/**
 * List uploads created by the issuer.
 *
 * @param {import('../types.js').InvocationConfig} conf Configuration
 * for the UCAN invocation. An object with `issuer`, `with` and `proofs`.
 *
 * The `issuer` is the signing authority that is issuing the UCAN
 * invocation(s). It is typically the user _agent_.
 *
 * The `with` is the resource the invocation applies to. It is typically the
 * DID of a space.
 *
 * The `proofs` are a set of capability delegations that prove the issuer
 * has the capability to perform the action.
 *
 * The issuer needs the `upload/list` delegated capability.
 * @param {import('../types.js').ListRequestOptions} [options]
 * @returns {Promise<import('../types.js').UploadListSuccess>}
 */
async function list$2(
  { issuer, with: resource, proofs, audience },
  options = {}
) {
  /* c8 ignore next */
  const conn = options.connection ?? connection;
  const result = await list$8
    .invoke({
      issuer,
      /* c8 ignore next */
      audience: audience ?? servicePrincipal,
      with: SpaceDID$1.from(resource),
      proofs,
      nb: input$1(options.cursor, options.size, options.pre),
      nonce: options.nonce,
    })
    .execute(conn);
  if (!result.out.ok) {
    throw new Error(`failed ${list$8.can} invocation`, {
      cause: result.out.error,
    });
  }
  return result.out.ok;
}
/** Returns the ability used by an invocation. */
list$8.can;
/**
 * Returns required input to the invocation.
 *
 * @param {string} [cursor]
 * @param {number} [size]
 * @param {boolean} [pre]
 */
const input$1 = (cursor, size, pre) => ({ cursor, size, pre });

/**
 * Remove an upload by root data CID.
 *
 * @param {import('../types.js').InvocationConfig} conf Configuration
 * for the UCAN invocation. An object with `issuer`, `with` and `proofs`.
 *
 * The `issuer` is the signing authority that is issuing the UCAN
 * invocation(s). It is typically the user _agent_.
 *
 * The `with` is the resource the invocation applies to. It is typically the
 * DID of a space.
 *
 * The `proofs` are a set of capability delegations that prove the issuer
 * has the capability to perform the action.
 *
 * The issuer needs the `upload/remove` delegated capability.
 * @param {import('multiformats').UnknownLink} root Root data CID to remove.
 * @param {import('../types.js').RequestOptions} [options]
 */
async function remove$2(
  { issuer, with: resource, proofs, audience },
  root,
  options = {}
) {
  /* c8 ignore next */
  const conn = options.connection ?? connection;
  const result = await remove$7
    .invoke({
      issuer,
      /* c8 ignore next */
      audience: audience ?? servicePrincipal,
      with: SpaceDID$1.from(resource),
      nb: input(root),
      proofs,
      nonce: options.nonce,
    })
    .execute(conn);
  if (!result.out.ok) {
    throw new Error(`failed ${remove$7.can} invocation`, {
      cause: result.out.error,
    });
  }
  return result.out.ok;
}
/** Returns the ability used by an invocation. */
remove$7.can;
/**
 * Returns required input to the invocation.
 *
 * @param {import('multiformats').UnknownLink} root
 */
const input = (root) => ({ root });

const textDecoder = new TextDecoder();

/**
 * @typedef {import('./interface.js').RawPBLink} RawPBLink
 */

/**
 * @typedef {import('./interface.js').RawPBNode} RawPBNode
 */

/**
 * @param {Uint8Array} bytes
 * @param {number} offset
 * @returns {[number, number]}
 */
function decodeVarint(bytes, offset) {
  let v = 0;

  for (let shift = 0; ; shift += 7) {
    /* c8 ignore next 3 */
    if (shift >= 64) {
      throw new Error("protobuf: varint overflow");
    }
    /* c8 ignore next 3 */
    if (offset >= bytes.length) {
      throw new Error("protobuf: unexpected end of data");
    }

    const b = bytes[offset++];
    v += shift < 28 ? (b & 0x7f) << shift : (b & 0x7f) * 2 ** shift;
    if (b < 0x80) {
      break;
    }
  }
  return [v, offset];
}

/**
 * @param {Uint8Array} bytes
 * @param {number} offset
 * @returns {[Uint8Array, number]}
 */
function decodeBytes(bytes, offset) {
  let byteLen;
  [byteLen, offset] = decodeVarint(bytes, offset);
  const postOffset = offset + byteLen;

  /* c8 ignore next 3 */
  if (byteLen < 0 || postOffset < 0) {
    throw new Error("protobuf: invalid length");
  }
  /* c8 ignore next 3 */
  if (postOffset > bytes.length) {
    throw new Error("protobuf: unexpected end of data");
  }

  return [bytes.subarray(offset, postOffset), postOffset];
}

/**
 * @param {Uint8Array} bytes
 * @param {number} index
 * @returns {[number, number, number]}
 */
function decodeKey(bytes, index) {
  let wire;
  [wire, index] = decodeVarint(bytes, index);
  // [wireType, fieldNum, newIndex]
  return [wire & 0x7, wire >> 3, index];
}

/**
 * @param {Uint8Array} bytes
 * @returns {RawPBLink}
 */
function decodeLink(bytes) {
  /** @type {RawPBLink} */
  const link = {};
  const l = bytes.length;
  let index = 0;

  while (index < l) {
    let wireType, fieldNum;
    [wireType, fieldNum, index] = decodeKey(bytes, index);

    if (fieldNum === 1) {
      if (link.Hash) {
        throw new Error("protobuf: (PBLink) duplicate Hash section");
      }
      if (wireType !== 2) {
        throw new Error(
          `protobuf: (PBLink) wrong wireType (${wireType}) for Hash`
        );
      }
      if (link.Name !== undefined) {
        throw new Error(
          "protobuf: (PBLink) invalid order, found Name before Hash"
        );
      }
      if (link.Tsize !== undefined) {
        throw new Error(
          "protobuf: (PBLink) invalid order, found Tsize before Hash"
        );
      }

      [link.Hash, index] = decodeBytes(bytes, index);
    } else if (fieldNum === 2) {
      if (link.Name !== undefined) {
        throw new Error("protobuf: (PBLink) duplicate Name section");
      }
      if (wireType !== 2) {
        throw new Error(
          `protobuf: (PBLink) wrong wireType (${wireType}) for Name`
        );
      }
      if (link.Tsize !== undefined) {
        throw new Error(
          "protobuf: (PBLink) invalid order, found Tsize before Name"
        );
      }

      let byts;
      [byts, index] = decodeBytes(bytes, index);
      link.Name = textDecoder.decode(byts);
    } else if (fieldNum === 3) {
      if (link.Tsize !== undefined) {
        throw new Error("protobuf: (PBLink) duplicate Tsize section");
      }
      if (wireType !== 0) {
        throw new Error(
          `protobuf: (PBLink) wrong wireType (${wireType}) for Tsize`
        );
      }

      [link.Tsize, index] = decodeVarint(bytes, index);
    } else {
      throw new Error(
        `protobuf: (PBLink) invalid fieldNumber, expected 1, 2 or 3, got ${fieldNum}`
      );
    }
  }

  /* c8 ignore next 3 */
  if (index > l) {
    throw new Error("protobuf: (PBLink) unexpected end of data");
  }

  return link;
}

/**
 * @param {Uint8Array} bytes
 * @returns {RawPBNode}
 */
function decodeNode(bytes) {
  const l = bytes.length;
  let index = 0;
  /** @type {RawPBLink[]|void} */
  let links = undefined; // eslint-disable-line no-undef-init
  let linksBeforeData = false;
  /** @type {Uint8Array|void} */
  let data = undefined; // eslint-disable-line no-undef-init

  while (index < l) {
    let wireType, fieldNum;
    [wireType, fieldNum, index] = decodeKey(bytes, index);

    if (wireType !== 2) {
      throw new Error(
        `protobuf: (PBNode) invalid wireType, expected 2, got ${wireType}`
      );
    }

    if (fieldNum === 1) {
      if (data) {
        throw new Error("protobuf: (PBNode) duplicate Data section");
      }

      [data, index] = decodeBytes(bytes, index);
      if (links) {
        linksBeforeData = true;
      }
    } else if (fieldNum === 2) {
      if (linksBeforeData) {
        // interleaved Links/Data/Links
        throw new Error("protobuf: (PBNode) duplicate Links section");
      } else if (!links) {
        links = [];
      }
      let byts;
      [byts, index] = decodeBytes(bytes, index);
      links.push(decodeLink(byts));
    } else {
      throw new Error(
        `protobuf: (PBNode) invalid fieldNumber, expected 1 or 2, got ${fieldNum}`
      );
    }
  }

  /* c8 ignore next 3 */
  if (index > l) {
    throw new Error("protobuf: (PBNode) unexpected end of data");
  }

  /** @type {RawPBNode} */
  const node = {};
  if (data) {
    node.Data = data;
  }
  node.Links = links || [];
  return node;
}

const textEncoder$1 = new TextEncoder();
const maxInt32 = 2 ** 32;
const maxUInt32 = 2 ** 31;

/**
 * @typedef {import('./interface.js').RawPBLink} RawPBLink
 */

/**
 * @typedef {import('./interface.js').RawPBNode} RawPBNode
 */

// the encoders work backward from the end of the bytes array

/**
 * encodeLink() is passed a slice of the parent byte array that ends where this
 * link needs to end, so it packs to the right-most part of the passed `bytes`
 *
 * @param {RawPBLink} link
 * @param {Uint8Array} bytes
 * @returns {number}
 */
function encodeLink$1(link, bytes) {
  let i = bytes.length;

  if (typeof link.Tsize === "number") {
    if (link.Tsize < 0) {
      throw new Error("Tsize cannot be negative");
    }
    if (!Number.isSafeInteger(link.Tsize)) {
      throw new Error("Tsize too large for encoding");
    }
    i = encodeVarint(bytes, i, link.Tsize) - 1;
    bytes[i] = 0x18;
  }

  if (typeof link.Name === "string") {
    const nameBytes = textEncoder$1.encode(link.Name);
    i -= nameBytes.length;
    bytes.set(nameBytes, i);
    i = encodeVarint(bytes, i, nameBytes.length) - 1;
    bytes[i] = 0x12;
  }

  if (link.Hash) {
    i -= link.Hash.length;
    bytes.set(link.Hash, i);
    i = encodeVarint(bytes, i, link.Hash.length) - 1;
    bytes[i] = 0xa;
  }

  return bytes.length - i;
}

/**
 * Encodes a PBNode into a new byte array of precisely the correct size
 *
 * @param {RawPBNode} node
 * @returns {Uint8Array}
 */
function encodeNode(node) {
  const size = sizeNode(node);
  const bytes = new Uint8Array(size);
  let i = size;

  if (node.Data) {
    i -= node.Data.length;
    bytes.set(node.Data, i);
    i = encodeVarint(bytes, i, node.Data.length) - 1;
    bytes[i] = 0xa;
  }

  if (node.Links) {
    for (let index = node.Links.length - 1; index >= 0; index--) {
      const size = encodeLink$1(node.Links[index], bytes.subarray(0, i));
      i -= size;
      i = encodeVarint(bytes, i, size) - 1;
      bytes[i] = 0x12;
    }
  }

  return bytes;
}

/**
 * work out exactly how many bytes this link takes up
 *
 * @param {RawPBLink} link
 * @returns
 */
function sizeLink(link) {
  let n = 0;

  if (link.Hash) {
    const l = link.Hash.length;
    n += 1 + l + sov(l);
  }

  if (typeof link.Name === "string") {
    const l = textEncoder$1.encode(link.Name).length;
    n += 1 + l + sov(l);
  }

  if (typeof link.Tsize === "number") {
    n += 1 + sov(link.Tsize);
  }

  return n;
}

/**
 * Work out exactly how many bytes this node takes up
 *
 * @param {RawPBNode} node
 * @returns {number}
 */
function sizeNode(node) {
  let n = 0;

  if (node.Data) {
    const l = node.Data.length;
    n += 1 + l + sov(l);
  }

  if (node.Links) {
    for (const link of node.Links) {
      const l = sizeLink(link);
      n += 1 + l + sov(l);
    }
  }

  return n;
}

/**
 * @param {Uint8Array} bytes
 * @param {number} offset
 * @param {number} v
 * @returns {number}
 */
function encodeVarint(bytes, offset, v) {
  offset -= sov(v);
  const base = offset;

  while (v >= maxUInt32) {
    bytes[offset++] = (v & 0x7f) | 0x80;
    v /= 128;
  }

  while (v >= 128) {
    bytes[offset++] = (v & 0x7f) | 0x80;
    v >>>= 7;
  }

  bytes[offset] = v;

  return base;
}

/**
 * size of varint
 *
 * @param {number} x
 * @returns {number}
 */
function sov(x) {
  if (x % 2 === 0) {
    x++;
  }
  return Math.floor((len64(x) + 6) / 7);
}

/**
 * golang math/bits, how many bits does it take to represent this integer?
 *
 * @param {number} x
 * @returns {number}
 */
function len64(x) {
  let n = 0;
  if (x >= maxInt32) {
    x = Math.floor(x / maxInt32);
    n = 32;
  }
  if (x >= 1 << 16) {
    x >>>= 16;
    n += 16;
  }
  if (x >= 1 << 8) {
    x >>>= 8;
    n += 8;
  }
  return n + len8tab[x];
}

// golang math/bits
const len8tab = [
  0, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
  5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
  6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
  7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
  7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8,
  8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
  8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
  8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
  8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
  8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,
];

/**
 * @typedef {import('./interface.js').PBLink} PBLink
 * @typedef {import('./interface.js').PBNode} PBNode
 */

/**
 * @template T
 * @typedef {import('multiformats/codecs/interface').ByteView<T>} ByteView
 */

/**
 * @template T
 * @typedef {import('multiformats/codecs/interface').ArrayBufferView<T>} ArrayBufferView
 */

const pbNodeProperties = ["Data", "Links"];
const pbLinkProperties = ["Hash", "Name", "Tsize"];

const textEncoder = new TextEncoder();

/**
 * @param {PBLink} a
 * @param {PBLink} b
 * @returns {number}
 */
function linkComparator(a, b) {
  if (a === b) {
    return 0;
  }

  const abuf = a.Name ? textEncoder.encode(a.Name) : [];
  const bbuf = b.Name ? textEncoder.encode(b.Name) : [];

  let x = abuf.length;
  let y = bbuf.length;

  for (let i = 0, len = Math.min(x, y); i < len; ++i) {
    if (abuf[i] !== bbuf[i]) {
      x = abuf[i];
      y = bbuf[i];
      break;
    }
  }

  return x < y ? -1 : y < x ? 1 : 0;
}

/**
 * @param {any} node
 * @param {string[]} properties
 * @returns {boolean}
 */
function hasOnlyProperties(node, properties) {
  return !Object.keys(node).some((p) => !properties.includes(p));
}

/**
 * Converts a CID, or a PBLink-like object to a PBLink
 *
 * @param {any} link
 * @returns {PBLink}
 */
function asLink(link) {
  if (typeof link.asCID === "object") {
    const Hash = CID$2.asCID(link);
    if (!Hash) {
      throw new TypeError("Invalid DAG-PB form");
    }
    return { Hash };
  }

  if (typeof link !== "object" || Array.isArray(link)) {
    throw new TypeError("Invalid DAG-PB form");
  }

  const pbl = {};

  if (link.Hash) {
    let cid = CID$2.asCID(link.Hash);
    try {
      if (!cid) {
        if (typeof link.Hash === "string") {
          cid = CID$2.parse(link.Hash);
        } else if (link.Hash instanceof Uint8Array) {
          cid = CID$2.decode(link.Hash);
        }
      }
    } catch (/** @type {any} */ e) {
      throw new TypeError(`Invalid DAG-PB form: ${e.message}`);
    }

    if (cid) {
      pbl.Hash = cid;
    }
  }

  if (!pbl.Hash) {
    throw new TypeError("Invalid DAG-PB form");
  }

  if (typeof link.Name === "string") {
    pbl.Name = link.Name;
  }

  if (typeof link.Tsize === "number") {
    pbl.Tsize = link.Tsize;
  }

  return pbl;
}

/**
 * @param {any} node
 * @returns {PBNode}
 */
function prepare(node) {
  if (node instanceof Uint8Array || typeof node === "string") {
    node = { Data: node };
  }

  if (typeof node !== "object" || Array.isArray(node)) {
    throw new TypeError("Invalid DAG-PB form");
  }

  /** @type {PBNode} */
  const pbn = {};

  if (node.Data !== undefined) {
    if (typeof node.Data === "string") {
      pbn.Data = textEncoder.encode(node.Data);
    } else if (node.Data instanceof Uint8Array) {
      pbn.Data = node.Data;
    } else {
      throw new TypeError("Invalid DAG-PB form");
    }
  }

  if (node.Links !== undefined) {
    if (Array.isArray(node.Links)) {
      pbn.Links = node.Links.map(asLink);
      pbn.Links.sort(linkComparator);
    } else {
      throw new TypeError("Invalid DAG-PB form");
    }
  } else {
    pbn.Links = [];
  }

  return pbn;
}

/**
 * @param {PBNode} node
 */
function validate(node) {
  /*
  type PBLink struct {
    Hash optional Link
    Name optional String
    Tsize optional Int
  }

  type PBNode struct {
    Links [PBLink]
    Data optional Bytes
  }
  */
  // @ts-ignore private property for TS
  if (
    !node ||
    typeof node !== "object" ||
    Array.isArray(node) ||
    node instanceof Uint8Array ||
    (node["/"] && node["/"] === node.bytes)
  ) {
    throw new TypeError("Invalid DAG-PB form");
  }

  if (!hasOnlyProperties(node, pbNodeProperties)) {
    throw new TypeError("Invalid DAG-PB form (extraneous properties)");
  }

  if (node.Data !== undefined && !(node.Data instanceof Uint8Array)) {
    throw new TypeError("Invalid DAG-PB form (Data must be bytes)");
  }

  if (!Array.isArray(node.Links)) {
    throw new TypeError("Invalid DAG-PB form (Links must be a list)");
  }

  for (let i = 0; i < node.Links.length; i++) {
    const link = node.Links[i];
    // @ts-ignore private property for TS
    if (
      !link ||
      typeof link !== "object" ||
      Array.isArray(link) ||
      link instanceof Uint8Array ||
      (link["/"] && link["/"] === link.bytes)
    ) {
      throw new TypeError("Invalid DAG-PB form (bad link)");
    }

    if (!hasOnlyProperties(link, pbLinkProperties)) {
      throw new TypeError(
        "Invalid DAG-PB form (extraneous properties on link)"
      );
    }

    if (link.Hash === undefined) {
      throw new TypeError("Invalid DAG-PB form (link must have a Hash)");
    }

    // @ts-ignore private property for TS
    if (
      link.Hash == null ||
      !link.Hash["/"] ||
      link.Hash["/"] !== link.Hash.bytes
    ) {
      throw new TypeError("Invalid DAG-PB form (link Hash must be a CID)");
    }

    if (link.Name !== undefined && typeof link.Name !== "string") {
      throw new TypeError("Invalid DAG-PB form (link Name must be a string)");
    }

    if (link.Tsize !== undefined) {
      if (typeof link.Tsize !== "number" || link.Tsize % 1 !== 0) {
        throw new TypeError(
          "Invalid DAG-PB form (link Tsize must be an integer)"
        );
      }
      if (link.Tsize < 0) {
        throw new TypeError(
          "Invalid DAG-PB form (link Tsize cannot be negative)"
        );
      }
    }

    if (i > 0 && linkComparator(link, node.Links[i - 1]) === -1) {
      throw new TypeError(
        "Invalid DAG-PB form (links must be sorted by Name bytes)"
      );
    }
  }
}

/**
 * @template T
 * @param {ByteView<T> | ArrayBufferView<T>} buf
 * @returns {ByteView<T>}
 */
function toByteView(buf) {
  if (buf instanceof ArrayBuffer) {
    return new Uint8Array(buf, 0, buf.byteLength);
  }

  return buf;
}

const code$2 = 0x70;

/**
 * @param {PBNode} node
 * @returns {ByteView<PBNode>}
 */
function encode$6(node) {
  validate(node);

  const pbn = {};
  if (node.Links) {
    pbn.Links = node.Links.map((l) => {
      const link = {};
      if (l.Hash) {
        link.Hash = l.Hash.bytes; // cid -> bytes
      }
      if (l.Name !== undefined) {
        link.Name = l.Name;
      }
      if (l.Tsize !== undefined) {
        link.Tsize = l.Tsize;
      }
      return link;
    });
  }
  if (node.Data) {
    pbn.Data = node.Data;
  }

  return encodeNode(pbn);
}

/**
 * @param {ByteView<PBNode> | ArrayBufferView<PBNode>} bytes
 * @returns {PBNode}
 */
function decode$7(bytes) {
  const buf = toByteView(bytes);
  const pbn = decodeNode(buf);

  const node = {};

  if (pbn.Data) {
    node.Data = pbn.Data;
  }

  if (pbn.Links) {
    node.Links = pbn.Links.map((l) => {
      const link = {};
      try {
        link.Hash = CID$2.decode(l.Hash);
      } catch (e) {}
      if (!link.Hash) {
        throw new Error("Invalid Hash field found in link, expected CID");
      }
      if (l.Name !== undefined) {
        link.Name = l.Name;
      }
      if (l.Tsize !== undefined) {
        link.Tsize = l.Tsize;
      }
      return link;
    });
  }

  return node;
}

var indexMinimal = {};

var minimal$1 = {};

var aspromise;
var hasRequiredAspromise;

function requireAspromise() {
  if (hasRequiredAspromise) return aspromise;
  hasRequiredAspromise = 1;
  aspromise = asPromise;

  /**
   * Callback as used by {@link util.asPromise}.
   * @typedef asPromiseCallback
   * @type {function}
   * @param {Error|null} error Error, if any
   * @param {...*} params Additional arguments
   * @returns {undefined}
   */

  /**
   * Returns a promise from a node-style callback function.
   * @memberof util
   * @param {asPromiseCallback} fn Function to call
   * @param {*} ctx Function context
   * @param {...*} params Function arguments
   * @returns {Promise<*>} Promisified function
   */
  function asPromise(fn, ctx /*, varargs */) {
    var params = new Array(arguments.length - 1),
      offset = 0,
      index = 2,
      pending = true;
    while (index < arguments.length) params[offset++] = arguments[index++];
    return new Promise(function executor(resolve, reject) {
      params[offset] = function callback(err /*, varargs */) {
        if (pending) {
          pending = false;
          if (err) reject(err);
          else {
            var params = new Array(arguments.length - 1),
              offset = 0;
            while (offset < params.length) params[offset++] = arguments[offset];
            resolve.apply(null, params);
          }
        }
      };
      try {
        fn.apply(ctx || null, params);
      } catch (err) {
        if (pending) {
          pending = false;
          reject(err);
        }
      }
    });
  }
  return aspromise;
}

var base64 = {};

var hasRequiredBase64;

function requireBase64() {
  if (hasRequiredBase64) return base64;
  hasRequiredBase64 = 1;
  (function (exports) {
    /**
     * A minimal base64 implementation for number arrays.
     * @memberof util
     * @namespace
     */
    var base64 = exports;

    /**
     * Calculates the byte length of a base64 encoded string.
     * @param {string} string Base64 encoded string
     * @returns {number} Byte length
     */
    base64.length = function length(string) {
      var p = string.length;
      if (!p) return 0;
      var n = 0;
      while (--p % 4 > 1 && string.charAt(p) === "=") ++n;
      return Math.ceil(string.length * 3) / 4 - n;
    };

    // Base64 encoding table
    var b64 = new Array(64);

    // Base64 decoding table
    var s64 = new Array(123);

    // 65..90, 97..122, 48..57, 43, 47
    for (var i = 0; i < 64; )
      s64[
        (b64[i] =
          i < 26 ? i + 65 : i < 52 ? i + 71 : i < 62 ? i - 4 : (i - 59) | 43)
      ] = i++;

    /**
     * Encodes a buffer to a base64 encoded string.
     * @param {Uint8Array} buffer Source buffer
     * @param {number} start Source start
     * @param {number} end Source end
     * @returns {string} Base64 encoded string
     */
    base64.encode = function encode(buffer, start, end) {
      var parts = null,
        chunk = [];
      var i = 0, // output index
        j = 0, // goto index
        t; // temporary
      while (start < end) {
        var b = buffer[start++];
        switch (j) {
          case 0:
            chunk[i++] = b64[b >> 2];
            t = (b & 3) << 4;
            j = 1;
            break;
          case 1:
            chunk[i++] = b64[t | (b >> 4)];
            t = (b & 15) << 2;
            j = 2;
            break;
          case 2:
            chunk[i++] = b64[t | (b >> 6)];
            chunk[i++] = b64[b & 63];
            j = 0;
            break;
        }
        if (i > 8191) {
          (parts || (parts = [])).push(
            String.fromCharCode.apply(String, chunk)
          );
          i = 0;
        }
      }
      if (j) {
        chunk[i++] = b64[t];
        chunk[i++] = 61;
        if (j === 1) chunk[i++] = 61;
      }
      if (parts) {
        if (i) parts.push(String.fromCharCode.apply(String, chunk.slice(0, i)));
        return parts.join("");
      }
      return String.fromCharCode.apply(String, chunk.slice(0, i));
    };

    var invalidEncoding = "invalid encoding";

    /**
     * Decodes a base64 encoded string to a buffer.
     * @param {string} string Source string
     * @param {Uint8Array} buffer Destination buffer
     * @param {number} offset Destination offset
     * @returns {number} Number of bytes written
     * @throws {Error} If encoding is invalid
     */
    base64.decode = function decode(string, buffer, offset) {
      var start = offset;
      var j = 0, // goto index
        t; // temporary
      for (var i = 0; i < string.length; ) {
        var c = string.charCodeAt(i++);
        if (c === 61 && j > 1) break;
        if ((c = s64[c]) === undefined) throw Error(invalidEncoding);
        switch (j) {
          case 0:
            t = c;
            j = 1;
            break;
          case 1:
            buffer[offset++] = (t << 2) | ((c & 48) >> 4);
            t = c;
            j = 2;
            break;
          case 2:
            buffer[offset++] = ((t & 15) << 4) | ((c & 60) >> 2);
            t = c;
            j = 3;
            break;
          case 3:
            buffer[offset++] = ((t & 3) << 6) | c;
            j = 0;
            break;
        }
      }
      if (j === 1) throw Error(invalidEncoding);
      return offset - start;
    };

    /**
     * Tests if the specified string appears to be base64 encoded.
     * @param {string} string String to test
     * @returns {boolean} `true` if probably base64 encoded, otherwise false
     */
    base64.test = function test(string) {
      return /^(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=)?$/.test(
        string
      );
    };
  })(base64);
  return base64;
}

var eventemitter;
var hasRequiredEventemitter;

function requireEventemitter() {
  if (hasRequiredEventemitter) return eventemitter;
  hasRequiredEventemitter = 1;
  eventemitter = EventEmitter;

  /**
   * Constructs a new event emitter instance.
   * @classdesc A minimal event emitter.
   * @memberof util
   * @constructor
   */
  function EventEmitter() {
    /**
     * Registered listeners.
     * @type {Object.<string,*>}
     * @private
     */
    this._listeners = {};
  }

  /**
   * Registers an event listener.
   * @param {string} evt Event name
   * @param {function} fn Listener
   * @param {*} [ctx] Listener context
   * @returns {util.EventEmitter} `this`
   */
  EventEmitter.prototype.on = function on(evt, fn, ctx) {
    (this._listeners[evt] || (this._listeners[evt] = [])).push({
      fn: fn,
      ctx: ctx || this,
    });
    return this;
  };

  /**
   * Removes an event listener or any matching listeners if arguments are omitted.
   * @param {string} [evt] Event name. Removes all listeners if omitted.
   * @param {function} [fn] Listener to remove. Removes all listeners of `evt` if omitted.
   * @returns {util.EventEmitter} `this`
   */
  EventEmitter.prototype.off = function off(evt, fn) {
    if (evt === undefined) this._listeners = {};
    else {
      if (fn === undefined) this._listeners[evt] = [];
      else {
        var listeners = this._listeners[evt];
        for (var i = 0; i < listeners.length; )
          if (listeners[i].fn === fn) listeners.splice(i, 1);
          else ++i;
      }
    }
    return this;
  };

  /**
   * Emits an event by calling its listeners with the specified arguments.
   * @param {string} evt Event name
   * @param {...*} args Arguments
   * @returns {util.EventEmitter} `this`
   */
  EventEmitter.prototype.emit = function emit(evt) {
    var listeners = this._listeners[evt];
    if (listeners) {
      var args = [],
        i = 1;
      for (; i < arguments.length; ) args.push(arguments[i++]);
      for (i = 0; i < listeners.length; )
        listeners[i].fn.apply(listeners[i++].ctx, args);
    }
    return this;
  };
  return eventemitter;
}

var float;
var hasRequiredFloat;

function requireFloat() {
  if (hasRequiredFloat) return float;
  hasRequiredFloat = 1;

  float = factory(factory);

  /**
   * Reads / writes floats / doubles from / to buffers.
   * @name util.float
   * @namespace
   */

  /**
   * Writes a 32 bit float to a buffer using little endian byte order.
   * @name util.float.writeFloatLE
   * @function
   * @param {number} val Value to write
   * @param {Uint8Array} buf Target buffer
   * @param {number} pos Target buffer offset
   * @returns {undefined}
   */

  /**
   * Writes a 32 bit float to a buffer using big endian byte order.
   * @name util.float.writeFloatBE
   * @function
   * @param {number} val Value to write
   * @param {Uint8Array} buf Target buffer
   * @param {number} pos Target buffer offset
   * @returns {undefined}
   */

  /**
   * Reads a 32 bit float from a buffer using little endian byte order.
   * @name util.float.readFloatLE
   * @function
   * @param {Uint8Array} buf Source buffer
   * @param {number} pos Source buffer offset
   * @returns {number} Value read
   */

  /**
   * Reads a 32 bit float from a buffer using big endian byte order.
   * @name util.float.readFloatBE
   * @function
   * @param {Uint8Array} buf Source buffer
   * @param {number} pos Source buffer offset
   * @returns {number} Value read
   */

  /**
   * Writes a 64 bit double to a buffer using little endian byte order.
   * @name util.float.writeDoubleLE
   * @function
   * @param {number} val Value to write
   * @param {Uint8Array} buf Target buffer
   * @param {number} pos Target buffer offset
   * @returns {undefined}
   */

  /**
   * Writes a 64 bit double to a buffer using big endian byte order.
   * @name util.float.writeDoubleBE
   * @function
   * @param {number} val Value to write
   * @param {Uint8Array} buf Target buffer
   * @param {number} pos Target buffer offset
   * @returns {undefined}
   */

  /**
   * Reads a 64 bit double from a buffer using little endian byte order.
   * @name util.float.readDoubleLE
   * @function
   * @param {Uint8Array} buf Source buffer
   * @param {number} pos Source buffer offset
   * @returns {number} Value read
   */

  /**
   * Reads a 64 bit double from a buffer using big endian byte order.
   * @name util.float.readDoubleBE
   * @function
   * @param {Uint8Array} buf Source buffer
   * @param {number} pos Source buffer offset
   * @returns {number} Value read
   */

  // Factory function for the purpose of node-based testing in modified global environments
  function factory(exports) {
    // float: typed array
    if (typeof Float32Array !== "undefined")
      (function () {
        var f32 = new Float32Array([-0]),
          f8b = new Uint8Array(f32.buffer),
          le = f8b[3] === 128;

        function writeFloat_f32_cpy(val, buf, pos) {
          f32[0] = val;
          buf[pos] = f8b[0];
          buf[pos + 1] = f8b[1];
          buf[pos + 2] = f8b[2];
          buf[pos + 3] = f8b[3];
        }

        function writeFloat_f32_rev(val, buf, pos) {
          f32[0] = val;
          buf[pos] = f8b[3];
          buf[pos + 1] = f8b[2];
          buf[pos + 2] = f8b[1];
          buf[pos + 3] = f8b[0];
        }

        /* istanbul ignore next */
        exports.writeFloatLE = le ? writeFloat_f32_cpy : writeFloat_f32_rev;
        /* istanbul ignore next */
        exports.writeFloatBE = le ? writeFloat_f32_rev : writeFloat_f32_cpy;

        function readFloat_f32_cpy(buf, pos) {
          f8b[0] = buf[pos];
          f8b[1] = buf[pos + 1];
          f8b[2] = buf[pos + 2];
          f8b[3] = buf[pos + 3];
          return f32[0];
        }

        function readFloat_f32_rev(buf, pos) {
          f8b[3] = buf[pos];
          f8b[2] = buf[pos + 1];
          f8b[1] = buf[pos + 2];
          f8b[0] = buf[pos + 3];
          return f32[0];
        }

        /* istanbul ignore next */
        exports.readFloatLE = le ? readFloat_f32_cpy : readFloat_f32_rev;
        /* istanbul ignore next */
        exports.readFloatBE = le ? readFloat_f32_rev : readFloat_f32_cpy;

        // float: ieee754
      })();
    else
      (function () {
        function writeFloat_ieee754(writeUint, val, buf, pos) {
          var sign = val < 0 ? 1 : 0;
          if (sign) val = -val;
          if (val === 0)
            writeUint(
              1 / val > 0 ? /* positive */ 0 : /* negative 0 */ 2147483648,
              buf,
              pos
            );
          else if (isNaN(val)) writeUint(2143289344, buf, pos);
          else if (val > 3.4028234663852886e38)
            // +-Infinity
            writeUint(((sign << 31) | 2139095040) >>> 0, buf, pos);
          else if (val < 1.1754943508222875e-38)
            // denormal
            writeUint(
              ((sign << 31) | Math.round(val / 1.401298464324817e-45)) >>> 0,
              buf,
              pos
            );
          else {
            var exponent = Math.floor(Math.log(val) / Math.LN2),
              mantissa =
                Math.round(val * Math.pow(2, -exponent) * 8388608) & 8388607;
            writeUint(
              ((sign << 31) | ((exponent + 127) << 23) | mantissa) >>> 0,
              buf,
              pos
            );
          }
        }

        exports.writeFloatLE = writeFloat_ieee754.bind(null, writeUintLE);
        exports.writeFloatBE = writeFloat_ieee754.bind(null, writeUintBE);

        function readFloat_ieee754(readUint, buf, pos) {
          var uint = readUint(buf, pos),
            sign = (uint >> 31) * 2 + 1,
            exponent = (uint >>> 23) & 255,
            mantissa = uint & 8388607;
          return exponent === 255
            ? mantissa
              ? NaN
              : sign * Infinity
            : exponent === 0 // denormal
            ? sign * 1.401298464324817e-45 * mantissa
            : sign * Math.pow(2, exponent - 150) * (mantissa + 8388608);
        }

        exports.readFloatLE = readFloat_ieee754.bind(null, readUintLE);
        exports.readFloatBE = readFloat_ieee754.bind(null, readUintBE);
      })();

    // double: typed array
    if (typeof Float64Array !== "undefined")
      (function () {
        var f64 = new Float64Array([-0]),
          f8b = new Uint8Array(f64.buffer),
          le = f8b[7] === 128;

        function writeDouble_f64_cpy(val, buf, pos) {
          f64[0] = val;
          buf[pos] = f8b[0];
          buf[pos + 1] = f8b[1];
          buf[pos + 2] = f8b[2];
          buf[pos + 3] = f8b[3];
          buf[pos + 4] = f8b[4];
          buf[pos + 5] = f8b[5];
          buf[pos + 6] = f8b[6];
          buf[pos + 7] = f8b[7];
        }

        function writeDouble_f64_rev(val, buf, pos) {
          f64[0] = val;
          buf[pos] = f8b[7];
          buf[pos + 1] = f8b[6];
          buf[pos + 2] = f8b[5];
          buf[pos + 3] = f8b[4];
          buf[pos + 4] = f8b[3];
          buf[pos + 5] = f8b[2];
          buf[pos + 6] = f8b[1];
          buf[pos + 7] = f8b[0];
        }

        /* istanbul ignore next */
        exports.writeDoubleLE = le ? writeDouble_f64_cpy : writeDouble_f64_rev;
        /* istanbul ignore next */
        exports.writeDoubleBE = le ? writeDouble_f64_rev : writeDouble_f64_cpy;

        function readDouble_f64_cpy(buf, pos) {
          f8b[0] = buf[pos];
          f8b[1] = buf[pos + 1];
          f8b[2] = buf[pos + 2];
          f8b[3] = buf[pos + 3];
          f8b[4] = buf[pos + 4];
          f8b[5] = buf[pos + 5];
          f8b[6] = buf[pos + 6];
          f8b[7] = buf[pos + 7];
          return f64[0];
        }

        function readDouble_f64_rev(buf, pos) {
          f8b[7] = buf[pos];
          f8b[6] = buf[pos + 1];
          f8b[5] = buf[pos + 2];
          f8b[4] = buf[pos + 3];
          f8b[3] = buf[pos + 4];
          f8b[2] = buf[pos + 5];
          f8b[1] = buf[pos + 6];
          f8b[0] = buf[pos + 7];
          return f64[0];
        }

        /* istanbul ignore next */
        exports.readDoubleLE = le ? readDouble_f64_cpy : readDouble_f64_rev;
        /* istanbul ignore next */
        exports.readDoubleBE = le ? readDouble_f64_rev : readDouble_f64_cpy;

        // double: ieee754
      })();
    else
      (function () {
        function writeDouble_ieee754(writeUint, off0, off1, val, buf, pos) {
          var sign = val < 0 ? 1 : 0;
          if (sign) val = -val;
          if (val === 0) {
            writeUint(0, buf, pos + off0);
            writeUint(
              1 / val > 0 ? /* positive */ 0 : /* negative 0 */ 2147483648,
              buf,
              pos + off1
            );
          } else if (isNaN(val)) {
            writeUint(0, buf, pos + off0);
            writeUint(2146959360, buf, pos + off1);
          } else if (val > 1.7976931348623157e308) {
            // +-Infinity
            writeUint(0, buf, pos + off0);
            writeUint(((sign << 31) | 2146435072) >>> 0, buf, pos + off1);
          } else {
            var mantissa;
            if (val < 2.2250738585072014e-308) {
              // denormal
              mantissa = val / 5e-324;
              writeUint(mantissa >>> 0, buf, pos + off0);
              writeUint(
                ((sign << 31) | (mantissa / 4294967296)) >>> 0,
                buf,
                pos + off1
              );
            } else {
              var exponent = Math.floor(Math.log(val) / Math.LN2);
              if (exponent === 1024) exponent = 1023;
              mantissa = val * Math.pow(2, -exponent);
              writeUint((mantissa * 4503599627370496) >>> 0, buf, pos + off0);
              writeUint(
                ((sign << 31) |
                  ((exponent + 1023) << 20) |
                  ((mantissa * 1048576) & 1048575)) >>>
                  0,
                buf,
                pos + off1
              );
            }
          }
        }

        exports.writeDoubleLE = writeDouble_ieee754.bind(
          null,
          writeUintLE,
          0,
          4
        );
        exports.writeDoubleBE = writeDouble_ieee754.bind(
          null,
          writeUintBE,
          4,
          0
        );

        function readDouble_ieee754(readUint, off0, off1, buf, pos) {
          var lo = readUint(buf, pos + off0),
            hi = readUint(buf, pos + off1);
          var sign = (hi >> 31) * 2 + 1,
            exponent = (hi >>> 20) & 2047,
            mantissa = 4294967296 * (hi & 1048575) + lo;
          return exponent === 2047
            ? mantissa
              ? NaN
              : sign * Infinity
            : exponent === 0 // denormal
            ? sign * 5e-324 * mantissa
            : sign *
              Math.pow(2, exponent - 1075) *
              (mantissa + 4503599627370496);
        }

        exports.readDoubleLE = readDouble_ieee754.bind(null, readUintLE, 0, 4);
        exports.readDoubleBE = readDouble_ieee754.bind(null, readUintBE, 4, 0);
      })();

    return exports;
  }

  // uint helpers

  function writeUintLE(val, buf, pos) {
    buf[pos] = val & 255;
    buf[pos + 1] = (val >>> 8) & 255;
    buf[pos + 2] = (val >>> 16) & 255;
    buf[pos + 3] = val >>> 24;
  }

  function writeUintBE(val, buf, pos) {
    buf[pos] = val >>> 24;
    buf[pos + 1] = (val >>> 16) & 255;
    buf[pos + 2] = (val >>> 8) & 255;
    buf[pos + 3] = val & 255;
  }

  function readUintLE(buf, pos) {
    return (
      (buf[pos] |
        (buf[pos + 1] << 8) |
        (buf[pos + 2] << 16) |
        (buf[pos + 3] << 24)) >>>
      0
    );
  }

  function readUintBE(buf, pos) {
    return (
      ((buf[pos] << 24) |
        (buf[pos + 1] << 16) |
        (buf[pos + 2] << 8) |
        buf[pos + 3]) >>>
      0
    );
  }
  return float;
}

var inquire_1;
var hasRequiredInquire;

function requireInquire() {
  if (hasRequiredInquire) return inquire_1;
  hasRequiredInquire = 1;
  inquire_1 = inquire;

  /**
   * Requires a module only if available.
   * @memberof util
   * @param {string} moduleName Module to require
   * @returns {?Object} Required module if available and not empty, otherwise `null`
   */
  function inquire(moduleName) {
    try {
      var mod = eval("quire".replace(/^/, "re"))(moduleName); // eslint-disable-line no-eval
      if (mod && (mod.length || Object.keys(mod).length)) return mod;
    } catch (e) {} // eslint-disable-line no-empty
    return null;
  }
  return inquire_1;
}

var utf8$3 = {};

var hasRequiredUtf8;

function requireUtf8() {
  if (hasRequiredUtf8) return utf8$3;
  hasRequiredUtf8 = 1;
  (function (exports) {
    /**
     * A minimal UTF8 implementation for number arrays.
     * @memberof util
     * @namespace
     */
    var utf8 = exports;

    /**
     * Calculates the UTF8 byte length of a string.
     * @param {string} string String
     * @returns {number} Byte length
     */
    utf8.length = function utf8_length(string) {
      var len = 0,
        c = 0;
      for (var i = 0; i < string.length; ++i) {
        c = string.charCodeAt(i);
        if (c < 128) len += 1;
        else if (c < 2048) len += 2;
        else if (
          (c & 0xfc00) === 0xd800 &&
          (string.charCodeAt(i + 1) & 0xfc00) === 0xdc00
        ) {
          ++i;
          len += 4;
        } else len += 3;
      }
      return len;
    };

    /**
     * Reads UTF8 bytes as a string.
     * @param {Uint8Array} buffer Source buffer
     * @param {number} start Source start
     * @param {number} end Source end
     * @returns {string} String read
     */
    utf8.read = function utf8_read(buffer, start, end) {
      var len = end - start;
      if (len < 1) return "";
      var parts = null,
        chunk = [],
        i = 0, // char offset
        t; // temporary
      while (start < end) {
        t = buffer[start++];
        if (t < 128) chunk[i++] = t;
        else if (t > 191 && t < 224)
          chunk[i++] = ((t & 31) << 6) | (buffer[start++] & 63);
        else if (t > 239 && t < 365) {
          t =
            (((t & 7) << 18) |
              ((buffer[start++] & 63) << 12) |
              ((buffer[start++] & 63) << 6) |
              (buffer[start++] & 63)) -
            0x10000;
          chunk[i++] = 0xd800 + (t >> 10);
          chunk[i++] = 0xdc00 + (t & 1023);
        } else
          chunk[i++] =
            ((t & 15) << 12) |
            ((buffer[start++] & 63) << 6) |
            (buffer[start++] & 63);
        if (i > 8191) {
          (parts || (parts = [])).push(
            String.fromCharCode.apply(String, chunk)
          );
          i = 0;
        }
      }
      if (parts) {
        if (i) parts.push(String.fromCharCode.apply(String, chunk.slice(0, i)));
        return parts.join("");
      }
      return String.fromCharCode.apply(String, chunk.slice(0, i));
    };

    /**
     * Writes a string as UTF8 bytes.
     * @param {string} string Source string
     * @param {Uint8Array} buffer Destination buffer
     * @param {number} offset Destination offset
     * @returns {number} Bytes written
     */
    utf8.write = function utf8_write(string, buffer, offset) {
      var start = offset,
        c1, // character 1
        c2; // character 2
      for (var i = 0; i < string.length; ++i) {
        c1 = string.charCodeAt(i);
        if (c1 < 128) {
          buffer[offset++] = c1;
        } else if (c1 < 2048) {
          buffer[offset++] = (c1 >> 6) | 192;
          buffer[offset++] = (c1 & 63) | 128;
        } else if (
          (c1 & 0xfc00) === 0xd800 &&
          ((c2 = string.charCodeAt(i + 1)) & 0xfc00) === 0xdc00
        ) {
          c1 = 0x10000 + ((c1 & 0x03ff) << 10) + (c2 & 0x03ff);
          ++i;
          buffer[offset++] = (c1 >> 18) | 240;
          buffer[offset++] = ((c1 >> 12) & 63) | 128;
          buffer[offset++] = ((c1 >> 6) & 63) | 128;
          buffer[offset++] = (c1 & 63) | 128;
        } else {
          buffer[offset++] = (c1 >> 12) | 224;
          buffer[offset++] = ((c1 >> 6) & 63) | 128;
          buffer[offset++] = (c1 & 63) | 128;
        }
      }
      return offset - start;
    };
  })(utf8$3);
  return utf8$3;
}

var pool_1;
var hasRequiredPool;

function requirePool() {
  if (hasRequiredPool) return pool_1;
  hasRequiredPool = 1;
  pool_1 = pool;

  /**
   * An allocator as used by {@link util.pool}.
   * @typedef PoolAllocator
   * @type {function}
   * @param {number} size Buffer size
   * @returns {Uint8Array} Buffer
   */

  /**
   * A slicer as used by {@link util.pool}.
   * @typedef PoolSlicer
   * @type {function}
   * @param {number} start Start offset
   * @param {number} end End offset
   * @returns {Uint8Array} Buffer slice
   * @this {Uint8Array}
   */

  /**
   * A general purpose buffer pool.
   * @memberof util
   * @function
   * @param {PoolAllocator} alloc Allocator
   * @param {PoolSlicer} slice Slicer
   * @param {number} [size=8192] Slab size
   * @returns {PoolAllocator} Pooled allocator
   */
  function pool(alloc, slice, size) {
    var SIZE = size || 8192;
    var MAX = SIZE >>> 1;
    var slab = null;
    var offset = SIZE;
    return function pool_alloc(size) {
      if (size < 1 || size > MAX) return alloc(size);
      if (offset + size > SIZE) {
        slab = alloc(SIZE);
        offset = 0;
      }
      var buf = slice.call(slab, offset, (offset += size));
      if (offset & 7)
        // align to 32 bit
        offset = (offset | 7) + 1;
      return buf;
    };
  }
  return pool_1;
}

var longbits;
var hasRequiredLongbits;

function requireLongbits() {
  if (hasRequiredLongbits) return longbits;
  hasRequiredLongbits = 1;
  longbits = LongBits;

  var util = requireMinimal$1();

  /**
   * Constructs new long bits.
   * @classdesc Helper class for working with the low and high bits of a 64 bit value.
   * @memberof util
   * @constructor
   * @param {number} lo Low 32 bits, unsigned
   * @param {number} hi High 32 bits, unsigned
   */
  function LongBits(lo, hi) {
    // note that the casts below are theoretically unnecessary as of today, but older statically
    // generated converter code might still call the ctor with signed 32bits. kept for compat.

    /**
     * Low bits.
     * @type {number}
     */
    this.lo = lo >>> 0;

    /**
     * High bits.
     * @type {number}
     */
    this.hi = hi >>> 0;
  }

  /**
   * Zero bits.
   * @memberof util.LongBits
   * @type {util.LongBits}
   */
  var zero = (LongBits.zero = new LongBits(0, 0));

  zero.toNumber = function () {
    return 0;
  };
  zero.zzEncode = zero.zzDecode = function () {
    return this;
  };
  zero.length = function () {
    return 1;
  };

  /**
   * Zero hash.
   * @memberof util.LongBits
   * @type {string}
   */
  var zeroHash = (LongBits.zeroHash = "\0\0\0\0\0\0\0\0");

  /**
   * Constructs new long bits from the specified number.
   * @param {number} value Value
   * @returns {util.LongBits} Instance
   */
  LongBits.fromNumber = function fromNumber(value) {
    if (value === 0) return zero;
    var sign = value < 0;
    if (sign) value = -value;
    var lo = value >>> 0,
      hi = ((value - lo) / 4294967296) >>> 0;
    if (sign) {
      hi = ~hi >>> 0;
      lo = ~lo >>> 0;
      if (++lo > 4294967295) {
        lo = 0;
        if (++hi > 4294967295) hi = 0;
      }
    }
    return new LongBits(lo, hi);
  };

  /**
   * Constructs new long bits from a number, long or string.
   * @param {Long|number|string} value Value
   * @returns {util.LongBits} Instance
   */
  LongBits.from = function from(value) {
    if (typeof value === "number") return LongBits.fromNumber(value);
    if (util.isString(value)) {
      /* istanbul ignore else */
      if (util.Long) value = util.Long.fromString(value);
      else return LongBits.fromNumber(parseInt(value, 10));
    }
    return value.low || value.high
      ? new LongBits(value.low >>> 0, value.high >>> 0)
      : zero;
  };

  /**
   * Converts this long bits to a possibly unsafe JavaScript number.
   * @param {boolean} [unsigned=false] Whether unsigned or not
   * @returns {number} Possibly unsafe number
   */
  LongBits.prototype.toNumber = function toNumber(unsigned) {
    if (!unsigned && this.hi >>> 31) {
      var lo = (~this.lo + 1) >>> 0,
        hi = ~this.hi >>> 0;
      if (!lo) hi = (hi + 1) >>> 0;
      return -(lo + hi * 4294967296);
    }
    return this.lo + this.hi * 4294967296;
  };

  /**
   * Converts this long bits to a long.
   * @param {boolean} [unsigned=false] Whether unsigned or not
   * @returns {Long} Long
   */
  LongBits.prototype.toLong = function toLong(unsigned) {
    return util.Long
      ? new util.Long(this.lo | 0, this.hi | 0, Boolean(unsigned))
      : /* istanbul ignore next */
        { low: this.lo | 0, high: this.hi | 0, unsigned: Boolean(unsigned) };
  };

  var charCodeAt = String.prototype.charCodeAt;

  /**
   * Constructs new long bits from the specified 8 characters long hash.
   * @param {string} hash Hash
   * @returns {util.LongBits} Bits
   */
  LongBits.fromHash = function fromHash(hash) {
    if (hash === zeroHash) return zero;
    return new LongBits(
      (charCodeAt.call(hash, 0) |
        (charCodeAt.call(hash, 1) << 8) |
        (charCodeAt.call(hash, 2) << 16) |
        (charCodeAt.call(hash, 3) << 24)) >>>
        0,
      (charCodeAt.call(hash, 4) |
        (charCodeAt.call(hash, 5) << 8) |
        (charCodeAt.call(hash, 6) << 16) |
        (charCodeAt.call(hash, 7) << 24)) >>>
        0
    );
  };

  /**
   * Converts this long bits to a 8 characters long hash.
   * @returns {string} Hash
   */
  LongBits.prototype.toHash = function toHash() {
    return String.fromCharCode(
      this.lo & 255,
      (this.lo >>> 8) & 255,
      (this.lo >>> 16) & 255,
      this.lo >>> 24,
      this.hi & 255,
      (this.hi >>> 8) & 255,
      (this.hi >>> 16) & 255,
      this.hi >>> 24
    );
  };

  /**
   * Zig-zag encodes this long bits.
   * @returns {util.LongBits} `this`
   */
  LongBits.prototype.zzEncode = function zzEncode() {
    var mask = this.hi >> 31;
    this.hi = (((this.hi << 1) | (this.lo >>> 31)) ^ mask) >>> 0;
    this.lo = ((this.lo << 1) ^ mask) >>> 0;
    return this;
  };

  /**
   * Zig-zag decodes this long bits.
   * @returns {util.LongBits} `this`
   */
  LongBits.prototype.zzDecode = function zzDecode() {
    var mask = -(this.lo & 1);
    this.lo = (((this.lo >>> 1) | (this.hi << 31)) ^ mask) >>> 0;
    this.hi = ((this.hi >>> 1) ^ mask) >>> 0;
    return this;
  };

  /**
   * Calculates the length of this longbits when encoded as a varint.
   * @returns {number} Length
   */
  LongBits.prototype.length = function length() {
    var part0 = this.lo,
      part1 = ((this.lo >>> 28) | (this.hi << 4)) >>> 0,
      part2 = this.hi >>> 24;
    return part2 === 0
      ? part1 === 0
        ? part0 < 16384
          ? part0 < 128
            ? 1
            : 2
          : part0 < 2097152
          ? 3
          : 4
        : part1 < 16384
        ? part1 < 128
          ? 5
          : 6
        : part1 < 2097152
        ? 7
        : 8
      : part2 < 128
      ? 9
      : 10;
  };
  return longbits;
}

var hasRequiredMinimal$1;

function requireMinimal$1() {
  if (hasRequiredMinimal$1) return minimal$1;
  hasRequiredMinimal$1 = 1;
  (function (exports) {
    var util = exports;

    // used to return a Promise where callback is omitted
    util.asPromise = requireAspromise();

    // converts to / from base64 encoded strings
    util.base64 = requireBase64();

    // base class of rpc.Service
    util.EventEmitter = requireEventemitter();

    // float handling accross browsers
    util.float = requireFloat();

    // requires modules optionally and hides the call from bundlers
    util.inquire = requireInquire();

    // converts to / from utf8 encoded strings
    util.utf8 = requireUtf8();

    // provides a node-like buffer pool in the browser
    util.pool = requirePool();

    // utility to work with the low and high bits of a 64 bit value
    util.LongBits = requireLongbits();

    /**
     * Whether running within node or not.
     * @memberof util
     * @type {boolean}
     */
    util.isNode = Boolean(
      typeof commonjsGlobal !== "undefined" &&
        commonjsGlobal &&
        commonjsGlobal.process &&
        commonjsGlobal.process.versions &&
        commonjsGlobal.process.versions.node
    );

    /**
     * Global object reference.
     * @memberof util
     * @type {Object}
     */
    util.global =
      (util.isNode && commonjsGlobal) ||
      (typeof window !== "undefined" && window) ||
      (typeof self !== "undefined" && self) ||
      minimal$1; // eslint-disable-line no-invalid-this

    /**
     * An immuable empty array.
     * @memberof util
     * @type {Array.<*>}
     * @const
     */
    util.emptyArray = Object.freeze
      ? Object.freeze([])
      : /* istanbul ignore next */ []; // used on prototypes

    /**
     * An immutable empty object.
     * @type {Object}
     * @const
     */
    util.emptyObject = Object.freeze
      ? Object.freeze({})
      : /* istanbul ignore next */ {}; // used on prototypes

    /**
     * Tests if the specified value is an integer.
     * @function
     * @param {*} value Value to test
     * @returns {boolean} `true` if the value is an integer
     */
    util.isInteger =
      Number.isInteger ||
      /* istanbul ignore next */ function isInteger(value) {
        return (
          typeof value === "number" &&
          isFinite(value) &&
          Math.floor(value) === value
        );
      };

    /**
     * Tests if the specified value is a string.
     * @param {*} value Value to test
     * @returns {boolean} `true` if the value is a string
     */
    util.isString = function isString(value) {
      return typeof value === "string" || value instanceof String;
    };

    /**
     * Tests if the specified value is a non-null object.
     * @param {*} value Value to test
     * @returns {boolean} `true` if the value is a non-null object
     */
    util.isObject = function isObject(value) {
      return value && typeof value === "object";
    };

    /**
     * Checks if a property on a message is considered to be present.
     * This is an alias of {@link util.isSet}.
     * @function
     * @param {Object} obj Plain object or message instance
     * @param {string} prop Property name
     * @returns {boolean} `true` if considered to be present, otherwise `false`
     */
    util.isset =
      /**
       * Checks if a property on a message is considered to be present.
       * @param {Object} obj Plain object or message instance
       * @param {string} prop Property name
       * @returns {boolean} `true` if considered to be present, otherwise `false`
       */
      util.isSet = function isSet(obj, prop) {
        var value = obj[prop];
        if (value != null && obj.hasOwnProperty(prop))
          // eslint-disable-line eqeqeq, no-prototype-builtins
          return (
            typeof value !== "object" ||
            (Array.isArray(value) ? value.length : Object.keys(value).length) >
              0
          );
        return false;
      };

    /**
     * Any compatible Buffer instance.
     * This is a minimal stand-alone definition of a Buffer instance. The actual type is that exported by node's typings.
     * @interface Buffer
     * @extends Uint8Array
     */

    /**
     * Node's Buffer class if available.
     * @type {Constructor<Buffer>}
     */
    util.Buffer = (function () {
      try {
        var Buffer = util.inquire("buffer").Buffer;
        // refuse to use non-node buffers if not explicitly assigned (perf reasons):
        return Buffer.prototype.utf8Write
          ? Buffer
          : /* istanbul ignore next */ null;
      } catch (e) {
        /* istanbul ignore next */
        return null;
      }
    })();

    // Internal alias of or polyfull for Buffer.from.
    util._Buffer_from = null;

    // Internal alias of or polyfill for Buffer.allocUnsafe.
    util._Buffer_allocUnsafe = null;

    /**
     * Creates a new buffer of whatever type supported by the environment.
     * @param {number|number[]} [sizeOrArray=0] Buffer size or number array
     * @returns {Uint8Array|Buffer} Buffer
     */
    util.newBuffer = function newBuffer(sizeOrArray) {
      /* istanbul ignore next */
      return typeof sizeOrArray === "number"
        ? util.Buffer
          ? util._Buffer_allocUnsafe(sizeOrArray)
          : new util.Array(sizeOrArray)
        : util.Buffer
        ? util._Buffer_from(sizeOrArray)
        : typeof Uint8Array === "undefined"
        ? sizeOrArray
        : new Uint8Array(sizeOrArray);
    };

    /**
     * Array implementation used in the browser. `Uint8Array` if supported, otherwise `Array`.
     * @type {Constructor<Uint8Array>}
     */
    util.Array =
      typeof Uint8Array !== "undefined"
        ? Uint8Array /* istanbul ignore next */
        : Array;

    /**
     * Any compatible Long instance.
     * This is a minimal stand-alone definition of a Long instance. The actual type is that exported by long.js.
     * @interface Long
     * @property {number} low Low bits
     * @property {number} high High bits
     * @property {boolean} unsigned Whether unsigned or not
     */

    /**
     * Long.js's Long class if available.
     * @type {Constructor<Long>}
     */
    util.Long =
      /* istanbul ignore next */ (util.global.dcodeIO &&
        /* istanbul ignore next */ util.global.dcodeIO.Long) ||
      /* istanbul ignore next */ util.global.Long ||
      util.inquire("long");

    /**
     * Regular expression used to verify 2 bit (`bool`) map keys.
     * @type {RegExp}
     * @const
     */
    util.key2Re = /^true|false|0|1$/;

    /**
     * Regular expression used to verify 32 bit (`int32` etc.) map keys.
     * @type {RegExp}
     * @const
     */
    util.key32Re = /^-?(?:0|[1-9][0-9]*)$/;

    /**
     * Regular expression used to verify 64 bit (`int64` etc.) map keys.
     * @type {RegExp}
     * @const
     */
    util.key64Re = /^(?:[\\x00-\\xff]{8}|-?(?:0|[1-9][0-9]*))$/;

    /**
     * Converts a number or long to an 8 characters long hash string.
     * @param {Long|number} value Value to convert
     * @returns {string} Hash
     */
    util.longToHash = function longToHash(value) {
      return value
        ? util.LongBits.from(value).toHash()
        : util.LongBits.zeroHash;
    };

    /**
     * Converts an 8 characters long hash string to a long or number.
     * @param {string} hash Hash
     * @param {boolean} [unsigned=false] Whether unsigned or not
     * @returns {Long|number} Original value
     */
    util.longFromHash = function longFromHash(hash, unsigned) {
      var bits = util.LongBits.fromHash(hash);
      if (util.Long) return util.Long.fromBits(bits.lo, bits.hi, unsigned);
      return bits.toNumber(Boolean(unsigned));
    };

    /**
     * Merges the properties of the source object into the destination object.
     * @memberof util
     * @param {Object.<string,*>} dst Destination object
     * @param {Object.<string,*>} src Source object
     * @param {boolean} [ifNotSet=false] Merges only if the key is not already set
     * @returns {Object.<string,*>} Destination object
     */
    function merge(dst, src, ifNotSet) {
      // used by converters
      for (var keys = Object.keys(src), i = 0; i < keys.length; ++i)
        if (dst[keys[i]] === undefined || !ifNotSet)
          dst[keys[i]] = src[keys[i]];
      return dst;
    }

    util.merge = merge;

    /**
     * Converts the first character of a string to lower case.
     * @param {string} str String to convert
     * @returns {string} Converted string
     */
    util.lcFirst = function lcFirst(str) {
      return str.charAt(0).toLowerCase() + str.substring(1);
    };

    /**
     * Creates a custom error constructor.
     * @memberof util
     * @param {string} name Error name
     * @returns {Constructor<Error>} Custom error constructor
     */
    function newError(name) {
      function CustomError(message, properties) {
        if (!(this instanceof CustomError))
          return new CustomError(message, properties);

        // Error.call(this, message);
        // ^ just returns a new error instance because the ctor can be called as a function

        Object.defineProperty(this, "message", {
          get: function () {
            return message;
          },
        });

        /* istanbul ignore next */
        if (Error.captureStackTrace)
          // node
          Error.captureStackTrace(this, CustomError);
        else
          Object.defineProperty(this, "stack", {
            value: new Error().stack || "",
          });

        if (properties) merge(this, properties);
      }

      CustomError.prototype = Object.create(Error.prototype, {
        constructor: {
          value: CustomError,
          writable: true,
          enumerable: false,
          configurable: true,
        },
        name: {
          get: function get() {
            return name;
          },
          set: undefined,
          enumerable: false,
          // configurable: false would accurately preserve the behavior of
          // the original, but I'm guessing that was not intentional.
          // For an actual error subclass, this property would
          // be configurable.
          configurable: true,
        },
        toString: {
          value: function value() {
            return this.name + ": " + this.message;
          },
          writable: true,
          enumerable: false,
          configurable: true,
        },
      });

      return CustomError;
    }

    util.newError = newError;

    /**
     * Constructs a new protocol error.
     * @classdesc Error subclass indicating a protocol specifc error.
     * @memberof util
     * @extends Error
     * @template T extends Message<T>
     * @constructor
     * @param {string} message Error message
     * @param {Object.<string,*>} [properties] Additional properties
     * @example
     * try {
     *     MyMessage.decode(someBuffer); // throws if required fields are missing
     * } catch (e) {
     *     if (e instanceof ProtocolError && e.instance)
     *         console.log("decoded so far: " + JSON.stringify(e.instance));
     * }
     */
    util.ProtocolError = newError("ProtocolError");

    /**
     * So far decoded message instance.
     * @name util.ProtocolError#instance
     * @type {Message<T>}
     */

    /**
     * A OneOf getter as returned by {@link util.oneOfGetter}.
     * @typedef OneOfGetter
     * @type {function}
     * @returns {string|undefined} Set field name, if any
     */

    /**
     * Builds a getter for a oneof's present field name.
     * @param {string[]} fieldNames Field names
     * @returns {OneOfGetter} Unbound getter
     */
    util.oneOfGetter = function getOneOf(fieldNames) {
      var fieldMap = {};
      for (var i = 0; i < fieldNames.length; ++i) fieldMap[fieldNames[i]] = 1;

      /**
       * @returns {string|undefined} Set field name, if any
       * @this Object
       * @ignore
       */
      return function () {
        // eslint-disable-line consistent-return
        for (var keys = Object.keys(this), i = keys.length - 1; i > -1; --i)
          if (
            fieldMap[keys[i]] === 1 &&
            this[keys[i]] !== undefined &&
            this[keys[i]] !== null
          )
            return keys[i];
      };
    };

    /**
     * A OneOf setter as returned by {@link util.oneOfSetter}.
     * @typedef OneOfSetter
     * @type {function}
     * @param {string|undefined} value Field name
     * @returns {undefined}
     */

    /**
     * Builds a setter for a oneof's present field name.
     * @param {string[]} fieldNames Field names
     * @returns {OneOfSetter} Unbound setter
     */
    util.oneOfSetter = function setOneOf(fieldNames) {
      /**
       * @param {string} name Field name
       * @returns {undefined}
       * @this Object
       * @ignore
       */
      return function (name) {
        for (var i = 0; i < fieldNames.length; ++i)
          if (fieldNames[i] !== name) delete this[fieldNames[i]];
      };
    };

    /**
     * Default conversion options used for {@link Message#toJSON} implementations.
     *
     * These options are close to proto3's JSON mapping with the exception that internal types like Any are handled just like messages. More precisely:
     *
     * - Longs become strings
     * - Enums become string keys
     * - Bytes become base64 encoded strings
     * - (Sub-)Messages become plain objects
     * - Maps become plain objects with all string keys
     * - Repeated fields become arrays
     * - NaN and Infinity for float and double fields become strings
     *
     * @type {IConversionOptions}
     * @see https://developers.google.com/protocol-buffers/docs/proto3?hl=en#json
     */
    util.toJSONOptions = {
      longs: String,
      enums: String,
      bytes: String,
      json: true,
    };

    // Sets up buffer utility according to the environment (called in index-minimal)
    util._configure = function () {
      var Buffer = util.Buffer;
      /* istanbul ignore if */
      if (!Buffer) {
        util._Buffer_from = util._Buffer_allocUnsafe = null;
        return;
      }
      // because node 4.x buffers are incompatible & immutable
      // see: https://github.com/dcodeIO/protobuf.js/pull/665
      util._Buffer_from =
        (Buffer.from !== Uint8Array.from && Buffer.from) ||
        /* istanbul ignore next */
        function Buffer_from(value, encoding) {
          return new Buffer(value, encoding);
        };
      util._Buffer_allocUnsafe =
        Buffer.allocUnsafe ||
        /* istanbul ignore next */
        function Buffer_allocUnsafe(size) {
          return new Buffer(size);
        };
    };
  })(minimal$1);
  return minimal$1;
}

var writer;
var hasRequiredWriter;

function requireWriter() {
  if (hasRequiredWriter) return writer;
  hasRequiredWriter = 1;
  writer = Writer;

  var util = requireMinimal$1();

  var BufferWriter; // cyclic

  var LongBits = util.LongBits,
    base64 = util.base64,
    utf8 = util.utf8;

  /**
   * Constructs a new writer operation instance.
   * @classdesc Scheduled writer operation.
   * @constructor
   * @param {function(*, Uint8Array, number)} fn Function to call
   * @param {number} len Value byte length
   * @param {*} val Value to write
   * @ignore
   */
  function Op(fn, len, val) {
    /**
     * Function to call.
     * @type {function(Uint8Array, number, *)}
     */
    this.fn = fn;

    /**
     * Value byte length.
     * @type {number}
     */
    this.len = len;

    /**
     * Next operation.
     * @type {Writer.Op|undefined}
     */
    this.next = undefined;

    /**
     * Value to write.
     * @type {*}
     */
    this.val = val; // type varies
  }

  /* istanbul ignore next */
  function noop() {} // eslint-disable-line no-empty-function

  /**
   * Constructs a new writer state instance.
   * @classdesc Copied writer state.
   * @memberof Writer
   * @constructor
   * @param {Writer} writer Writer to copy state from
   * @ignore
   */
  function State(writer) {
    /**
     * Current head.
     * @type {Writer.Op}
     */
    this.head = writer.head;

    /**
     * Current tail.
     * @type {Writer.Op}
     */
    this.tail = writer.tail;

    /**
     * Current buffer length.
     * @type {number}
     */
    this.len = writer.len;

    /**
     * Next state.
     * @type {State|null}
     */
    this.next = writer.states;
  }

  /**
   * Constructs a new writer instance.
   * @classdesc Wire format writer using `Uint8Array` if available, otherwise `Array`.
   * @constructor
   */
  function Writer() {
    /**
     * Current length.
     * @type {number}
     */
    this.len = 0;

    /**
     * Operations head.
     * @type {Object}
     */
    this.head = new Op(noop, 0, 0);

    /**
     * Operations tail
     * @type {Object}
     */
    this.tail = this.head;

    /**
     * Linked forked states.
     * @type {Object|null}
     */
    this.states = null;

    // When a value is written, the writer calculates its byte length and puts it into a linked
    // list of operations to perform when finish() is called. This both allows us to allocate
    // buffers of the exact required size and reduces the amount of work we have to do compared
    // to first calculating over objects and then encoding over objects. In our case, the encoding
    // part is just a linked list walk calling operations with already prepared values.
  }

  var create = function create() {
    return util.Buffer
      ? function create_buffer_setup() {
          return (Writer.create = function create_buffer() {
            return new BufferWriter();
          })();
        }
      : /* istanbul ignore next */
        function create_array() {
          return new Writer();
        };
  };

  /**
   * Creates a new writer.
   * @function
   * @returns {BufferWriter|Writer} A {@link BufferWriter} when Buffers are supported, otherwise a {@link Writer}
   */
  Writer.create = create();

  /**
   * Allocates a buffer of the specified size.
   * @param {number} size Buffer size
   * @returns {Uint8Array} Buffer
   */
  Writer.alloc = function alloc(size) {
    return new util.Array(size);
  };

  // Use Uint8Array buffer pool in the browser, just like node does with buffers
  /* istanbul ignore else */
  if (util.Array !== Array)
    Writer.alloc = util.pool(Writer.alloc, util.Array.prototype.subarray);

  /**
   * Pushes a new operation to the queue.
   * @param {function(Uint8Array, number, *)} fn Function to call
   * @param {number} len Value byte length
   * @param {number} val Value to write
   * @returns {Writer} `this`
   * @private
   */
  Writer.prototype._push = function push(fn, len, val) {
    this.tail = this.tail.next = new Op(fn, len, val);
    this.len += len;
    return this;
  };

  function writeByte(val, buf, pos) {
    buf[pos] = val & 255;
  }

  function writeVarint32(val, buf, pos) {
    while (val > 127) {
      buf[pos++] = (val & 127) | 128;
      val >>>= 7;
    }
    buf[pos] = val;
  }

  /**
   * Constructs a new varint writer operation instance.
   * @classdesc Scheduled varint writer operation.
   * @extends Op
   * @constructor
   * @param {number} len Value byte length
   * @param {number} val Value to write
   * @ignore
   */
  function VarintOp(len, val) {
    this.len = len;
    this.next = undefined;
    this.val = val;
  }

  VarintOp.prototype = Object.create(Op.prototype);
  VarintOp.prototype.fn = writeVarint32;

  /**
   * Writes an unsigned 32 bit value as a varint.
   * @param {number} value Value to write
   * @returns {Writer} `this`
   */
  Writer.prototype.uint32 = function write_uint32(value) {
    // here, the call to this.push has been inlined and a varint specific Op subclass is used.
    // uint32 is by far the most frequently used operation and benefits significantly from this.
    this.len += (this.tail = this.tail.next =
      new VarintOp(
        (value = value >>> 0) < 128
          ? 1
          : value < 16384
          ? 2
          : value < 2097152
          ? 3
          : value < 268435456
          ? 4
          : 5,
        value
      )).len;
    return this;
  };

  /**
   * Writes a signed 32 bit value as a varint.
   * @function
   * @param {number} value Value to write
   * @returns {Writer} `this`
   */
  Writer.prototype.int32 = function write_int32(value) {
    return value < 0
      ? this._push(writeVarint64, 10, LongBits.fromNumber(value)) // 10 bytes per spec
      : this.uint32(value);
  };

  /**
   * Writes a 32 bit value as a varint, zig-zag encoded.
   * @param {number} value Value to write
   * @returns {Writer} `this`
   */
  Writer.prototype.sint32 = function write_sint32(value) {
    return this.uint32(((value << 1) ^ (value >> 31)) >>> 0);
  };

  function writeVarint64(val, buf, pos) {
    while (val.hi) {
      buf[pos++] = (val.lo & 127) | 128;
      val.lo = ((val.lo >>> 7) | (val.hi << 25)) >>> 0;
      val.hi >>>= 7;
    }
    while (val.lo > 127) {
      buf[pos++] = (val.lo & 127) | 128;
      val.lo = val.lo >>> 7;
    }
    buf[pos++] = val.lo;
  }

  /**
   * Writes an unsigned 64 bit value as a varint.
   * @param {Long|number|string} value Value to write
   * @returns {Writer} `this`
   * @throws {TypeError} If `value` is a string and no long library is present.
   */
  Writer.prototype.uint64 = function write_uint64(value) {
    var bits = LongBits.from(value);
    return this._push(writeVarint64, bits.length(), bits);
  };

  /**
   * Writes a signed 64 bit value as a varint.
   * @function
   * @param {Long|number|string} value Value to write
   * @returns {Writer} `this`
   * @throws {TypeError} If `value` is a string and no long library is present.
   */
  Writer.prototype.int64 = Writer.prototype.uint64;

  /**
   * Writes a signed 64 bit value as a varint, zig-zag encoded.
   * @param {Long|number|string} value Value to write
   * @returns {Writer} `this`
   * @throws {TypeError} If `value` is a string and no long library is present.
   */
  Writer.prototype.sint64 = function write_sint64(value) {
    var bits = LongBits.from(value).zzEncode();
    return this._push(writeVarint64, bits.length(), bits);
  };

  /**
   * Writes a boolish value as a varint.
   * @param {boolean} value Value to write
   * @returns {Writer} `this`
   */
  Writer.prototype.bool = function write_bool(value) {
    return this._push(writeByte, 1, value ? 1 : 0);
  };

  function writeFixed32(val, buf, pos) {
    buf[pos] = val & 255;
    buf[pos + 1] = (val >>> 8) & 255;
    buf[pos + 2] = (val >>> 16) & 255;
    buf[pos + 3] = val >>> 24;
  }

  /**
   * Writes an unsigned 32 bit value as fixed 32 bits.
   * @param {number} value Value to write
   * @returns {Writer} `this`
   */
  Writer.prototype.fixed32 = function write_fixed32(value) {
    return this._push(writeFixed32, 4, value >>> 0);
  };

  /**
   * Writes a signed 32 bit value as fixed 32 bits.
   * @function
   * @param {number} value Value to write
   * @returns {Writer} `this`
   */
  Writer.prototype.sfixed32 = Writer.prototype.fixed32;

  /**
   * Writes an unsigned 64 bit value as fixed 64 bits.
   * @param {Long|number|string} value Value to write
   * @returns {Writer} `this`
   * @throws {TypeError} If `value` is a string and no long library is present.
   */
  Writer.prototype.fixed64 = function write_fixed64(value) {
    var bits = LongBits.from(value);
    return this._push(writeFixed32, 4, bits.lo)._push(writeFixed32, 4, bits.hi);
  };

  /**
   * Writes a signed 64 bit value as fixed 64 bits.
   * @function
   * @param {Long|number|string} value Value to write
   * @returns {Writer} `this`
   * @throws {TypeError} If `value` is a string and no long library is present.
   */
  Writer.prototype.sfixed64 = Writer.prototype.fixed64;

  /**
   * Writes a float (32 bit).
   * @function
   * @param {number} value Value to write
   * @returns {Writer} `this`
   */
  Writer.prototype.float = function write_float(value) {
    return this._push(util.float.writeFloatLE, 4, value);
  };

  /**
   * Writes a double (64 bit float).
   * @function
   * @param {number} value Value to write
   * @returns {Writer} `this`
   */
  Writer.prototype.double = function write_double(value) {
    return this._push(util.float.writeDoubleLE, 8, value);
  };

  var writeBytes = util.Array.prototype.set
    ? function writeBytes_set(val, buf, pos) {
        buf.set(val, pos); // also works for plain array values
      }
    : /* istanbul ignore next */
      function writeBytes_for(val, buf, pos) {
        for (var i = 0; i < val.length; ++i) buf[pos + i] = val[i];
      };

  /**
   * Writes a sequence of bytes.
   * @param {Uint8Array|string} value Buffer or base64 encoded string to write
   * @returns {Writer} `this`
   */
  Writer.prototype.bytes = function write_bytes(value) {
    var len = value.length >>> 0;
    if (!len) return this._push(writeByte, 1, 0);
    if (util.isString(value)) {
      var buf = Writer.alloc((len = base64.length(value)));
      base64.decode(value, buf, 0);
      value = buf;
    }
    return this.uint32(len)._push(writeBytes, len, value);
  };

  /**
   * Writes a string.
   * @param {string} value Value to write
   * @returns {Writer} `this`
   */
  Writer.prototype.string = function write_string(value) {
    var len = utf8.length(value);
    return len
      ? this.uint32(len)._push(utf8.write, len, value)
      : this._push(writeByte, 1, 0);
  };

  /**
   * Forks this writer's state by pushing it to a stack.
   * Calling {@link Writer#reset|reset} or {@link Writer#ldelim|ldelim} resets the writer to the previous state.
   * @returns {Writer} `this`
   */
  Writer.prototype.fork = function fork() {
    this.states = new State(this);
    this.head = this.tail = new Op(noop, 0, 0);
    this.len = 0;
    return this;
  };

  /**
   * Resets this instance to the last state.
   * @returns {Writer} `this`
   */
  Writer.prototype.reset = function reset() {
    if (this.states) {
      this.head = this.states.head;
      this.tail = this.states.tail;
      this.len = this.states.len;
      this.states = this.states.next;
    } else {
      this.head = this.tail = new Op(noop, 0, 0);
      this.len = 0;
    }
    return this;
  };

  /**
   * Resets to the last state and appends the fork state's current write length as a varint followed by its operations.
   * @returns {Writer} `this`
   */
  Writer.prototype.ldelim = function ldelim() {
    var head = this.head,
      tail = this.tail,
      len = this.len;
    this.reset().uint32(len);
    if (len) {
      this.tail.next = head.next; // skip noop
      this.tail = tail;
      this.len += len;
    }
    return this;
  };

  /**
   * Finishes the write operation.
   * @returns {Uint8Array} Finished buffer
   */
  Writer.prototype.finish = function finish() {
    var head = this.head.next, // skip noop
      buf = this.constructor.alloc(this.len),
      pos = 0;
    while (head) {
      head.fn(head.val, buf, pos);
      pos += head.len;
      head = head.next;
    }
    // this.head = this.tail = null;
    return buf;
  };

  Writer._configure = function (BufferWriter_) {
    BufferWriter = BufferWriter_;
    Writer.create = create();
    BufferWriter._configure();
  };
  return writer;
}

var writer_buffer;
var hasRequiredWriter_buffer;

function requireWriter_buffer() {
  if (hasRequiredWriter_buffer) return writer_buffer;
  hasRequiredWriter_buffer = 1;
  writer_buffer = BufferWriter;

  // extends Writer
  var Writer = requireWriter();
  (BufferWriter.prototype = Object.create(Writer.prototype)).constructor =
    BufferWriter;

  var util = requireMinimal$1();

  /**
   * Constructs a new buffer writer instance.
   * @classdesc Wire format writer using node buffers.
   * @extends Writer
   * @constructor
   */
  function BufferWriter() {
    Writer.call(this);
  }

  BufferWriter._configure = function () {
    /**
     * Allocates a buffer of the specified size.
     * @function
     * @param {number} size Buffer size
     * @returns {Buffer} Buffer
     */
    BufferWriter.alloc = util._Buffer_allocUnsafe;

    BufferWriter.writeBytesBuffer =
      util.Buffer &&
      util.Buffer.prototype instanceof Uint8Array &&
      util.Buffer.prototype.set.name === "set"
        ? function writeBytesBuffer_set(val, buf, pos) {
            buf.set(val, pos); // faster than copy (requires node >= 4 where Buffers extend Uint8Array and set is properly inherited)
            // also works for plain array values
          }
        : /* istanbul ignore next */
          function writeBytesBuffer_copy(val, buf, pos) {
            if (val.copy)
              // Buffer values
              val.copy(buf, pos, 0, val.length);
            else
              for (
                var i = 0;
                i < val.length; // plain array values

              )
                buf[pos++] = val[i++];
          };
  };

  /**
   * @override
   */
  BufferWriter.prototype.bytes = function write_bytes_buffer(value) {
    if (util.isString(value)) value = util._Buffer_from(value, "base64");
    var len = value.length >>> 0;
    this.uint32(len);
    if (len) this._push(BufferWriter.writeBytesBuffer, len, value);
    return this;
  };

  function writeStringBuffer(val, buf, pos) {
    if (val.length < 40)
      // plain js is faster for short strings (probably due to redundant assertions)
      util.utf8.write(val, buf, pos);
    else if (buf.utf8Write) buf.utf8Write(val, pos);
    else buf.write(val, pos);
  }

  /**
   * @override
   */
  BufferWriter.prototype.string = function write_string_buffer(value) {
    var len = util.Buffer.byteLength(value);
    this.uint32(len);
    if (len) this._push(writeStringBuffer, len, value);
    return this;
  };

  /**
   * Finishes the write operation.
   * @name BufferWriter#finish
   * @function
   * @returns {Buffer} Finished buffer
   */

  BufferWriter._configure();
  return writer_buffer;
}

var reader;
var hasRequiredReader;

function requireReader() {
  if (hasRequiredReader) return reader;
  hasRequiredReader = 1;
  reader = Reader;

  var util = requireMinimal$1();

  var BufferReader; // cyclic

  var LongBits = util.LongBits,
    utf8 = util.utf8;

  /* istanbul ignore next */
  function indexOutOfRange(reader, writeLength) {
    return RangeError(
      "index out of range: " +
        reader.pos +
        " + " +
        (writeLength || 1) +
        " > " +
        reader.len
    );
  }

  /**
   * Constructs a new reader instance using the specified buffer.
   * @classdesc Wire format reader using `Uint8Array` if available, otherwise `Array`.
   * @constructor
   * @param {Uint8Array} buffer Buffer to read from
   */
  function Reader(buffer) {
    /**
     * Read buffer.
     * @type {Uint8Array}
     */
    this.buf = buffer;

    /**
     * Read buffer position.
     * @type {number}
     */
    this.pos = 0;

    /**
     * Read buffer length.
     * @type {number}
     */
    this.len = buffer.length;
  }

  var create_array =
    typeof Uint8Array !== "undefined"
      ? function create_typed_array(buffer) {
          if (buffer instanceof Uint8Array || Array.isArray(buffer))
            return new Reader(buffer);
          throw Error("illegal buffer");
        }
      : /* istanbul ignore next */
        function create_array(buffer) {
          if (Array.isArray(buffer)) return new Reader(buffer);
          throw Error("illegal buffer");
        };

  var create = function create() {
    return util.Buffer
      ? function create_buffer_setup(buffer) {
          return (Reader.create = function create_buffer(buffer) {
            return util.Buffer.isBuffer(buffer)
              ? new BufferReader(buffer)
              : /* istanbul ignore next */
                create_array(buffer);
          })(buffer);
        }
      : /* istanbul ignore next */
        create_array;
  };

  /**
   * Creates a new reader using the specified buffer.
   * @function
   * @param {Uint8Array|Buffer} buffer Buffer to read from
   * @returns {Reader|BufferReader} A {@link BufferReader} if `buffer` is a Buffer, otherwise a {@link Reader}
   * @throws {Error} If `buffer` is not a valid buffer
   */
  Reader.create = create();

  Reader.prototype._slice =
    util.Array.prototype.subarray ||
    /* istanbul ignore next */ util.Array.prototype.slice;

  /**
   * Reads a varint as an unsigned 32 bit value.
   * @function
   * @returns {number} Value read
   */
  Reader.prototype.uint32 = (function read_uint32_setup() {
    var value = 4294967295; // optimizer type-hint, tends to deopt otherwise (?!)
    return function read_uint32() {
      value = (this.buf[this.pos] & 127) >>> 0;
      if (this.buf[this.pos++] < 128) return value;
      value = (value | ((this.buf[this.pos] & 127) << 7)) >>> 0;
      if (this.buf[this.pos++] < 128) return value;
      value = (value | ((this.buf[this.pos] & 127) << 14)) >>> 0;
      if (this.buf[this.pos++] < 128) return value;
      value = (value | ((this.buf[this.pos] & 127) << 21)) >>> 0;
      if (this.buf[this.pos++] < 128) return value;
      value = (value | ((this.buf[this.pos] & 15) << 28)) >>> 0;
      if (this.buf[this.pos++] < 128) return value;

      /* istanbul ignore if */
      if ((this.pos += 5) > this.len) {
        this.pos = this.len;
        throw indexOutOfRange(this, 10);
      }
      return value;
    };
  })();

  /**
   * Reads a varint as a signed 32 bit value.
   * @returns {number} Value read
   */
  Reader.prototype.int32 = function read_int32() {
    return this.uint32() | 0;
  };

  /**
   * Reads a zig-zag encoded varint as a signed 32 bit value.
   * @returns {number} Value read
   */
  Reader.prototype.sint32 = function read_sint32() {
    var value = this.uint32();
    return ((value >>> 1) ^ -(value & 1)) | 0;
  };

  /* eslint-disable no-invalid-this */

  function readLongVarint() {
    // tends to deopt with local vars for octet etc.
    var bits = new LongBits(0, 0);
    var i = 0;
    if (this.len - this.pos > 4) {
      // fast route (lo)
      for (; i < 4; ++i) {
        // 1st..4th
        bits.lo = (bits.lo | ((this.buf[this.pos] & 127) << (i * 7))) >>> 0;
        if (this.buf[this.pos++] < 128) return bits;
      }
      // 5th
      bits.lo = (bits.lo | ((this.buf[this.pos] & 127) << 28)) >>> 0;
      bits.hi = (bits.hi | ((this.buf[this.pos] & 127) >> 4)) >>> 0;
      if (this.buf[this.pos++] < 128) return bits;
      i = 0;
    } else {
      for (; i < 3; ++i) {
        /* istanbul ignore if */
        if (this.pos >= this.len) throw indexOutOfRange(this);
        // 1st..3th
        bits.lo = (bits.lo | ((this.buf[this.pos] & 127) << (i * 7))) >>> 0;
        if (this.buf[this.pos++] < 128) return bits;
      }
      // 4th
      bits.lo = (bits.lo | ((this.buf[this.pos++] & 127) << (i * 7))) >>> 0;
      return bits;
    }
    if (this.len - this.pos > 4) {
      // fast route (hi)
      for (; i < 5; ++i) {
        // 6th..10th
        bits.hi = (bits.hi | ((this.buf[this.pos] & 127) << (i * 7 + 3))) >>> 0;
        if (this.buf[this.pos++] < 128) return bits;
      }
    } else {
      for (; i < 5; ++i) {
        /* istanbul ignore if */
        if (this.pos >= this.len) throw indexOutOfRange(this);
        // 6th..10th
        bits.hi = (bits.hi | ((this.buf[this.pos] & 127) << (i * 7 + 3))) >>> 0;
        if (this.buf[this.pos++] < 128) return bits;
      }
    }
    /* istanbul ignore next */
    throw Error("invalid varint encoding");
  }

  /* eslint-enable no-invalid-this */

  /**
   * Reads a varint as a signed 64 bit value.
   * @name Reader#int64
   * @function
   * @returns {Long} Value read
   */

  /**
   * Reads a varint as an unsigned 64 bit value.
   * @name Reader#uint64
   * @function
   * @returns {Long} Value read
   */

  /**
   * Reads a zig-zag encoded varint as a signed 64 bit value.
   * @name Reader#sint64
   * @function
   * @returns {Long} Value read
   */

  /**
   * Reads a varint as a boolean.
   * @returns {boolean} Value read
   */
  Reader.prototype.bool = function read_bool() {
    return this.uint32() !== 0;
  };

  function readFixed32_end(buf, end) {
    // note that this uses `end`, not `pos`
    return (
      (buf[end - 4] |
        (buf[end - 3] << 8) |
        (buf[end - 2] << 16) |
        (buf[end - 1] << 24)) >>>
      0
    );
  }

  /**
   * Reads fixed 32 bits as an unsigned 32 bit integer.
   * @returns {number} Value read
   */
  Reader.prototype.fixed32 = function read_fixed32() {
    /* istanbul ignore if */
    if (this.pos + 4 > this.len) throw indexOutOfRange(this, 4);

    return readFixed32_end(this.buf, (this.pos += 4));
  };

  /**
   * Reads fixed 32 bits as a signed 32 bit integer.
   * @returns {number} Value read
   */
  Reader.prototype.sfixed32 = function read_sfixed32() {
    /* istanbul ignore if */
    if (this.pos + 4 > this.len) throw indexOutOfRange(this, 4);

    return readFixed32_end(this.buf, (this.pos += 4)) | 0;
  };

  /* eslint-disable no-invalid-this */

  function readFixed64(/* this: Reader */) {
    /* istanbul ignore if */
    if (this.pos + 8 > this.len) throw indexOutOfRange(this, 8);

    return new LongBits(
      readFixed32_end(this.buf, (this.pos += 4)),
      readFixed32_end(this.buf, (this.pos += 4))
    );
  }

  /* eslint-enable no-invalid-this */

  /**
   * Reads fixed 64 bits.
   * @name Reader#fixed64
   * @function
   * @returns {Long} Value read
   */

  /**
   * Reads zig-zag encoded fixed 64 bits.
   * @name Reader#sfixed64
   * @function
   * @returns {Long} Value read
   */

  /**
   * Reads a float (32 bit) as a number.
   * @function
   * @returns {number} Value read
   */
  Reader.prototype.float = function read_float() {
    /* istanbul ignore if */
    if (this.pos + 4 > this.len) throw indexOutOfRange(this, 4);

    var value = util.float.readFloatLE(this.buf, this.pos);
    this.pos += 4;
    return value;
  };

  /**
   * Reads a double (64 bit float) as a number.
   * @function
   * @returns {number} Value read
   */
  Reader.prototype.double = function read_double() {
    /* istanbul ignore if */
    if (this.pos + 8 > this.len) throw indexOutOfRange(this, 4);

    var value = util.float.readDoubleLE(this.buf, this.pos);
    this.pos += 8;
    return value;
  };

  /**
   * Reads a sequence of bytes preceeded by its length as a varint.
   * @returns {Uint8Array} Value read
   */
  Reader.prototype.bytes = function read_bytes() {
    var length = this.uint32(),
      start = this.pos,
      end = this.pos + length;

    /* istanbul ignore if */
    if (end > this.len) throw indexOutOfRange(this, length);

    this.pos += length;
    if (Array.isArray(this.buf))
      // plain array
      return this.buf.slice(start, end);

    if (start === end) {
      // fix for IE 10/Win8 and others' subarray returning array of size 1
      var nativeBuffer = util.Buffer;
      return nativeBuffer ? nativeBuffer.alloc(0) : new this.buf.constructor(0);
    }
    return this._slice.call(this.buf, start, end);
  };

  /**
   * Reads a string preceeded by its byte length as a varint.
   * @returns {string} Value read
   */
  Reader.prototype.string = function read_string() {
    var bytes = this.bytes();
    return utf8.read(bytes, 0, bytes.length);
  };

  /**
   * Skips the specified number of bytes if specified, otherwise skips a varint.
   * @param {number} [length] Length if known, otherwise a varint is assumed
   * @returns {Reader} `this`
   */
  Reader.prototype.skip = function skip(length) {
    if (typeof length === "number") {
      /* istanbul ignore if */
      if (this.pos + length > this.len) throw indexOutOfRange(this, length);
      this.pos += length;
    } else {
      do {
        /* istanbul ignore if */
        if (this.pos >= this.len) throw indexOutOfRange(this);
      } while (this.buf[this.pos++] & 128);
    }
    return this;
  };

  /**
   * Skips the next element of the specified wire type.
   * @param {number} wireType Wire type received
   * @returns {Reader} `this`
   */
  Reader.prototype.skipType = function (wireType) {
    switch (wireType) {
      case 0:
        this.skip();
        break;
      case 1:
        this.skip(8);
        break;
      case 2:
        this.skip(this.uint32());
        break;
      case 3:
        while ((wireType = this.uint32() & 7) !== 4) {
          this.skipType(wireType);
        }
        break;
      case 5:
        this.skip(4);
        break;

      /* istanbul ignore next */
      default:
        throw Error("invalid wire type " + wireType + " at offset " + this.pos);
    }
    return this;
  };

  Reader._configure = function (BufferReader_) {
    BufferReader = BufferReader_;
    Reader.create = create();
    BufferReader._configure();

    var fn = util.Long ? "toLong" : /* istanbul ignore next */ "toNumber";
    util.merge(Reader.prototype, {
      int64: function read_int64() {
        return readLongVarint.call(this)[fn](false);
      },

      uint64: function read_uint64() {
        return readLongVarint.call(this)[fn](true);
      },

      sint64: function read_sint64() {
        return readLongVarint.call(this).zzDecode()[fn](false);
      },

      fixed64: function read_fixed64() {
        return readFixed64.call(this)[fn](true);
      },

      sfixed64: function read_sfixed64() {
        return readFixed64.call(this)[fn](false);
      },
    });
  };
  return reader;
}

var reader_buffer;
var hasRequiredReader_buffer;

function requireReader_buffer() {
  if (hasRequiredReader_buffer) return reader_buffer;
  hasRequiredReader_buffer = 1;
  reader_buffer = BufferReader;

  // extends Reader
  var Reader = requireReader();
  (BufferReader.prototype = Object.create(Reader.prototype)).constructor =
    BufferReader;

  var util = requireMinimal$1();

  /**
   * Constructs a new buffer reader instance.
   * @classdesc Wire format reader using node buffers.
   * @extends Reader
   * @constructor
   * @param {Buffer} buffer Buffer to read from
   */
  function BufferReader(buffer) {
    Reader.call(this, buffer);

    /**
     * Read buffer.
     * @name BufferReader#buf
     * @type {Buffer}
     */
  }

  BufferReader._configure = function () {
    /* istanbul ignore else */
    if (util.Buffer)
      BufferReader.prototype._slice = util.Buffer.prototype.slice;
  };

  /**
   * @override
   */
  BufferReader.prototype.string = function read_string_buffer() {
    var len = this.uint32(); // modifies pos
    return this.buf.utf8Slice
      ? this.buf.utf8Slice(
          this.pos,
          (this.pos = Math.min(this.pos + len, this.len))
        )
      : this.buf.toString(
          "utf-8",
          this.pos,
          (this.pos = Math.min(this.pos + len, this.len))
        );
  };

  /**
   * Reads a sequence of bytes preceeded by its length as a varint.
   * @name BufferReader#bytes
   * @function
   * @returns {Buffer} Value read
   */

  BufferReader._configure();
  return reader_buffer;
}

var rpc = {};

var service;
var hasRequiredService;

function requireService() {
  if (hasRequiredService) return service;
  hasRequiredService = 1;
  service = Service;

  var util = requireMinimal$1();

  // Extends EventEmitter
  (Service.prototype = Object.create(util.EventEmitter.prototype)).constructor =
    Service;

  /**
   * A service method callback as used by {@link rpc.ServiceMethod|ServiceMethod}.
   *
   * Differs from {@link RPCImplCallback} in that it is an actual callback of a service method which may not return `response = null`.
   * @typedef rpc.ServiceMethodCallback
   * @template TRes extends Message<TRes>
   * @type {function}
   * @param {Error|null} error Error, if any
   * @param {TRes} [response] Response message
   * @returns {undefined}
   */

  /**
   * A service method part of a {@link rpc.Service} as created by {@link Service.create}.
   * @typedef rpc.ServiceMethod
   * @template TReq extends Message<TReq>
   * @template TRes extends Message<TRes>
   * @type {function}
   * @param {TReq|Properties<TReq>} request Request message or plain object
   * @param {rpc.ServiceMethodCallback<TRes>} [callback] Node-style callback called with the error, if any, and the response message
   * @returns {Promise<Message<TRes>>} Promise if `callback` has been omitted, otherwise `undefined`
   */

  /**
   * Constructs a new RPC service instance.
   * @classdesc An RPC service as returned by {@link Service#create}.
   * @exports rpc.Service
   * @extends util.EventEmitter
   * @constructor
   * @param {RPCImpl} rpcImpl RPC implementation
   * @param {boolean} [requestDelimited=false] Whether requests are length-delimited
   * @param {boolean} [responseDelimited=false] Whether responses are length-delimited
   */
  function Service(rpcImpl, requestDelimited, responseDelimited) {
    if (typeof rpcImpl !== "function")
      throw TypeError("rpcImpl must be a function");

    util.EventEmitter.call(this);

    /**
     * RPC implementation. Becomes `null` once the service is ended.
     * @type {RPCImpl|null}
     */
    this.rpcImpl = rpcImpl;

    /**
     * Whether requests are length-delimited.
     * @type {boolean}
     */
    this.requestDelimited = Boolean(requestDelimited);

    /**
     * Whether responses are length-delimited.
     * @type {boolean}
     */
    this.responseDelimited = Boolean(responseDelimited);
  }

  /**
   * Calls a service method through {@link rpc.Service#rpcImpl|rpcImpl}.
   * @param {Method|rpc.ServiceMethod<TReq,TRes>} method Reflected or static method
   * @param {Constructor<TReq>} requestCtor Request constructor
   * @param {Constructor<TRes>} responseCtor Response constructor
   * @param {TReq|Properties<TReq>} request Request message or plain object
   * @param {rpc.ServiceMethodCallback<TRes>} callback Service callback
   * @returns {undefined}
   * @template TReq extends Message<TReq>
   * @template TRes extends Message<TRes>
   */
  Service.prototype.rpcCall = function rpcCall(
    method,
    requestCtor,
    responseCtor,
    request,
    callback
  ) {
    if (!request) throw TypeError("request must be specified");

    var self = this;
    if (!callback)
      return util.asPromise(
        rpcCall,
        self,
        method,
        requestCtor,
        responseCtor,
        request
      );

    if (!self.rpcImpl) {
      setTimeout(function () {
        callback(Error("already ended"));
      }, 0);
      return undefined;
    }

    try {
      return self.rpcImpl(
        method,
        requestCtor[self.requestDelimited ? "encodeDelimited" : "encode"](
          request
        ).finish(),
        function rpcCallback(err, response) {
          if (err) {
            self.emit("error", err, method);
            return callback(err);
          }

          if (response === null) {
            self.end(/* endedByRPC */ true);
            return undefined;
          }

          if (!(response instanceof responseCtor)) {
            try {
              response =
                responseCtor[
                  self.responseDelimited ? "decodeDelimited" : "decode"
                ](response);
            } catch (err) {
              self.emit("error", err, method);
              return callback(err);
            }
          }

          self.emit("data", response, method);
          return callback(null, response);
        }
      );
    } catch (err) {
      self.emit("error", err, method);
      setTimeout(function () {
        callback(err);
      }, 0);
      return undefined;
    }
  };

  /**
   * Ends this service and emits the `end` event.
   * @param {boolean} [endedByRPC=false] Whether the service has been ended by the RPC implementation.
   * @returns {rpc.Service} `this`
   */
  Service.prototype.end = function end(endedByRPC) {
    if (this.rpcImpl) {
      if (!endedByRPC)
        // signal end to rpcImpl
        this.rpcImpl(null, null, null);
      this.rpcImpl = null;
      this.emit("end").off();
    }
    return this;
  };
  return service;
}

var hasRequiredRpc;

function requireRpc() {
  if (hasRequiredRpc) return rpc;
  hasRequiredRpc = 1;
  (function (exports) {
    /**
     * Streaming RPC helpers.
     * @namespace
     */
    var rpc = exports;

    /**
     * RPC implementation passed to {@link Service#create} performing a service request on network level, i.e. by utilizing http requests or websockets.
     * @typedef RPCImpl
     * @type {function}
     * @param {Method|rpc.ServiceMethod<Message<{}>,Message<{}>>} method Reflected or static method being called
     * @param {Uint8Array} requestData Request data
     * @param {RPCImplCallback} callback Callback function
     * @returns {undefined}
     * @example
     * function rpcImpl(method, requestData, callback) {
     *     if (protobuf.util.lcFirst(method.name) !== "myMethod") // compatible with static code
     *         throw Error("no such method");
     *     asynchronouslyObtainAResponse(requestData, function(err, responseData) {
     *         callback(err, responseData);
     *     });
     * }
     */

    /**
     * Node-style callback as used by {@link RPCImpl}.
     * @typedef RPCImplCallback
     * @type {function}
     * @param {Error|null} error Error, if any, otherwise `null`
     * @param {Uint8Array|null} [response] Response data or `null` to signal end of stream, if there hasn't been an error
     * @returns {undefined}
     */

    rpc.Service = requireService();
  })(rpc);
  return rpc;
}

var roots;
var hasRequiredRoots;

function requireRoots() {
  if (hasRequiredRoots) return roots;
  hasRequiredRoots = 1;
  roots = {};

  /**
   * Named roots.
   * This is where pbjs stores generated structures (the option `-r, --root` specifies a name).
   * Can also be used manually to make roots available across modules.
   * @name roots
   * @type {Object.<string,Root>}
   * @example
   * // pbjs -r myroot -o compiled.js ...
   *
   * // in another module:
   * require("./compiled.js");
   *
   * // in any subsequent module:
   * var root = protobuf.roots["myroot"];
   */
  return roots;
}

var hasRequiredIndexMinimal;

function requireIndexMinimal() {
  if (hasRequiredIndexMinimal) return indexMinimal;
  hasRequiredIndexMinimal = 1;
  (function (exports) {
    var protobuf = exports;

    /**
     * Build type, one of `"full"`, `"light"` or `"minimal"`.
     * @name build
     * @type {string}
     * @const
     */
    protobuf.build = "minimal";

    // Serialization
    protobuf.Writer = requireWriter();
    protobuf.BufferWriter = requireWriter_buffer();
    protobuf.Reader = requireReader();
    protobuf.BufferReader = requireReader_buffer();

    // Utility
    protobuf.util = requireMinimal$1();
    protobuf.rpc = requireRpc();
    protobuf.roots = requireRoots();
    protobuf.configure = configure;

    /* istanbul ignore next */
    /**
     * Reconfigures the library according to the environment.
     * @returns {undefined}
     */
    function configure() {
      protobuf.util._configure();
      protobuf.Writer._configure(protobuf.BufferWriter);
      protobuf.Reader._configure(protobuf.BufferReader);
    }

    // Set up buffer utility according to the environment
    configure();
  })(indexMinimal);
  return indexMinimal;
}

var minimal;
var hasRequiredMinimal;

function requireMinimal() {
  if (hasRequiredMinimal) return minimal;
  hasRequiredMinimal = 1;
  minimal = requireIndexMinimal();
  return minimal;
}

var minimalExports = requireMinimal();
var $protobuf = /*@__PURE__*/ getDefaultExportFromCjs(minimalExports);

/*eslint-disable*/

// Common aliases
const $Reader = $protobuf.Reader,
  $Writer = $protobuf.Writer,
  $util = $protobuf.util;

// Exported root namespace
const $root = $protobuf.roots.unixfs || ($protobuf.roots.unixfs = {});

const Data = ($root.Data = (() => {
  /**
   * Properties of a Data.
   * @exports IData
   * @interface IData
   * @property {Data.DataType} Type Data Type
   * @property {Uint8Array|null} [Data] Data Data
   * @property {number|null} [filesize] Data filesize
   * @property {Array.<number>|null} [blocksizes] Data blocksizes
   * @property {number|null} [hashType] Data hashType
   * @property {number|null} [fanout] Data fanout
   * @property {number|null} [mode] Data mode
   * @property {IUnixTime|null} [mtime] Data mtime
   */

  /**
   * Constructs a new Data.
   * @exports Data
   * @classdesc Represents a Data.
   * @implements IData
   * @constructor
   * @param {IData=} [p] Properties to set
   */
  function Data(p) {
    this.blocksizes = [];
    if (p)
      for (var ks = Object.keys(p), i = 0; i < ks.length; ++i)
        if (p[ks[i]] != null) this[ks[i]] = p[ks[i]];
  }

  /**
   * Data Type.
   * @member {Data.DataType} Type
   * @memberof Data
   * @instance
   */
  Data.prototype.Type = 0;

  /**
   * Data Data.
   * @member {Uint8Array} Data
   * @memberof Data
   * @instance
   */
  Data.prototype.Data = $util.newBuffer([]);

  /**
   * Data filesize.
   * @member {number} filesize
   * @memberof Data
   * @instance
   */
  Data.prototype.filesize = $util.Long ? $util.Long.fromBits(0, 0, true) : 0;

  /**
   * Data blocksizes.
   * @member {Array.<number>} blocksizes
   * @memberof Data
   * @instance
   */
  Data.prototype.blocksizes = $util.emptyArray;

  /**
   * Data hashType.
   * @member {number} hashType
   * @memberof Data
   * @instance
   */
  Data.prototype.hashType = $util.Long ? $util.Long.fromBits(0, 0, true) : 0;

  /**
   * Data fanout.
   * @member {number} fanout
   * @memberof Data
   * @instance
   */
  Data.prototype.fanout = $util.Long ? $util.Long.fromBits(0, 0, true) : 0;

  /**
   * Data mode.
   * @member {number} mode
   * @memberof Data
   * @instance
   */
  Data.prototype.mode = 0;

  /**
   * Data mtime.
   * @member {IUnixTime|null|undefined} mtime
   * @memberof Data
   * @instance
   */
  Data.prototype.mtime = null;

  /**
   * Encodes the specified Data message. Does not implicitly {@link Data.verify|verify} messages.
   * @function encode
   * @memberof Data
   * @static
   * @param {IData} m Data message or plain object to encode
   * @param {$protobuf.Writer} [w] Writer to encode to
   * @returns {$protobuf.Writer} Writer
   */
  Data.encode = function encode(m, w) {
    if (!w) w = $Writer.create();
    w.uint32(8).int32(m.Type);
    if (m.Data != null && Object.hasOwnProperty.call(m, "Data"))
      w.uint32(18).bytes(m.Data);
    if (m.filesize != null && Object.hasOwnProperty.call(m, "filesize"))
      w.uint32(24).uint64(m.filesize);
    if (m.blocksizes != null && m.blocksizes.length) {
      for (var i = 0; i < m.blocksizes.length; ++i)
        w.uint32(32).uint64(m.blocksizes[i]);
    }
    if (m.hashType != null && Object.hasOwnProperty.call(m, "hashType"))
      w.uint32(40).uint64(m.hashType);
    if (m.fanout != null && Object.hasOwnProperty.call(m, "fanout"))
      w.uint32(48).uint64(m.fanout);
    if (m.mode != null && Object.hasOwnProperty.call(m, "mode"))
      w.uint32(56).uint32(m.mode);
    if (m.mtime != null && Object.hasOwnProperty.call(m, "mtime"))
      $root.UnixTime.encode(m.mtime, w.uint32(66).fork()).ldelim();
    return w;
  };

  /**
   * Decodes a Data message from the specified reader or buffer.
   * @function decode
   * @memberof Data
   * @static
   * @param {$protobuf.Reader|Uint8Array} r Reader or buffer to decode from
   * @param {number} [l] Message length if known beforehand
   * @returns {Data} Data
   * @throws {Error} If the payload is not a reader or valid buffer
   * @throws {$protobuf.util.ProtocolError} If required fields are missing
   */
  Data.decode = function decode(r, l) {
    if (!(r instanceof $Reader)) r = $Reader.create(r);
    var c = l === undefined ? r.len : r.pos + l,
      m = new $root.Data();
    while (r.pos < c) {
      var t = r.uint32();
      switch (t >>> 3) {
        case 1:
          m.Type = r.int32();
          break;
        case 2:
          m.Data = r.bytes();
          break;
        case 3:
          m.filesize = r.uint64();
          break;
        case 4:
          if (!(m.blocksizes && m.blocksizes.length)) m.blocksizes = [];
          if ((t & 7) === 2) {
            var c2 = r.uint32() + r.pos;
            while (r.pos < c2) m.blocksizes.push(r.uint64());
          } else m.blocksizes.push(r.uint64());
          break;
        case 5:
          m.hashType = r.uint64();
          break;
        case 6:
          m.fanout = r.uint64();
          break;
        case 7:
          m.mode = r.uint32();
          break;
        case 8:
          m.mtime = $root.UnixTime.decode(r, r.uint32());
          break;
        default:
          r.skipType(t & 7);
          break;
      }
    }
    if (!m.hasOwnProperty("Type"))
      throw $util.ProtocolError("missing required 'Type'", { instance: m });
    return m;
  };

  /**
   * Creates a Data message from a plain object. Also converts values to their respective internal types.
   * @function fromObject
   * @memberof Data
   * @static
   * @param {Object.<string,*>} d Plain object
   * @returns {Data} Data
   */
  Data.fromObject = function fromObject(d) {
    if (d instanceof $root.Data) return d;
    var m = new $root.Data();
    switch (d.Type) {
      case "Raw":
      case 0:
        m.Type = 0;
        break;
      case "Directory":
      case 1:
        m.Type = 1;
        break;
      case "File":
      case 2:
        m.Type = 2;
        break;
      case "Metadata":
      case 3:
        m.Type = 3;
        break;
      case "Symlink":
      case 4:
        m.Type = 4;
        break;
      case "HAMTShard":
      case 5:
        m.Type = 5;
        break;
    }
    if (d.Data != null) {
      if (typeof d.Data === "string")
        $util.base64.decode(
          d.Data,
          (m.Data = $util.newBuffer($util.base64.length(d.Data))),
          0
        );
      else if (d.Data.length) m.Data = d.Data;
    }
    if (d.filesize != null) {
      if ($util.Long)
        (m.filesize = $util.Long.fromValue(d.filesize)).unsigned = true;
      else if (typeof d.filesize === "string")
        m.filesize = parseInt(d.filesize, 10);
      else if (typeof d.filesize === "number") m.filesize = d.filesize;
      else if (typeof d.filesize === "object")
        m.filesize = new $util.LongBits(
          d.filesize.low >>> 0,
          d.filesize.high >>> 0
        ).toNumber(true);
    }
    if (d.blocksizes) {
      if (!Array.isArray(d.blocksizes))
        throw TypeError(".Data.blocksizes: array expected");
      m.blocksizes = [];
      for (var i = 0; i < d.blocksizes.length; ++i) {
        if ($util.Long)
          (m.blocksizes[i] = $util.Long.fromValue(
            d.blocksizes[i]
          )).unsigned = true;
        else if (typeof d.blocksizes[i] === "string")
          m.blocksizes[i] = parseInt(d.blocksizes[i], 10);
        else if (typeof d.blocksizes[i] === "number")
          m.blocksizes[i] = d.blocksizes[i];
        else if (typeof d.blocksizes[i] === "object")
          m.blocksizes[i] = new $util.LongBits(
            d.blocksizes[i].low >>> 0,
            d.blocksizes[i].high >>> 0
          ).toNumber(true);
      }
    }
    if (d.hashType != null) {
      if ($util.Long)
        (m.hashType = $util.Long.fromValue(d.hashType)).unsigned = true;
      else if (typeof d.hashType === "string")
        m.hashType = parseInt(d.hashType, 10);
      else if (typeof d.hashType === "number") m.hashType = d.hashType;
      else if (typeof d.hashType === "object")
        m.hashType = new $util.LongBits(
          d.hashType.low >>> 0,
          d.hashType.high >>> 0
        ).toNumber(true);
    }
    if (d.fanout != null) {
      if ($util.Long)
        (m.fanout = $util.Long.fromValue(d.fanout)).unsigned = true;
      else if (typeof d.fanout === "string") m.fanout = parseInt(d.fanout, 10);
      else if (typeof d.fanout === "number") m.fanout = d.fanout;
      else if (typeof d.fanout === "object")
        m.fanout = new $util.LongBits(
          d.fanout.low >>> 0,
          d.fanout.high >>> 0
        ).toNumber(true);
    }
    if (d.mode != null) {
      m.mode = d.mode >>> 0;
    }
    if (d.mtime != null) {
      if (typeof d.mtime !== "object")
        throw TypeError(".Data.mtime: object expected");
      m.mtime = $root.UnixTime.fromObject(d.mtime);
    }
    return m;
  };

  /**
   * Creates a plain object from a Data message. Also converts values to other types if specified.
   * @function toObject
   * @memberof Data
   * @static
   * @param {Data} m Data
   * @param {$protobuf.IConversionOptions} [o] Conversion options
   * @returns {Object.<string,*>} Plain object
   */
  Data.toObject = function toObject(m, o) {
    if (!o) o = {};
    var d = {};
    if (o.arrays || o.defaults) {
      d.blocksizes = [];
    }
    if (o.defaults) {
      d.Type = o.enums === String ? "Raw" : 0;
      if (o.bytes === String) d.Data = "";
      else {
        d.Data = [];
        if (o.bytes !== Array) d.Data = $util.newBuffer(d.Data);
      }
      if ($util.Long) {
        var n = new $util.Long(0, 0, true);
        d.filesize =
          o.longs === String
            ? n.toString()
            : o.longs === Number
            ? n.toNumber()
            : n;
      } else d.filesize = o.longs === String ? "0" : 0;
      if ($util.Long) {
        var n = new $util.Long(0, 0, true);
        d.hashType =
          o.longs === String
            ? n.toString()
            : o.longs === Number
            ? n.toNumber()
            : n;
      } else d.hashType = o.longs === String ? "0" : 0;
      if ($util.Long) {
        var n = new $util.Long(0, 0, true);
        d.fanout =
          o.longs === String
            ? n.toString()
            : o.longs === Number
            ? n.toNumber()
            : n;
      } else d.fanout = o.longs === String ? "0" : 0;
      d.mode = 0;
      d.mtime = null;
    }
    if (m.Type != null && m.hasOwnProperty("Type")) {
      d.Type = o.enums === String ? $root.Data.DataType[m.Type] : m.Type;
    }
    if (m.Data != null && m.hasOwnProperty("Data")) {
      d.Data =
        o.bytes === String
          ? $util.base64.encode(m.Data, 0, m.Data.length)
          : o.bytes === Array
          ? Array.prototype.slice.call(m.Data)
          : m.Data;
    }
    if (m.filesize != null && m.hasOwnProperty("filesize")) {
      if (typeof m.filesize === "number")
        d.filesize = o.longs === String ? String(m.filesize) : m.filesize;
      else
        d.filesize =
          o.longs === String
            ? $util.Long.prototype.toString.call(m.filesize)
            : o.longs === Number
            ? new $util.LongBits(
                m.filesize.low >>> 0,
                m.filesize.high >>> 0
              ).toNumber(true)
            : m.filesize;
    }
    if (m.blocksizes && m.blocksizes.length) {
      d.blocksizes = [];
      for (var j = 0; j < m.blocksizes.length; ++j) {
        if (typeof m.blocksizes[j] === "number")
          d.blocksizes[j] =
            o.longs === String ? String(m.blocksizes[j]) : m.blocksizes[j];
        else
          d.blocksizes[j] =
            o.longs === String
              ? $util.Long.prototype.toString.call(m.blocksizes[j])
              : o.longs === Number
              ? new $util.LongBits(
                  m.blocksizes[j].low >>> 0,
                  m.blocksizes[j].high >>> 0
                ).toNumber(true)
              : m.blocksizes[j];
      }
    }
    if (m.hashType != null && m.hasOwnProperty("hashType")) {
      if (typeof m.hashType === "number")
        d.hashType = o.longs === String ? String(m.hashType) : m.hashType;
      else
        d.hashType =
          o.longs === String
            ? $util.Long.prototype.toString.call(m.hashType)
            : o.longs === Number
            ? new $util.LongBits(
                m.hashType.low >>> 0,
                m.hashType.high >>> 0
              ).toNumber(true)
            : m.hashType;
    }
    if (m.fanout != null && m.hasOwnProperty("fanout")) {
      if (typeof m.fanout === "number")
        d.fanout = o.longs === String ? String(m.fanout) : m.fanout;
      else
        d.fanout =
          o.longs === String
            ? $util.Long.prototype.toString.call(m.fanout)
            : o.longs === Number
            ? new $util.LongBits(
                m.fanout.low >>> 0,
                m.fanout.high >>> 0
              ).toNumber(true)
            : m.fanout;
    }
    if (m.mode != null && m.hasOwnProperty("mode")) {
      d.mode = m.mode;
    }
    if (m.mtime != null && m.hasOwnProperty("mtime")) {
      d.mtime = $root.UnixTime.toObject(m.mtime, o);
    }
    return d;
  };

  /**
   * Converts this Data to JSON.
   * @function toJSON
   * @memberof Data
   * @instance
   * @returns {Object.<string,*>} JSON object
   */
  Data.prototype.toJSON = function toJSON() {
    return this.constructor.toObject(this, $protobuf.util.toJSONOptions);
  };

  /**
   * DataType enum.
   * @name Data.DataType
   * @enum {number}
   * @property {number} Raw=0 Raw value
   * @property {number} Directory=1 Directory value
   * @property {number} File=2 File value
   * @property {number} Metadata=3 Metadata value
   * @property {number} Symlink=4 Symlink value
   * @property {number} HAMTShard=5 HAMTShard value
   */
  Data.DataType = (function () {
    const valuesById = {},
      values = Object.create(valuesById);
    values[(valuesById[0] = "Raw")] = 0;
    values[(valuesById[1] = "Directory")] = 1;
    values[(valuesById[2] = "File")] = 2;
    values[(valuesById[3] = "Metadata")] = 3;
    values[(valuesById[4] = "Symlink")] = 4;
    values[(valuesById[5] = "HAMTShard")] = 5;
    return values;
  })();

  return Data;
})());

$root.UnixTime = (() => {
  /**
   * Properties of an UnixTime.
   * @exports IUnixTime
   * @interface IUnixTime
   * @property {number} Seconds UnixTime Seconds
   * @property {number|null} [FractionalNanoseconds] UnixTime FractionalNanoseconds
   */

  /**
   * Constructs a new UnixTime.
   * @exports UnixTime
   * @classdesc Represents an UnixTime.
   * @implements IUnixTime
   * @constructor
   * @param {IUnixTime=} [p] Properties to set
   */
  function UnixTime(p) {
    if (p)
      for (var ks = Object.keys(p), i = 0; i < ks.length; ++i)
        if (p[ks[i]] != null) this[ks[i]] = p[ks[i]];
  }

  /**
   * UnixTime Seconds.
   * @member {number} Seconds
   * @memberof UnixTime
   * @instance
   */
  UnixTime.prototype.Seconds = $util.Long
    ? $util.Long.fromBits(0, 0, false)
    : 0;

  /**
   * UnixTime FractionalNanoseconds.
   * @member {number} FractionalNanoseconds
   * @memberof UnixTime
   * @instance
   */
  UnixTime.prototype.FractionalNanoseconds = 0;

  /**
   * Encodes the specified UnixTime message. Does not implicitly {@link UnixTime.verify|verify} messages.
   * @function encode
   * @memberof UnixTime
   * @static
   * @param {IUnixTime} m UnixTime message or plain object to encode
   * @param {$protobuf.Writer} [w] Writer to encode to
   * @returns {$protobuf.Writer} Writer
   */
  UnixTime.encode = function encode(m, w) {
    if (!w) w = $Writer.create();
    w.uint32(8).int64(m.Seconds);
    if (
      m.FractionalNanoseconds != null &&
      Object.hasOwnProperty.call(m, "FractionalNanoseconds")
    )
      w.uint32(21).fixed32(m.FractionalNanoseconds);
    return w;
  };

  /**
   * Decodes an UnixTime message from the specified reader or buffer.
   * @function decode
   * @memberof UnixTime
   * @static
   * @param {$protobuf.Reader|Uint8Array} r Reader or buffer to decode from
   * @param {number} [l] Message length if known beforehand
   * @returns {UnixTime} UnixTime
   * @throws {Error} If the payload is not a reader or valid buffer
   * @throws {$protobuf.util.ProtocolError} If required fields are missing
   */
  UnixTime.decode = function decode(r, l) {
    if (!(r instanceof $Reader)) r = $Reader.create(r);
    var c = l === undefined ? r.len : r.pos + l,
      m = new $root.UnixTime();
    while (r.pos < c) {
      var t = r.uint32();
      switch (t >>> 3) {
        case 1:
          m.Seconds = r.int64();
          break;
        case 2:
          m.FractionalNanoseconds = r.fixed32();
          break;
        default:
          r.skipType(t & 7);
          break;
      }
    }
    if (!m.hasOwnProperty("Seconds"))
      throw $util.ProtocolError("missing required 'Seconds'", { instance: m });
    return m;
  };

  /**
   * Creates an UnixTime message from a plain object. Also converts values to their respective internal types.
   * @function fromObject
   * @memberof UnixTime
   * @static
   * @param {Object.<string,*>} d Plain object
   * @returns {UnixTime} UnixTime
   */
  UnixTime.fromObject = function fromObject(d) {
    if (d instanceof $root.UnixTime) return d;
    var m = new $root.UnixTime();
    if (d.Seconds != null) {
      if ($util.Long)
        (m.Seconds = $util.Long.fromValue(d.Seconds)).unsigned = false;
      else if (typeof d.Seconds === "string")
        m.Seconds = parseInt(d.Seconds, 10);
      else if (typeof d.Seconds === "number") m.Seconds = d.Seconds;
      else if (typeof d.Seconds === "object")
        m.Seconds = new $util.LongBits(
          d.Seconds.low >>> 0,
          d.Seconds.high >>> 0
        ).toNumber();
    }
    if (d.FractionalNanoseconds != null) {
      m.FractionalNanoseconds = d.FractionalNanoseconds >>> 0;
    }
    return m;
  };

  /**
   * Creates a plain object from an UnixTime message. Also converts values to other types if specified.
   * @function toObject
   * @memberof UnixTime
   * @static
   * @param {UnixTime} m UnixTime
   * @param {$protobuf.IConversionOptions} [o] Conversion options
   * @returns {Object.<string,*>} Plain object
   */
  UnixTime.toObject = function toObject(m, o) {
    if (!o) o = {};
    var d = {};
    if (o.defaults) {
      if ($util.Long) {
        var n = new $util.Long(0, 0, false);
        d.Seconds =
          o.longs === String
            ? n.toString()
            : o.longs === Number
            ? n.toNumber()
            : n;
      } else d.Seconds = o.longs === String ? "0" : 0;
      d.FractionalNanoseconds = 0;
    }
    if (m.Seconds != null && m.hasOwnProperty("Seconds")) {
      if (typeof m.Seconds === "number")
        d.Seconds = o.longs === String ? String(m.Seconds) : m.Seconds;
      else
        d.Seconds =
          o.longs === String
            ? $util.Long.prototype.toString.call(m.Seconds)
            : o.longs === Number
            ? new $util.LongBits(
                m.Seconds.low >>> 0,
                m.Seconds.high >>> 0
              ).toNumber()
            : m.Seconds;
    }
    if (
      m.FractionalNanoseconds != null &&
      m.hasOwnProperty("FractionalNanoseconds")
    ) {
      d.FractionalNanoseconds = m.FractionalNanoseconds;
    }
    return d;
  };

  /**
   * Converts this UnixTime to JSON.
   * @function toJSON
   * @memberof UnixTime
   * @instance
   * @returns {Object.<string,*>} JSON object
   */
  UnixTime.prototype.toJSON = function toJSON() {
    return this.constructor.toObject(this, $protobuf.util.toJSONOptions);
  };

  return UnixTime;
})();

$root.Metadata = (() => {
  /**
   * Properties of a Metadata.
   * @exports IMetadata
   * @interface IMetadata
   * @property {string|null} [MimeType] Metadata MimeType
   */

  /**
   * Constructs a new Metadata.
   * @exports Metadata
   * @classdesc Represents a Metadata.
   * @implements IMetadata
   * @constructor
   * @param {IMetadata=} [p] Properties to set
   */
  function Metadata(p) {
    if (p)
      for (var ks = Object.keys(p), i = 0; i < ks.length; ++i)
        if (p[ks[i]] != null) this[ks[i]] = p[ks[i]];
  }

  /**
   * Metadata MimeType.
   * @member {string} MimeType
   * @memberof Metadata
   * @instance
   */
  Metadata.prototype.MimeType = "";

  /**
   * Encodes the specified Metadata message. Does not implicitly {@link Metadata.verify|verify} messages.
   * @function encode
   * @memberof Metadata
   * @static
   * @param {IMetadata} m Metadata message or plain object to encode
   * @param {$protobuf.Writer} [w] Writer to encode to
   * @returns {$protobuf.Writer} Writer
   */
  Metadata.encode = function encode(m, w) {
    if (!w) w = $Writer.create();
    if (m.MimeType != null && Object.hasOwnProperty.call(m, "MimeType"))
      w.uint32(10).string(m.MimeType);
    return w;
  };

  /**
   * Decodes a Metadata message from the specified reader or buffer.
   * @function decode
   * @memberof Metadata
   * @static
   * @param {$protobuf.Reader|Uint8Array} r Reader or buffer to decode from
   * @param {number} [l] Message length if known beforehand
   * @returns {Metadata} Metadata
   * @throws {Error} If the payload is not a reader or valid buffer
   * @throws {$protobuf.util.ProtocolError} If required fields are missing
   */
  Metadata.decode = function decode(r, l) {
    if (!(r instanceof $Reader)) r = $Reader.create(r);
    var c = l === undefined ? r.len : r.pos + l,
      m = new $root.Metadata();
    while (r.pos < c) {
      var t = r.uint32();
      switch (t >>> 3) {
        case 1:
          m.MimeType = r.string();
          break;
        default:
          r.skipType(t & 7);
          break;
      }
    }
    return m;
  };

  /**
   * Creates a Metadata message from a plain object. Also converts values to their respective internal types.
   * @function fromObject
   * @memberof Metadata
   * @static
   * @param {Object.<string,*>} d Plain object
   * @returns {Metadata} Metadata
   */
  Metadata.fromObject = function fromObject(d) {
    if (d instanceof $root.Metadata) return d;
    var m = new $root.Metadata();
    if (d.MimeType != null) {
      m.MimeType = String(d.MimeType);
    }
    return m;
  };

  /**
   * Creates a plain object from a Metadata message. Also converts values to other types if specified.
   * @function toObject
   * @memberof Metadata
   * @static
   * @param {Metadata} m Metadata
   * @param {$protobuf.IConversionOptions} [o] Conversion options
   * @returns {Object.<string,*>} Plain object
   */
  Metadata.toObject = function toObject(m, o) {
    if (!o) o = {};
    var d = {};
    if (o.defaults) {
      d.MimeType = "";
    }
    if (m.MimeType != null && m.hasOwnProperty("MimeType")) {
      d.MimeType = m.MimeType;
    }
    return d;
  };

  /**
   * Converts this Metadata to JSON.
   * @function toJSON
   * @memberof Metadata
   * @instance
   * @returns {Object.<string,*>} JSON object
   */
  Metadata.prototype.toJSON = function toJSON() {
    return this.constructor.toObject(this, $protobuf.util.toJSONOptions);
  };

  return Metadata;
})();

// @ts-nocheck

const NodeType = Data.DataType;

/** @type {ReadonlyArray<any>} */
const EMPTY$3 = Object.freeze([]);
const EMPTY_BUFFER$1 = new Uint8Array(0);

const BLANK$2 = Object.freeze({});
const DEFAULT_FILE_MODE = parseInt("0644", 8);
const DEFAULT_DIRECTORY_MODE = parseInt("0755", 8);

const code$1 = code$2;
const name$1 = "UnixFS";

/**
 * @param {UnixFS.IData} data
 * @param {ReadonlyArray<UnixFS.PBLink>} links
 */
const encodePB = (data, links) => {
  Object(globalThis).debug && console.log({ data, links });

  return encode$6(
    // We run through prepare as links need to be sorted by name which it will
    // do.
    prepare({
      Data: Data.encode(data).finish(),
      // We can cast to mutable array as we know no mutation occurs there
      Links: /** @type {PB.PBLink[]} */ (links),
    })
  );
};

/**
 * @param {Uint8Array} content
 * @returns {UnixFS.Raw}
 */
const createRaw = (content) => ({
  type: NodeType.Raw,
  content,
});

/**
 * @param {UnixFS.Metadata} [metadata]
 * @returns {UnixFS.SimpleFile}
 */
const createEmptyFile = (metadata) =>
  createSimpleFile(EMPTY_BUFFER$1, metadata);

/**
 * @param {Uint8Array} content
 * @param {UnixFS.Metadata} [metadata]
 * @returns {UnixFS.SimpleFile}
 */
const createSimpleFile = (content, metadata) => ({
  type: NodeType.File,
  layout: "simple",
  content,
  metadata: decodeMetadata(metadata),
});

/**
 * @param {Uint8Array} content
 * @returns {UnixFS.FileChunk}
 */
const createFileChunk = (content) => ({
  type: NodeType.File,
  layout: "simple",
  content,
});

/**
 * @param {UnixFS.FileLink[]} parts
 * @param {UnixFS.Metadata} [metadata]
 * @returns {UnixFS.AdvancedFile}
 */
const createAdvancedFile = (parts, metadata) => ({
  type: NodeType.File,
  layout: "advanced",
  parts,
  metadata: decodeMetadata(metadata),
});

/**
 * @param {UnixFS.FileLink[]} parts
 * @returns {UnixFS.FileShard}
 */
const createFileShard = (parts) => ({
  type: NodeType.File,
  layout: "advanced",
  parts,
});

/**
 * @deprecated
 * @param {Uint8Array} content
 * @param {UnixFS.FileLink[]} parts
 * @param {UnixFS.Metadata} [metadata]
 * @returns {UnixFS.ComplexFile}
 */
const createComplexFile = (content, parts, metadata) => ({
  type: NodeType.File,
  layout: "complex",
  content,
  parts,
  metadata: decodeMetadata(metadata),
});

/**
 * @param {UnixFS.DirectoryEntryLink[]} entries
 * @param {UnixFS.Metadata} [metadata]
 * @returns {UnixFS.FlatDirectory}
 */
const createFlatDirectory = (entries, metadata) => ({
  type: NodeType.Directory,
  metadata: decodeMetadata(metadata),
  entries,
});

/**
 * @param {UnixFS.ShardedDirectoryLink[]} entries
 * @param {Uint8Array} bitfield
 * @param {number} fanout
 * @param {number} hashType
 * @param {UnixFS.Metadata} [metadata]
 * @returns {UnixFS.ShardedDirectory}
 */
const createShardedDirectory = (
  entries,
  bitfield,
  fanout,
  hashType,
  metadata = BLANK$2
) => ({
  type: NodeType.HAMTShard,
  bitfield,
  fanout: readFanout(fanout),
  hashType: readInt(hashType),
  entries,
  metadata: decodeMetadata(metadata),
});

/**
 * @param {UnixFS.ShardedDirectoryLink[]} entries
 * @param {Uint8Array} bitfield
 * @param {number} fanout
 * @param {number} hashType
 * @returns {UnixFS.DirectoryShard}
 */
const createDirectoryShard = (entries, bitfield, fanout, hashType) => ({
  type: NodeType.HAMTShard,
  bitfield,
  fanout: readFanout(fanout),
  hashType: readInt(hashType),
  entries,
});

/**
 *
 * @param {Uint8Array} content
 * @returns {UnixFS.ByteView<UnixFS.Raw>}
 */
const encodeRaw = (content) =>
  encodePB(
    {
      Type: NodeType.Raw,
      // TODO:
      Data: content.byteLength > 0 ? content : undefined,
      filesize: content.byteLength,
      // @ts-ignore
      blocksizes: EMPTY$3,
    },
    []
  );

/**
 * @param {UnixFS.File|UnixFS.FileChunk|UnixFS.FileShard} node
 * @param {boolean} [ignoreMetadata]
 * @returns {UnixFS.ByteView<UnixFS.SimpleFile|UnixFS.AdvancedFile|UnixFS.ComplexFile>}
 */
const encodeFile = (node, ignoreMetadata = false) => {
  const metadata = ignoreMetadata ? BLANK$2 : Object(node).metadata;
  switch (node.layout) {
    case "simple":
      return encodeSimpleFile(node.content, metadata);
    case "advanced":
      return encodeAdvancedFile(node.parts, metadata);
    case "complex":
      return encodeComplexFile(node.content, node.parts, metadata);
    default:
      throw new TypeError(
        `File with unknown layout "${Object(node).layout}" was passed`
      );
  }
};

/**
 * @param {Uint8Array} content
 * @returns {UnixFS.ByteView<UnixFS.FileChunk>}
 */
const encodeFileChunk = (content) => encodeSimpleFile(content, BLANK$2);

/**
 * @param {ReadonlyArray<UnixFS.FileLink>} parts
 * @returns {UnixFS.ByteView<UnixFS.FileShard>}
 */
const encodeFileShard = (parts) =>
  encodePB(
    {
      Type: NodeType.File,
      blocksizes: parts.map(contentByteLength),
      filesize: cumulativeContentByteLength(parts),
    },
    parts.map(encodeLink)
  );

/**
 * @param {ReadonlyArray<UnixFS.FileLink>} parts
 * @param {UnixFS.Metadata} [metadata]
 * @returns {UnixFS.ByteView<UnixFS.AdvancedFile>}
 */
const encodeAdvancedFile = (parts, metadata = BLANK$2) =>
  encodePB(
    {
      Type: NodeType.File,
      blocksizes: parts.map(contentByteLength),
      filesize: cumulativeContentByteLength(parts),

      ...encodeMetadata(metadata),
    },
    parts.map(encodeLink)
  );

/**
 * @param {UnixFS.DAGLink} dag
 * @returns {UnixFS.PBLink}
 */
const encodeLink = (dag) => ({
  Name: "",
  Tsize: dag.dagByteLength,
  // @ts-ignore - @see https://github.com/multiformats/js-multiformats/pull/161
  Hash: dag.cid,
});

/**
 * @param {Uint8Array} content
 * @param {UnixFS.Metadata} [metadata]
 * @returns {UnixFS.ByteView<UnixFS.SimpleFile>}
 */

const encodeSimpleFile = (content, metadata = BLANK$2) =>
  encodePB(
    {
      Type: NodeType.File,
      // adding empty file to both go-ipfs and js-ipfs produces block in
      // which `Data` is omitted but filesize and blocksizes are present.
      // For the sake of hash consistency we do the same.
      Data: content.byteLength > 0 ? content : undefined,
      filesize: content.byteLength,
      blocksizes: [],
      ...encodeMetadata(metadata),
    },
    []
  );

/**
 *
 * @param {Uint8Array} content
 * @param {ReadonlyArray<UnixFS.FileLink>} parts
 * @param {UnixFS.Metadata} [metadata]
 * @returns {UnixFS.ByteView<UnixFS.ComplexFile>}
 */
const encodeComplexFile = (content, parts, metadata = BLANK$2) =>
  encodePB(
    {
      Type: NodeType.File,
      Data: content,
      filesize: content.byteLength + cumulativeContentByteLength(parts),
      blocksizes: parts.map(contentByteLength),
    },
    parts.map(encodeLink)
  );

/**
 * @param {UnixFS.FlatDirectory} node
 * @returns {UnixFS.ByteView<UnixFS.FlatDirectory>}
 */
const encodeDirectory = (node) =>
  encodePB(
    {
      Type: node.type,
      ...encodeDirectoryMetadata(node.metadata || BLANK$2),
    },
    node.entries.map(encodeNamedLink)
  );

/**
 * @param {UnixFS.ShardedDirectory|UnixFS.DirectoryShard} node
 * @returns {UnixFS.ByteView<UnixFS.ShardedDirectory>}
 */
const encodeHAMTShard = ({
  bitfield,
  fanout,
  hashType,
  entries,
  metadata = BLANK$2,
}) =>
  encodePB(
    {
      Type: NodeType.HAMTShard,
      Data: bitfield.byteLength > 0 ? bitfield : undefined,
      fanout: readFanout(fanout),
      hashType: readInt(hashType),

      ...encodeDirectoryMetadata(metadata),
    },
    entries.map(encodeNamedLink)
  );

/**
 * @param {number} n
 * @returns {number}
 */
const readFanout = (n) => {
  if (Math.log2(n) % 1 === 0) {
    return n;
  } else {
    throw new TypeError(
      `Expected hamt size to be a power of two instead got ${n}`
    );
  }
};

/**
 * @param {number} n
 * @returns {number}
 */

const readInt = (n) => {
  if (Number.isInteger(n)) {
    return n;
  } else {
    throw new TypeError(`Expected an integer value instead got ${n}`);
  }
};

/**
 * @param {Uint8Array} path
 * @param {UnixFS.Metadata} [metadata]
 * @returns {UnixFS.Symlink}
 */
const createSymlink = (path, metadata = BLANK$2) => ({
  type: NodeType.Symlink,
  content: path,
  metadata: decodeMetadata(metadata),
});

/**
 * @param {UnixFS.Symlink} node
 * @param {boolean} [ignoreMetadata]
 * @returns {UnixFS.ByteView<UnixFS.Symlink>}
 */
const encodeSymlink = (node, ignoreMetadata = false) => {
  const metadata = ignoreMetadata ? BLANK$2 : Object(node).metadata;
  // We do not include filesize on symlinks because that is what go-ipfs does
  // when doing `ipfs add mysymlink`. js-ipfs on the other hand seems to store
  // it, here we choose to follow go-ipfs
  // @see https://explore.ipld.io/#/explore/QmPZ1CTc5fYErTH2XXDGrfsPsHicYXtkZeVojGycwAfm3v
  // @see https://github.com/ipfs/js-ipfs-unixfs/issues/195
  return encodePB(
    {
      Type: NodeType.Symlink,
      Data: node.content,
      ...encodeMetadata(metadata || BLANK$2),
    },
    []
  );
};

/**
 * @template {UnixFS.Node} T
 * @param {T} node
 * @param {boolean} root
 */
const encode$5 = (node, root = true) => {
  switch (node.type) {
    case NodeType.Raw:
      return encodeRaw(node.content);
    case NodeType.File:
      return encodeFile(node);
    case NodeType.Directory:
      return encodeDirectory(node);
    case NodeType.HAMTShard:
      return encodeHAMTShard(node);
    case NodeType.Symlink:
      return encodeSymlink(node);
    default:
      throw new Error(`Unknown node type ${Object(node).type}`);
  }
};

/**
 * @param {UnixFS.ByteView<UnixFS.Node>} bytes
 * @returns {UnixFS.Node}
 */
const decode$6 = (bytes) => {
  const pb = decode$7(bytes);
  const message = Data.decode(/** @type {Uint8Array} */ (pb.Data));

  const {
    Type: type,
    Data: data,
    mtime,
    mode,
    blocksizes,
    ...rest
  } = Data.toObject(message, {
    defaults: false,
    arrays: true,
    longs: Number,
    objects: false,
  });
  const metadata = {
    ...(mode && { mode }),
    ...decodeMtime(mtime),
  };
  /** @type {UnixFS.PBLink[]} */
  const links = pb.Links;

  switch (message.Type) {
    case NodeType.Raw:
      return createRaw(data);
    case NodeType.File:
      if (links.length === 0) {
        return new SimpleFileView(data, metadata);
      } else if (data.byteLength === 0) {
        return new AdvancedFileView(
          decodeFileLinks(rest.blocksizes, links),
          metadata
        );
      } else {
        return new ComplexFileView(
          data,
          decodeFileLinks(rest.blocksizes, links),
          metadata
        );
      }
    case NodeType.Directory:
      return createFlatDirectory(decodeDirectoryLinks(links), metadata);
    case NodeType.HAMTShard:
      return createShardedDirectory(
        decodeDirectoryLinks(links),
        data || EMPTY_BUFFER$1,
        rest.fanout,
        rest.hashType,
        metadata
      );
    case NodeType.Symlink:
      return createSymlink(data, metadata);
    default:
      throw new TypeError(`Unsupported node type ${message.Type}`);
  }
};

/**
 * @param {UnixFS.UnixTime|undefined} mtime
 */
const decodeMtime = (mtime) =>
  mtime == null
    ? undefined
    : {
        mtime: { secs: mtime.Seconds, nsecs: mtime.FractionalNanoseconds || 0 },
      };

/**
 *
 * @param {number[]} blocksizes
 * @param {UnixFS.PBLink[]} links
 * @returns {UnixFS.FileLink[]}
 */

const decodeFileLinks = (blocksizes, links) => {
  const parts = [];
  const length = blocksizes.length;
  let n = 0;
  while (n < length) {
    parts.push(
      /** @type {UnixFS.FileLink} */ ({
        cid: links[n].Hash,
        dagByteLength: links[n].Tsize || 0,
        contentByteLength: blocksizes[n],
      })
    );
  }
  return parts;
};

/**
 * @param {UnixFS.PBLink[]} links
 * @returns {UnixFS.DirectoryEntryLink[]}
 */
const decodeDirectoryLinks = (links) =>
  links.map(
    (link) =>
      /** @type {UnixFS.DirectoryEntryLink} */ ({
        cid: link.Hash,
        name: link.Name || "",
        dagByteLength: link.Tsize || 0,
      })
  );

/**
 * @param {ReadonlyArray<UnixFS.FileLink>} links
 * @returns {number}
 */
const cumulativeContentByteLength = (links) =>
  links.reduce((size, link) => size + link.contentByteLength, 0);

/**
 * @param {Uint8Array} root
 * @param {ReadonlyArray<UnixFS.DAGLink>} links
 * @returns {number}
 */
const cumulativeDagByteLength = (root, links) =>
  links.reduce((size, link) => size + link.dagByteLength, root.byteLength);

/**
 *
 * @param {UnixFS.FileLink} link
 */
const contentByteLength = (link) => link.contentByteLength;

/**
 * @param {UnixFS.NamedDAGLink<unknown>} link
 * @returns {UnixFS.PBLink}
 */
const encodeNamedLink = ({ name, dagByteLength, cid }) => ({
  Name: name,
  Tsize: dagByteLength,
  Hash: cid,
});

/**
 * @param {UnixFS.Metadata} metadata
 */
const encodeDirectoryMetadata = (metadata) =>
  encodeMetadata(metadata, DEFAULT_DIRECTORY_MODE);

/**
 * @param {UnixFS.Metadata} metadata
 * @param {UnixFS.Mode} defaultMode
 */
const encodeMetadata = ({ mode, mtime }, defaultMode = DEFAULT_FILE_MODE) => ({
  mode: mode != null ? encodeMode(mode, defaultMode) : undefined,
  mtime: mtime != null ? encodeMTime(mtime) : undefined,
});

/**
 * @param {UnixFS.Metadata} [data]
 */
const decodeMetadata = (data) =>
  data == null
    ? BLANK$2
    : {
        ...(data.mode == null ? undefined : { mode: decodeMode(data.mode) }),
        ...(data.mtime == null ? undefined : { mtime: data.mtime }),
      };

/**
 * @param {UnixFS.MTime} mtime
 */
const encodeMTime = (mtime) => {
  return mtime == null
    ? undefined
    : mtime.nsecs !== 0
    ? { Seconds: mtime.secs, FractionalNanoseconds: mtime.nsecs }
    : { Seconds: mtime.secs };
};

/**
 * @param {number} specifiedMode
 * @param {number} defaultMode
 */
const encodeMode = (specifiedMode, defaultMode) => {
  const mode = specifiedMode == null ? undefined : decodeMode(specifiedMode);
  return mode === defaultMode || mode == null ? undefined : mode;
};

/**
 * @param {UnixFS.Mode} mode
 * @returns {UnixFS.Mode}
 */
const decodeMode = (mode) => (mode & 0xfff) | (mode & 0xfffff000);

/**
 * @param {{content?: Uint8Array, parts?: ReadonlyArray<UnixFS.FileLink>, metadata?: UnixFS.Metadata }} node
 * @returns {UnixFS.SimpleFile|UnixFS.AdvancedFile|UnixFS.ComplexFile}
 */
const matchFile = ({
  content = EMPTY_BUFFER$1,
  parts = EMPTY$3,
  metadata = BLANK$2,
  ...rest
}) => {
  if (parts.length === 0) {
    return new SimpleFileView(content, metadata);
  } else if (content.byteLength === 0) {
    return new AdvancedFileView(parts, metadata);
  } else {
    return new ComplexFileView(content, parts, metadata);
  }
};

/**
 * @implements {UnixFS.SimpleFile}
 */
class SimpleFileView {
  /**
   * @param {Uint8Array} content
   * @param {UnixFS.Metadata} metadata
   */
  constructor(content, metadata) {
    this.content = content;
    this.metadata = metadata;
    /**
     * @readonly
     * @type {"simple"}
     */
    this.layout = "simple";
    /**
     * @readonly
     * @type {NodeType.File}
     */
    this.type = NodeType.File;
  }

  get filesize() {
    return this.content.byteLength;
  }

  encode() {
    return encodeSimpleFile(this.content, this.metadata);
  }
}

/**
 * @implements {UnixFS.AdvancedFile}
 */
class AdvancedFileView {
  /**
   * @param {ReadonlyArray<UnixFS.FileLink>} parts
   * @param {UnixFS.Metadata} metadata
   */
  constructor(parts, metadata) {
    this.parts = parts;
    this.metadata = metadata;
  }
  /** @type {"advanced"} */
  get layout() {
    return "advanced";
  }

  /**
   * @returns {NodeType.File}
   */
  get type() {
    return NodeType.File;
  }
  get fileSize() {
    return cumulativeContentByteLength(this.parts);
  }
  get blockSizes() {
    return this.parts.map(contentByteLength);
  }

  encode() {
    return encodeAdvancedFile(this.parts, this.metadata);
  }
}

/**
 * @implements {UnixFS.ComplexFile}
 */
class ComplexFileView {
  /**
   * @param {Uint8Array} content
   * @param {ReadonlyArray<UnixFS.FileLink>} parts
   * @param {UnixFS.Metadata} metadata
   */
  constructor(content, parts, metadata) {
    this.content = content;
    this.parts = parts;
    this.metadata = metadata;
  }
  /** @type {"complex"} */
  get layout() {
    return "complex";
  }

  /**
   * @returns {NodeType.File}
   */
  get type() {
    return NodeType.File;
  }
  get fileSize() {
    return this.content.byteLength + cumulativeContentByteLength(this.parts);
  }
  get blockSizes() {
    return this.parts.map(contentByteLength);
  }

  encode() {
    return encodeComplexFile(this.content, this.parts, this.metadata);
  }
}

/**
 * @param {UnixFS.File|UnixFS.Raw|UnixFS.FileChunk|UnixFS.FileShard|UnixFS.Symlink} node
 * @returns {number}
 */
const filesize = (node) => {
  switch (node.type) {
    case NodeType.Raw:
    case NodeType.Symlink:
      return node.content.byteLength;
    case NodeType.File:
      switch (node.layout) {
        case "simple":
          return node.content.byteLength;
        case "advanced":
          return cumulativeContentByteLength(node.parts);
        case "complex":
          return (
            node.content.byteLength + cumulativeContentByteLength(node.parts)
          );
      }
    default:
      return 0;
  }
};

var UnixFS = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  DEFAULT_DIRECTORY_MODE: DEFAULT_DIRECTORY_MODE,
  DEFAULT_FILE_MODE: DEFAULT_FILE_MODE,
  NodeType: NodeType,
  code: code$1,
  createAdvancedFile: createAdvancedFile,
  createComplexFile: createComplexFile,
  createDirectoryShard: createDirectoryShard,
  createEmptyFile: createEmptyFile,
  createFileChunk: createFileChunk,
  createFileShard: createFileShard,
  createFlatDirectory: createFlatDirectory,
  createRaw: createRaw,
  createShardedDirectory: createShardedDirectory,
  createSimpleFile: createSimpleFile,
  createSymlink: createSymlink,
  cumulativeContentByteLength: cumulativeContentByteLength,
  cumulativeDagByteLength: cumulativeDagByteLength,
  decode: decode$6,
  decodeMetadata: decodeMetadata,
  encode: encode$5,
  encodeAdvancedFile: encodeAdvancedFile,
  encodeComplexFile: encodeComplexFile,
  encodeDirectory: encodeDirectory,
  encodeDirectoryMetadata: encodeDirectoryMetadata,
  encodeFile: encodeFile,
  encodeFileChunk: encodeFileChunk,
  encodeFileShard: encodeFileShard,
  encodeHAMTShard: encodeHAMTShard,
  encodeLink: encodeLink,
  encodeMetadata: encodeMetadata,
  encodeMode: encodeMode,
  encodeRaw: encodeRaw,
  encodeSimpleFile: encodeSimpleFile,
  encodeSymlink: encodeSymlink,
  filesize: filesize,
  matchFile: matchFile,
  name: name$1,
});

/**
 * Turns a task (that never fails or sends messages) into an effect of it's
 * result.
 *
 * @template T
 * @param {Task.Task<T, never>} task
 * @returns {Task.Effect<T>}
 */
const effect = function* (task) {
  const message = yield* task;
  yield* send(message);
};

/**
 * Gets a handle to the task that invoked it. Useful when task needs to
 * suspend execution until some outside event occurs, in which case handle
 * can be used resume execution (see `suspend` code example for more details)
 *
 * @template T, M, X
 * @returns {Task.Task<Task.Controller<T, X, M>, never>}
 */
function* current() {
  return /** @type {Task.Controller<T, X, M>} */ (yield CURRENT);
}

/**
 * Suspends the current task (task that invokes it),  which can then be
 * resumed from another task or an outside event (e.g. `setTimeout` callback)
 * by calling the `resume` with an task's handle.
 *
 * Calling this in almost all cases is preceeded by call to `current()` in
 * order to obtain a `handle` which can be passed to `resume` function
 * to resume the execution.
 *
 * Note: This task never fails, although it may never resume either. However
 * you can utilize `finally` block to do a necessary cleanup in case execution
 * is aborted.
 *
 * @example
 * ```js
 * import { current, suspend, resume } from "actor"
 * function * sleep(duration) {
 *    // get a reference to this task so we can resume it.
 *    const self = yield * current()
 *    // resume this task when timeout fires
 *    const id = setTimeout(() => resume(self), duration)
 *    try {
 *      // suspend this task nothing below this line will run until task is
 *      // resumed.
 *      yield * suspend()
 *    } finally {
 *      // if task is aborted finally block will still run which given you
 *      // chance to cleanup.
 *      clearTimeout(id)
 *    }
 * }
 * ```
 *
 * @returns {Task.Task<void, never>}
 */
const suspend = function* () {
  yield SUSPEND;
};

/**
 * Provides equivalent of `await` in async functions. Specifically it takes
 * a value that you can `await` on (that is `Promise<T>|T`) and suspends
 * execution until promise is settled. If promise succeeds execution is resumed
 * with `T` otherwise an error of type `X` is thrown (which is by default
 * `unknown` since promises do not encode error type).
 *
 * It is useful when you need to deal with potentially async set of operations
 * without having to check if thing is a promise at every step.
 *
 * Please note: This that execution is suspended even if given value is not a
 * promise, however scheduler will still resume it in the same tick of the event
 * loop after, just processing other scheduled tasks. This avoids problematic
 * race condititions that can otherwise occur when values are sometimes promises
 * and other times are not.
 *
 * @example
 * ```js
 * function * fetchJSON (url, options) {
 *    const response = yield * wait(fetch(url, options))
 *    const json = yield * wait(response.json())
 *    return json
 * }
 * ```
 *
 * @template T, [X=unknown]
 * @param {Task.Await<T>} input
 * @returns {Task.Task<T, Error>}
 */
const wait = function* (input) {
  const task = yield* current();
  if (isAsync(input)) {
    let failed = false;
    /** @type {unknown} */
    let output = undefined;
    input.then(
      (value) => {
        failed = false;
        output = value;
        enqueue(task);
      },
      (error) => {
        failed = true;
        output = error;
        enqueue(task);
      }
    );

    yield* suspend();
    if (failed) {
      throw output;
    } else {
      return /** @type {T} */ (output);
    }
  } else {
    // This may seem redundunt but it is not, by enqueuing this task we allow
    // scheduler to perform other queued tasks first. This way many race
    // conditions can be avoided when values are sometimes promises and other
    // times aren't.
    // Unlike `await` however this will resume in the same tick.
    main(wake(task));
    yield* suspend();
    return input;
  }
};

/**
 * @template T, X, M
 * @param {Task.Controller<T, X, M>} task
 * @returns {Task.Task<void, never, never>}
 */
function* wake(task) {
  enqueue(task);
}

/**
 * Checks if value value is a promise (or it's lookalike).
 *
 * @template T
 * @param {any} node
 * @returns {node is PromiseLike<T>}
 */

const isAsync = (node) =>
  node != null &&
  typeof (/** @type {{then?:unknown}} */ (node).then) === "function";

/**
 * Task that sends given message (or rather an effect producing this message).
 * Please note, that while you could use `yield message` instead, but you'd risk
 * having to deal with potential breaking changes if library internals change
 * in the future, which in fact may happen as anticipated improvements in
 * TS generator inference could enable replace need for `yield *`.
 *
 * @see https://github.com/microsoft/TypeScript/issues/43632
 *
 * @template T
 * @param {T} message
 * @returns {Task.Effect<T>}
 */
const send = function* (message) {
  yield /** @type {Task.Message<T>} */ (message);
};

/**
 * Takes several effects and merges them into a single effect of tagged
 * variants so that their source could be identified via `type` field.
 *
 * @example
 * ```js
 * listen({
 *    read: Task.effect(dbRead),
 *    write: Task.effect(dbWrite)
 * })
 * ```
 *
 * @template {string} Tag
 * @template T
 * @param {{ [K in Tag]: Task.Effect<T> }} source
 * @returns {Task.Effect<Tagged<Tag, T>>}
 */
const listen = function* (source) {
  /** @type {Task.Fork<void, never, Tagged<Tag, T>>[]} */
  const forks = [];
  for (const entry of Object.entries(source)) {
    const [name, effect] = /** @type {[Tag, Task.Effect<T>]} */ (entry);
    if (effect !== NONE) {
      forks.push(yield* fork$3(tag(effect, name)));
    }
  }

  yield* group(forks);
};

/**
 * Takes several tasks and creates an effect of them all.
 *
 * @example
 * ```js
 * Task.effects([
 *    dbRead,
 *    dbWrite
 * ])
 * ```
 *
 * @template {string} Tag
 * @template T
 * @param {Task.Task<T, never>[]} tasks
 * @returns {Task.Effect<T>}
 */

const effects = (tasks) => (tasks.length > 0 ? batch(tasks.map(effect)) : NONE);

/**
 * Takes several effects and combines them into a one.
 *
 * @template T
 * @param {Task.Effect<T>[]} effects
 * @returns {Task.Effect<T>}
 */
function* batch(effects) {
  const forks = [];
  for (const effect of effects) {
    forks.push(yield* fork$3(effect));
  }

  yield* group(forks);
}

/**
 * @template {string} Tag
 * @template T
 * @typedef {{type: Tag} & {[K in Tag]: T}} Tagged
 */
/**
 * Tags an effect by boxing each event with an object that has `type` field
 * corresponding to given tag and same named field holding original message
 * e.g. given `nums` effect that produces numbers, `tag(nums, "inc")` would
 * create an effect that produces events like `{type:'inc', inc:1}`.
 *
 * @template {string} Tag
 * @template T, M, X
 * @param {Task.Task<T, X, M>} effect
 * @param {Tag} tag
 * @returns {Task.Task<T, X, Tagged<Tag, M>>}
 */
const tag = (effect, tag) =>
  // @ts-ignore
  effect === NONE
    ? NONE
    : effect instanceof Tagger
    ? new Tagger([...effect.tags, tag], effect.source)
    : new Tagger([tag], effect);

/**
 * @template {string} Tag
 * @template Success, Failure, Message
 *
 * @implements {Task.Task<Success, Failure, Tagged<Tag, Message>>}
 * @implements {Task.Controller<Success, Failure, Tagged<Tag, Message>>}
 */
class Tagger {
  /**
   * @param {Task.Task<Success, Failure, Message>} source
   * @param {string[]} tags
   */
  constructor(tags, source) {
    this.tags = tags;
    this.source = source;
    /** @type {Task.Controller<Success, Failure, Message>} */
    this.controller;
  }
  /* c8 ignore next 3 */
  [Symbol.iterator]() {
    if (!this.controller) {
      this.controller = this.source[Symbol.iterator]();
    }
    return this;
  }
  /**
   * @param {Task.TaskState<Success, Message>} state
   * @returns {Task.TaskState<Success, Tagged<Tag, Message>>}
   */
  box(state) {
    if (state.done) {
      return state;
    } else {
      switch (state.value) {
        case SUSPEND:
        case CURRENT:
          return /** @type {Task.TaskState<Success, Tagged<Tag, Message>>} */ (
            state
          );
        default: {
          // Instead of boxing result at each transform step we perform in-place
          // mutation as we know nothing else is accessing this value.
          const tagged = /** @type {{ done: false, value: any }} */ (state);
          let { value } = tagged;
          for (const tag of this.tags) {
            value = withTag(tag, value);
          }
          tagged.value = value;
          return tagged;
        }
      }
    }
  }
  /**
   *
   * @param {Task.Instruction<Message>} instruction
   */
  next(instruction) {
    return this.box(this.controller.next(instruction));
  }
  /**
   *
   * @param {Failure} error
   */
  throw(error) {
    return this.box(this.controller.throw(error));
  }
  /**
   * @param {Success} value
   */
  return(value) {
    return this.box(this.controller.return(value));
  }

  get [Symbol.toStringTag]() {
    return "TaggedEffect";
  }
}

/**
 * Returns empty `Effect`, that is produces no messages. Kind of like `[]` or
 * `""` but for effects.
 *
 * @type {() => Task.Effect<never>}
 */
const none = () => NONE;

/**
 * @template {string} Tag
 * @template T
 * @param {Tag} tag
 * @param {Task.Message<T>} value
 */
const withTag = (tag, value) =>
  /** @type {Tagged<Tag, T>} */
  ({ type: tag, [tag]: value });

// Special control instructions recognized by a scheduler.
const CURRENT = Symbol("current");
const SUSPEND = Symbol("suspend");

/**
 * @template T, X, M
 * @implements {Task.TaskGroup<T, X, M>}
 */
class Group {
  /**
   * @template T, X, M
   * @param {Task.Controller<T, X, M>|Task.Fork<T, X, M>} member
   * @returns {Task.Group<T, X, M>}
   */
  static of(member) {
    return (
      /** @type {{group?:Task.TaskGroup<T, X, M>}} */ (member).group || MAIN
    );
  }

  /**
   * @template T, X, M
   * @param {(Task.Controller<T, X, M>|Task.Fork<T, X, M>) & {group?:Task.TaskGroup<T, X, M>}} member
   * @param {Task.TaskGroup<T, X, M>} group
   */
  static enqueue(member, group) {
    member.group = group;
    group.stack.active.push(member);
  }
  /**
   * @param {Task.Controller<T, X, M>} driver
   * @param {Task.Controller<T, X, M>[]} [active]
   * @param {Set<Task.Controller<T, X, M>>} [idle]
   * @param {Task.Stack<T, X, M>} [stack]
   */
  constructor(
    driver,
    active = [],
    idle = new Set(),
    stack = new Stack(active, idle)
  ) {
    this.driver = driver;
    this.parent = Group.of(driver);
    this.stack = stack;
    this.id = ++ID;
  }
}

/**
 * @template T, X, M
 * @implements {Task.Main<T, X, M>}
 */
class Main {
  constructor() {
    this.status = IDLE;
    this.stack = new Stack();
    this.id = /** @type {0} */ (0);
  }
}

/**
 * @template T, X, M
 */
class Stack {
  /**
   * @param {Task.Controller<T, X, M>[]} [active]
   * @param {Set<Task.Controller<T, X, M>>} [idle]
   */
  constructor(active = [], idle = new Set()) {
    this.active = active;
    this.idle = idle;
  }

  /**
   *
   * @param {Task.Stack<unknown, unknown, unknown>} stack
   * @returns
   */
  static size({ active, idle }) {
    return active.length + idle.size;
  }
}

/**
 * Starts a main task.
 *
 * @param {Task.Task<void, never>} task
 */
const main = (task) => enqueue(task[Symbol.iterator]());

/**
 * @template T, X, M
 * @param {Task.Controller<T, X, M>} task
 */
const enqueue = (task) => {
  let group = Group.of(task);
  group.stack.active.push(task);
  group.stack.idle.delete(task);

  // then walk up the group chain and unblock their driver tasks.
  while (group.parent) {
    const { idle, active } = group.parent.stack;
    if (idle.has(group.driver)) {
      idle.delete(group.driver);
      active.push(group.driver);
    } else {
      // if driver was not blocked it must have been unblocked by
      // other task so stop there.
      break;
    }

    group = group.parent;
  }

  if (MAIN.status === IDLE) {
    MAIN.status = ACTIVE;
    while (true) {
      try {
        for (const _message of step(MAIN)) {
        }
        MAIN.status = IDLE;
        break;
      } catch (_error) {
        // Top level task may crash and throw an error, but given this is a main
        // group we do not want to interupt other unrelated tasks, which is why
        // we discard the error and the task that caused it.
        MAIN.stack.active.shift();
      }
    }
  }
};

/**
 * @template T, X, M
 * @param {Task.Controller<T, X, M>} task
 */
const resume = (task) => enqueue(task);

/**
 * @template T, X, M
 * @param {Task.Group<T, X, M>} group
 */

const step = function* (group) {
  const { active } = group.stack;
  let task = active[0];
  group.stack.idle.delete(task);
  while (task) {
    /** @type {Task.TaskState<T, M>} */
    let state = INIT;
    // Keep processing insturctions until task is done, it send suspend request
    // or it's has been removed from the active queue.
    // ⚠️ Group changes require extra care so please make sure to understand
    // the detail here. It occurs when spawned task(s) are joined into a group
    // which will change the task driver, that is when `task === active[0]` will
    // became false and need to to drop the task immediately otherwise race
    // condition will occur due to task been  driven by multiple concurrent
    // schedulers.
    loop: while (!state.done && task === active[0]) {
      const instruction = state.value;
      switch (instruction) {
        // if task is suspended we add it to the idle list and break the loop
        // to move to a next task.
        case SUSPEND:
          group.stack.idle.add(task);
          break loop;
        // if task requested a context (which is usually to suspend itself)
        // pass back a task reference and continue.
        case CURRENT:
          state = task.next(task);
          break;
        default:
          // otherwise task sent a message which we yield to the driver and
          // continue
          state = task.next(
            yield /** @type {M & Task.Message<M>}*/ (instruction)
          );
          break;
      }
    }

    // If task is complete, or got suspended we move to a next task
    active.shift();
    task = active[0];
    group.stack.idle.delete(task);
  }
};

/**
 * Executes given task concurrently with current task (the task that initiated
 * fork). Froked task is detached from the task that created it and it can
 * outlive it and / or fail without affecting it. You do however get a handle
 * for the fork which could be used to `join` the task, in which case `joining`
 * task will block until fork finishes execution.
 *
 * This is also a primary interface for executing tasks from the outside of the
 * task context. Function returns `Fork` which implements `Promise` interface
 * so it could be awaited. Please note that calling `fork` does not really do
 * anything, it lazily starts execution when you either `await fork(work())`
 * from arbitray context or `yield* fork(work())` in anothe task context.
 *
 * @template T, X, M
 * @param {Task.Task<T, X, M>} task
 * @param {Task.ForkOptions} [options]
 * @returns {Task.Fork<T, X, M>}
 */
const fork$3 = (task, options) => new Fork(task, options);

/**
 * Exits task succesfully with a given return value.
 *
 * @template T, M, X
 * @param  {Task.Controller<T, M, X>} handle
 * @param {T} value
 * @returns {Task.Task<void, never>}
 */
const exit = (handle, value) => conclude(handle, { ok: true, value });

/**
 * Aborts given task with an error. Task error type should match provided error.
 *
 * @template T, M, X
 * @param {Task.Controller<T, X, M>} handle
 * @param {X} [error]
 */
const abort = (handle, error) => conclude(handle, { ok: false, error });

/**
 * Aborts given task with an given error.
 *
 * @template T, M, X
 * @param {Task.Controller<T, X, M>} handle
 * @param {Task.Result<T, X>} result
 * @returns {Task.Task<void, never> & Task.Controller<void, never>}
 */
function* conclude(handle, result) {
  try {
    const task = handle;
    const state = result.ok
      ? task.return(result.value)
      : task.throw(result.error);

    if (!state.done) {
      if (state.value === SUSPEND) {
        const { idle } = Group.of(task).stack;
        idle.add(task);
      } else {
        enqueue(task);
      }
    }
  } catch (error) {}
}

/**
 * Groups multiple forks togather and joins joins them with current task.
 *
 * @template T, X, M
 * @param {Task.Fork<T, X, M>[]} forks
 * @returns {Task.Task<void, X, M>}
 */
function* group(forks) {
  // Abort eraly if there'se no work todo.
  if (forks.length === 0) return;

  const self = yield* current();
  /** @type {Task.TaskGroup<T, X, M>} */
  const group = new Group(self);
  /** @type {Task.Failure<X>|null} */
  let failure = null;

  for (const fork of forks) {
    const { result } = fork;
    if (result) {
      if (!result.ok && !failure) {
        failure = result;
      }
      continue;
    }
    move(fork, group);
  }

  // Keep work looping until there is nom more work to be done
  try {
    if (failure) {
      throw failure.error;
    }

    while (true) {
      yield* step(group);
      if (Stack.size(group.stack) > 0) {
        yield* suspend();
      } else {
        break;
      }
    }
  } catch (error) {
    for (const task of group.stack.active) {
      yield* abort(task, error);
    }

    for (const task of group.stack.idle) {
      yield* abort(task, error);
      enqueue(task);
    }

    throw error;
  }
}

/**
 * @template T, X, M
 * @param {Task.Fork<T, X, M>} fork
 * @param {Task.TaskGroup<T, X, M>} group
 */
const move = (fork, group) => {
  const from = Group.of(fork);
  if (from !== group) {
    const { active, idle } = from.stack;
    const target = group.stack;
    fork.group = group;
    // If it is idle just move from one group to the other
    // and update the group task thinks it belongs to.
    if (idle.has(fork)) {
      idle.delete(fork);
      target.idle.add(fork);
    } else {
      const index = active.indexOf(fork);
      // If task is in the job queue, we move it to a target job queue. Moving
      // top task in the queue requires extra care so it does not end up
      // processed by two groups which would lead to race. For that reason
      // `step` loop checks for group changes on each turn.
      if (index >= 0) {
        active.splice(index, 1);
        target.active.push(fork);
      }
      // otherwise task is complete
    }
  }
};

/**
 * @template T, X, M
 * @param {Task.Fork<T, X, M>} fork
 * @returns {Task.Task<T, X, M>}
 */
function* join(fork) {
  // If fork is still idle activate it.
  if (fork.status === IDLE) {
    yield* fork;
  }

  if (!fork.result) {
    yield* group([fork]);
  }

  const result = /** @type {Task.Result<T, X>} */ (fork.result);
  if (result.ok) {
    return result.value;
  } else {
    throw result.error;
  }
}

/**
 * @template T, X
 * @implements {Task.Future<T, X>}
 */
class Future {
  /**
   * @param {Task.StateHandler<T, X>} handler
   */
  constructor(handler) {
    this.handler = handler;
    /**
     * @abstract
     * @type {Task.Result<T, X>|void}
     */
    this.result;
  }
  /**
   * @type {Promise<T>}
   */
  get promise() {
    const { result } = this;
    const promise =
      result == null
        ? new Promise((succeed, fail) => {
            this.handler.onsuccess = succeed;
            this.handler.onfailure = fail;
          })
        : result.ok
        ? Promise.resolve(result.value)
        : Promise.reject(result.error);
    Object.defineProperty(this, "promise", { value: promise });
    return promise;
  }

  /**
   * @template U, [E=never]
   * @param {((value:T) => U | PromiseLike<U>)|undefined|null} [onresolve]
   * @param {((error:X) => E|PromiseLike<E>)|undefined|null} [onreject]
   * @returns {Promise<U|E>}
   */
  then(onresolve, onreject) {
    return this.activate().promise.then(onresolve, onreject);
  }
  /**
   * @template [U=never]
   * @param {(error:X) => U} onreject
   */
  catch(onreject) {
    return /** @type {Task.Future<T|U, never>} */ (
      this.activate().promise.catch(onreject)
    );
  }
  /**
   * @param {() => void} onfinally
   * @returns {Task.Future<T, X>}
   */
  finally(onfinally) {
    return /** @type {Task.Future<T, X>} */ (
      this.activate().promise.finally(onfinally)
    );
  }
  /**
   * @abstract
   */
  /* c8 ignore next 3 */
  activate() {
    return this;
  }
}

/**
 * @template T, X, M
 * @implements {Task.Fork<T, X, M>}
 * @implements {Task.Controller<T, X, M>}
 * @implements {Task.Task<Task.Fork<T, X, M>, never>}
 * @implements {Task.Future<T, X>}
 * @extends {Future<T, X>}
 */
class Fork extends Future {
  /**
   * @param {Task.Task<T, X, M>} task
   * @param {Task.ForkOptions} [options]
   * @param {Task.StateHandler<T, X>} [handler]
   * @param {Task.TaskState<T, M>} [state]
   */
  constructor(task, options = BLANK$1, handler = {}, state = INIT) {
    super(handler);
    this.id = ++ID;
    this.name = options.name || "";
    /** @type {Task.Task<T, X, M>} */
    this.task = task;
    this.state = state;
    this.status = IDLE;
    /** @type {Task.Result<T, X>} */
    this.result;
    this.handler = handler;

    /** @type {Task.Controller<T, X, M>} */
    this.controller;
  }

  *resume() {
    resume(this);
  }

  /**
   * @returns {Task.Task<T, X, M>}
   */
  join() {
    return join(this);
  }

  /**
   * @param {X} error
   */
  abort(error) {
    return abort(this, error);
  }
  /**
   * @param {T} value
   */
  exit(value) {
    return exit(this, value);
  }
  get [Symbol.toStringTag]() {
    return "Fork";
  }

  /**
   * @returns {Task.Controller<Task.Fork<T, X, M>, never, never>}
   */
  *[Symbol.iterator]() {
    return this.activate();
  }

  activate() {
    this.controller = this.task[Symbol.iterator]();
    this.status = ACTIVE;
    enqueue(this);
    return this;
  }

  /**
   * @private
   * @param {any} error
   * @returns {never}
   */
  panic(error) {
    this.result = { ok: false, error };
    this.status = FINISHED;
    const { handler } = this;
    if (handler.onfailure) {
      handler.onfailure(error);
    }

    throw error;
  }

  /**
   * @private
   * @param {Task.TaskState<T, M>} state
   */
  step(state) {
    this.state = state;
    if (state.done) {
      this.result = { ok: true, value: state.value };
      this.status = FINISHED;
      const { handler } = this;
      if (handler.onsuccess) {
        handler.onsuccess(state.value);
      }
    }

    return state;
  }

  /**
   * @param {unknown} value
   */
  next(value) {
    try {
      return this.step(this.controller.next(value));
    } catch (error) {
      return this.panic(error);
    }
  }
  /**
   * @param {T} value
   */
  return(value) {
    try {
      return this.step(this.controller.return(value));
    } catch (error) {
      return this.panic(error);
    }
  }
  /**
   * @param {X} error
   */
  throw(error) {
    try {
      return this.step(this.controller.throw(error));
    } catch (error) {
      return this.panic(error);
    }
  }
}

/**
 * @template M
 * @param {Task.Effect<M>} init
 * @param {(message:M) => Task.Effect<M>} next
 * @returns {Task.Task<void, never, never>}
 */
const loop = function* (init, next) {
  /** @type {Task.Controller<void, never, M>} */
  const controller = yield* current();
  const group = new Group(controller);
  Group.enqueue(init[Symbol.iterator](), group);

  while (true) {
    for (const message of step(group)) {
      Group.enqueue(next(message)[Symbol.iterator](), group);
    }

    if (Stack.size(group.stack) > 0) {
      yield* suspend();
    } else {
      break;
    }
  }
};

let ID = 0;
/** @type {Task.Status} */
const IDLE = "idle";
const ACTIVE = "active";
const FINISHED = "finished";
/** @type {Task.TaskState<any, any>} */
const INIT = { done: false, value: CURRENT };

const BLANK$1 = {};

/** @type {Task.Effect<never>} */
const NONE = (function* none() {})();

/** @type {Task.Main<any, any, any>} */
const MAIN = new Main();

function Indexed() {}

Object.defineProperties(Indexed, {
  prototype: {
    value: new Proxy(Object.prototype, {
      /**
       * @param {object} target
       * @param {PropertyKey} property
       * @param {{get(key:PropertyKey): any}} receiver
       */
      get(target, property, receiver) {
        return typeof property === "symbol"
          ? Reflect.get(target, property, receiver)
          : receiver.get(property);
      },
    }),
  },
});

/**
 * @typedef {{
 * readonly byteOffset: number
 * readonly byteLength: number
 * readonly segments: Uint8Array[]
 * }} BufferSlice
 */

/** @typedef {BufferView} View */
const empty$2 = () => new BufferView();

/**
 * @param {BufferSlice} buffer
 * @param {number} [startOffset]
 * @param {number} [endOffset]
 */
const slice = (buffer, startOffset = 0, endOffset = buffer.byteLength) => {
  const segments = [];
  const start = startOffset < 0 ? buffer.byteLength - startOffset : startOffset;
  const end = endOffset < 0 ? buffer.byteLength - endOffset : endOffset;

  // If start at 0 offset and end is past buffer range it is effectively
  // as same buffer.
  if (start === 0 && end >= buffer.byteLength) {
    return buffer;
  }

  // If range is not within the current buffer just create an empty slice.
  if (start > end || start > buffer.byteLength || end <= 0) {
    return empty$2();
  }

  let byteLength = 0;
  let offset = 0;
  for (const segment of buffer.segments) {
    const nextOffset = offset + segment.byteLength;
    // Have not found a start yet
    if (byteLength === 0) {
      // If end offset is within the current segment we know start is also,
      // because it preceeds the end & we had not found start yet.
      // In such case we create a view with only single segment of bytes
      // in the range.
      if (end <= nextOffset) {
        const range = segment.subarray(start - offset, end - offset);
        segments.push(range);
        byteLength = range.byteLength;
        break;
      }
      // If start offeset falls with in current range (but not the end)
      // we save matching buffer slice and update byteLength.
      else if (start < nextOffset) {
        const range =
          start === offset ? segment : segment.subarray(start - offset);
        segments.push(range);
        byteLength = range.byteLength;
      }
    }
    // Otherwise we already started collecting matching segments and are looking
    // for the end end of the slice. If it is with in the current range capture
    // the segment and create a view.
    else if (end <= nextOffset) {
      const range =
        end === nextOffset ? segment : segment.subarray(0, end - offset);
      segments.push(range);
      byteLength += range.byteLength;
      break;
    }
    // If end is past current range we just save the segment and continue.
    else {
      segments.push(segment);
      byteLength += segment.byteLength;
    }

    offset = nextOffset;
  }

  return new BufferView(segments, buffer.byteOffset + start, byteLength);
};

/**
 * @param {BufferSlice} buffer
 * @param {Uint8Array} part
 */

const push = (buffer, part) => {
  if (part.byteLength > 0) {
    // We MUTATE here but that is ok because it is out of bound for the passed
    // buffer view so there will be no visible side effects.
    buffer.segments.push(part);
    return new BufferView(
      buffer.segments,
      buffer.byteOffset,
      buffer.byteLength + part.byteLength
    );
  } else {
    return buffer;
  }
};

/**
 * @param {BufferSlice} buffer
 * @param {number} n
 */
const get$6 = (buffer, n) => {
  if (n < buffer.byteLength) {
    let offset = 0;
    for (const segment of buffer.segments) {
      if (n < offset + segment.byteLength) {
        return segment[n - offset];
      } else {
        offset += segment.byteLength;
      }
    }
  }

  return undefined;
};

/**
 *
 * @param {BufferView} buffer
 * @param {Uint8Array} target
 * @param {number} byteOffset
 */
const copyTo = (buffer, target, byteOffset) => {
  let offset = byteOffset;
  for (const segment of buffer.segments) {
    target.set(segment, offset);
    offset += segment.byteLength;
  }

  return target;
};

/**
 *
 * @param {BufferView} buffer
 */
function* iterate$1(buffer) {
  for (const part of buffer.segments) {
    yield* part;
  }
}

/**
 * @extends {Indexed<number>}
 */
class BufferView extends Indexed {
  /**
   * @param {Uint8Array[]} segments
   * @param {number} byteOffset
   * @param {number} byteLength
   */
  constructor(segments = [], byteOffset = 0, byteLength = 0) {
    super();
    /** @hide */
    this.segments = segments;
    /** @readonly */
    this.byteLength = byteLength;
    /** @readonly */
    this.length = byteLength;
    /** @readonly */
    this.byteOffset = byteOffset;
  }

  [Symbol.iterator]() {
    return iterate$1(this);
  }

  /**
   * @param {number} [start]
   * @param {number} [end]
   */
  slice(start, end) {
    return /** @type {BufferView} */ (slice(this, start, end));
  }

  /**
   * @param {number} [start]
   * @param {number} [end]
   */
  subarray(start, end) {
    return /** @type {BufferView} */ (slice(this, start, end));
  }

  /**
   *
   * @param {Uint8Array} bytes
   */
  push(bytes) {
    return /** @type {BufferView} */ (push(this, bytes));
  }

  /**
   * @param {number} n
   */
  get(n) {
    return get$6(this, n);
  }

  /**
   *
   * @param {Uint8Array} target
   * @param {number} offset
   */
  copyTo(target, offset) {
    return copyTo(this, target, offset);
  }
}

/**
 * @param {string} reason
 * @returns {never}
 */
const panic = (reason) => {
  throw new Error(reason);
};

/**
 * @param {{ raw: readonly string[] | ArrayLike<string>}} template
 * @param {never} [subject]
 * @param {unknown[]} substitutions
 * @returns {never}
 */
const unreachable = (template, subject, ...substitutions) =>
  panic(String.raw(template, JSON.stringify(subject), ...substitutions));

const EMPTY_BUFFER = new Uint8Array(0);
/** @type {any[]} */
const EMPTY$2 = [];

/**
 * @typedef {{
 * chunker: Chunker.Chunker
 * }} Config
 *
 *
 * @typedef {{
 * buffer: BufferQueue.View
 * config: Config
 * }} Chunker
 *
 * @typedef {Chunker & {chunks: Chunker.Chunk[]}} ChunkerWithChunks
 */

/**
 * @param {Config} config
 * @returns {Chunker}
 */
const open$1 = (config) => ({
  config,
  buffer: empty$2(),
});

/**
 * @param {Chunker} state
 * @param {Uint8Array} bytes
 * @returns {ChunkerWithChunks}
 */
const write$3 = (state, bytes) =>
  bytes.byteLength > 0
    ? split(state.config, state.buffer.push(bytes), false)
    : { ...state, chunks: EMPTY$2 };

/**
 * @param {Chunker} state
 * @returns {ChunkerWithChunks}
 */
const close$6 = (state) => split(state.config, state.buffer, true);

/**
 * @param {Config} config
 * @param {BufferQueue.View} buffer
 * @param {boolean} end
 * @returns {ChunkerWithChunks}
 */

const split = (config, buffer, end) => {
  const chunker = config.chunker;
  const chunks = [];

  let offset = 0;
  for (const size of chunker.cut(chunker.context, buffer, end)) {
    // We may be splitting empty buffer in which case there will be no chunks
    // in it so we make sure that we do not emit empty buffer.
    if (size > 0) {
      const chunk = buffer.subarray(offset, offset + size);
      chunks.push(chunk);
      offset += size;
    }
  }

  return { config, chunks, buffer: buffer.subarray(offset) };
};

const mutable = () => ({
  mutable: true,
  needs: {},
  nodes: {},
  links: {},
  linked: EMPTY$1,
});

/**
 *
 * @param {Layout.Branch[]} newNodes
 * @param {Queue.Queue} input
 * @returns {Queue.Result}
 */
const addNodes = (newNodes, input) => {
  let queue = patch(input, {});
  for (const node of newNodes) {
    const { ready, has, wants } = collect(node.children, queue.links);
    // If node isn't waiting on any of the children it's ready to be linked
    // so we add linked node diretly.
    if (wants.length === 0) {
      queue = patch(queue, {
        links: assign(undefined, has),
        linked: [{ id: node.id, links: ready }],
      });
    } else {
      queue = patch(queue, {
        needs: assign(node.id, wants),
        nodes: {
          [node.id]: {
            children: node.children,
            count: wants.length,
          },
        },
      });
    }
  }

  return queue;
};

/**
 * Adds link to the queue. If queue contains a node that needs this link it gets
 * updated. Either it's gets linked (when it was blocked only on this link) or
 * it's want could is reduced. If no node needed this link it just gets stored
 * for the future node that will need it.
 *
 *
 * @param {Queue.NodeID} id
 * @param {Queue.Link} link
 * @param {Queue.Queue} queue
 * @returns {Queue.Result}
 */

const addLink = (id, link, queue) => {
  const nodeID = queue.needs[id];
  const node = queue.nodes[nodeID];
  // We have node than needs this link.
  if (node != null) {
    // This is the only link it needed so we materialize the node and remove
    // links and needs associated with it.
    if (node.count === 1) {
      const { ready, has } = collect(node.children, {
        ...queue.links,
        [id]: link,
      });

      return patch(queue, {
        needs: { [id]: undefined },
        links: assign(undefined, has),
        nodes: { [nodeID]: undefined },
        linked: [{ id: nodeID, links: ready }],
      });
    }
    // If node needs more links we just reduce a want count and remove this
    // need.
    else {
      return patch(queue, {
        needs: { [id]: undefined },
        links: { [id]: link },
        nodes: {
          [nodeID]: {
            ...node,
            count: node.count - 1,
          },
        },
      });
    }
  }
  // If we have no one waiting for this link just add it to the queue
  else {
    return patch(queue, {
      links: { [id]: link },
    });
  }
};

/**
 *
 * @param {Queue.Queue} queue
 * @param {Queue.Delta} delta
 */

const patch = (queue, { needs, nodes, links, linked }) => {
  const result = queue.mutable ? queue : { ...queue };
  const original = queue.mutable ? BLANK : undefined;

  if (needs) {
    result.needs = patchDict(queue.needs, needs, original);
  }

  if (nodes) {
    result.nodes = patchDict(queue.nodes, nodes, original);
  }

  if (links) {
    result.links = patchDict(queue.links, links, original);
  }

  result.linked = linked
    ? append(queue.linked || EMPTY$1, linked, EMPTY$1)
    : queue.linked || [];

  return /** @type {Queue.Result} */ (result);
};

/**
 * @template V
 * @template {PropertyKey} K
 * @param {V} value
 * @param {K[]} keys
 * @returns {Record<K, V>}
 */

const assign = (value, keys) => {
  const delta = /** @type {Record<K, V>} */ ({});
  for (const key of keys) {
    delta[key] = value;
  }

  return delta;
};

/**
 * @template {PropertyKey} K
 * @template V
 * @param {Record<K, V>} target
 *
 * @param {Record<K, V|void>} delta
 * @param {Record<K, V>} original
 * @returns {Record<K, V>}
 */

const patchDict = (target, delta, original = target) => {
  const result = target === original ? { ...target } : target;
  for (const entry of Object.entries(delta)) {
    const [id, value] = /** @type {[K, V|void]} */ (entry);
    if (value == null) {
      delete result[id];
    } else {
      result[id] = value;
    }
  }

  return result;
};

/**
 * @template T
 * @param {T[]} target
 * @param {T[]} items
 * @param {T[]} original
 */
const append = (target, items, original = target) => {
  if (target === original) {
    return [...target, ...items];
  } else {
    for (const item of items) {
      target.push(item);
    }
    return target;
  }
};

/**
 * @param {Queue.NodeID[]} children
 * @param {Record<Queue.NodeID, Queue.Link>} source
 * @returns {{has:Queue.NodeID[], wants:Queue.NodeID[], ready:Queue.Link[]}}
 */
const collect = (children, source) => {
  const has = [];
  const wants = [];
  const ready = [];
  for (const child of children) {
    const link = source[child];
    if (link) {
      has.push(child);
      ready.push(link);
    } else {
      wants.push(child);
    }
  }

  return { has, wants, ready };
};

const EMPTY$1 = /** @type {never[]} */ (Object.freeze([]));

const BLANK = /** @type {Record<never, never>} */ (Object.freeze({}));

/**
 * @template Layout
 * @typedef {{
 * readonly status: 'open'
 * readonly metadata: UnixFS.Metadata
 * readonly config: API.EncoderSettings<Layout>
 * readonly writer: API.BlockWriter
 * chunker: Chunker.Chunker
 * layout: Layout
 * nodeQueue: Queue.Queue
 * }} Open
 */
/**
 * @template Layout
 * @typedef {{
 * readonly status: 'closed'
 * readonly metadata: UnixFS.Metadata
 * readonly config: API.EncoderSettings<Layout>
 * readonly writer: API.BlockWriter
 * readonly rootID: Layout.NodeID
 * readonly end?: Task.Fork<void, never>
 * chunker?: null
 * layout?: null
 * nodeQueue: Queue.Queue
 * }} Closed
 */
/**
 * @template Layout
 * @typedef {{
 * readonly status: 'linked'
 * readonly metadata: UnixFS.Metadata
 * readonly config: API.EncoderSettings<Layout>
 * readonly writer: API.BlockWriter
 * readonly link: Layout.Link
 * chunker?: null
 * layout?: null
 * nodeQueue: Queue.Queue
 * }} Linked
 */

/**
 * @template Layout
 * @typedef {Open<Layout>|Closed<Layout>|Linked<Layout>} State
 */

/**
 * @template {object} Layout
 * @typedef {{
 * state: State<Layout>
 * effect: Task.Effect<Message>
 * }} Update
 */
/**
 * @typedef {never
 * |{type:"write", bytes:Uint8Array}
 * |{type:"link", link:API.EncodedFile}
 * |{type:"block"}
 * |{type: "close"}
 * |{type: "end"}
 * } Message
 */

/**
 * @template Layout
 * @param {Message} message
 * @param {State<Layout>} state
 */
const update = (message, state) => {
  switch (message.type) {
    case "write":
      return write$2(state, message.bytes);
    case "link":
      return link(state, message.link);
    /* c8 ignore next 2 */
    case "block":
      return { state, effect: none() };
    case "close":
      return close$5(state);
    case "end":
      return { state, effect: none() };
    default:
      return unreachable`File Writer got unknown message ${message}`;
  }
};

/**
 * @template Layout
 * @param {API.BlockWriter} writer
 * @param {UnixFS.Metadata} metadata
 * @param {API.EncoderSettings} config
 * @returns {State<Layout>}
 */
const init = (writer, metadata, config) => {
  return {
    status: "open",
    metadata,
    config,
    writer,
    chunker: open$1({ chunker: config.chunker }),
    layout: config.fileLayout.open(),
    // Note: Writing in large slices e.g. 1GiB at a time creates large queues
    // with around `16353` items. Immutable version ends up copying it every
    // time state of the queue changes, which introduces significant overhead.
    // To avoid this overhead we use mutable implementation which is API
    // compatible but makes in place updates.
    // TODO: We should consider using Persistent bit-partitioned vector tries
    // instead of arrays which would provide immutable interface with neglegable
    // overhead.
    // @see https://github.com/Gozala/vectrie
    nodeQueue: mutable(),
  };
};
/**
 * @template Layout
 * @param {State<Layout>} state
 * @param {Uint8Array} bytes
 * @returns {Update<Layout>}
 */
const write$2 = (state, bytes) => {
  if (state.status === "open") {
    // Chunk up provided bytes
    const { chunks, ...chunker } = write$3(state.chunker, bytes);

    // Pass chunks to layout engine to produce nodes
    const { nodes, leaves, layout } = state.config.fileLayout.write(
      state.layout,
      chunks
    );

    const { linked, ...nodeQueue } = addNodes(nodes, state.nodeQueue);

    // Create leaf encode tasks for all new leaves
    const tasks = [
      ...encodeLeaves(leaves, state.config),
      ...encodeBranches(linked, state.config),
    ];

    return {
      state: {
        ...state,
        chunker,
        layout,
        nodeQueue,
      },
      effect: listen({
        link: effects(tasks),
      }),
    };
  } else {
    return panic("Unable to perform write on closed file");
  }
};

/**
 * @template Layout
 * @param {State<Layout>} state
 * @param {API.EncodedFile} entry
 * @returns {Update<Layout>}
 */
const link = (state, { id, link, block }) => {
  let { linked, ...nodeQueue } = addLink(id, link, state.nodeQueue);

  const tasks = encodeBranches(linked, state.config);

  /** @type {State<Layout>} */
  const newState =
    state.status === "closed" && id === state.rootID
      ? {
          ...state,
          status: "linked",
          link,
          nodeQueue,
        }
      : { ...state, nodeQueue };

  // If we just linked a root and there is a **suspended** "end" task we create
  // a task to resume it.
  const end =
    state.status === "closed" && id === state.rootID && state.end
      ? state.end.resume()
      : none();

  return {
    state: newState,
    effect: listen({
      link: effects(tasks),
      block: writeBlock(state.writer, block),
      end,
    }),
  };
};

/**
 * @template Layout
 * @param {State<Layout>} state
 * @returns {Update<Layout>}
 */
const close$5 = (state) => {
  if (state.status === "open") {
    const { chunks } = close$6(state.chunker);
    const { layout, ...write } = state.config.fileLayout.write(
      state.layout,
      chunks
    );

    const { root, ...close } = state.config.fileLayout.close(
      layout,
      state.metadata
    );

    const [nodes, leaves] = isLeafNode(root)
      ? [
          [...write.nodes, ...close.nodes],
          [...write.leaves, ...close.leaves, root],
        ]
      : [
          [...write.nodes, ...close.nodes, root],
          [...write.leaves, ...close.leaves],
        ];

    const { linked, ...nodeQueue } = addNodes(nodes, state.nodeQueue);

    const tasks = [
      ...encodeLeaves(leaves, state.config),
      ...encodeBranches(linked, state.config),
    ];

    // We want to keep run loop around until root node is linked. To
    // accomplish this we fork a task that suspends itself, which we will
    // resume when root is linked (see link function).
    // Below we join this forked task in our effect, this way effect is not
    // complete until task forked task is, which will do once we link the
    // root.
    const fork = fork$3(suspend());

    return {
      state: {
        ...state,
        chunker: null,
        layout: null,
        rootID: root.id,
        status: "closed",
        end: fork,
        nodeQueue,
      },
      effect: listen({
        link: effects(tasks),
        end: join(fork),
      }),
    };
  } else {
    return { state, effect: none() };
  }
};

/**
 * Creates concurrent leaf encode tasks. Each one will have an ID corresponding
 * to index in the queue.
 *
 * @param {Layout.Leaf[]} leaves
 * @param {API.EncoderSettings} config
 */
const encodeLeaves = (leaves, config) =>
  leaves.map((leaf) => encodeLeaf(config, leaf, config.fileChunkEncoder));

/**
 * @param {API.EncoderSettings} config
 * @param {Layout.Leaf} leaf
 * @param {API.FileChunkEncoder} encoder
 * @returns {Task.Task<API.EncodedFile, never>}
 */
const encodeLeaf = function* ({ hasher, linker }, { id, content }, encoder) {
  const bytes = encoder.encode(content ? asUint8Array(content) : EMPTY_BUFFER);
  const hash = yield* wait(hasher.digest(bytes));
  const cid = linker.createLink(encoder.code, hash);

  const block = { cid, bytes };
  const link = /** @type {UnixFS.FileLink} */ ({
    cid,
    contentByteLength: content ? content.byteLength : 0,
    dagByteLength: bytes.byteLength,
  });

  return { id, block, link };
};

/**
 * @param {Queue.LinkedNode[]} nodes
 * @param {API.EncoderSettings} config
 */
const encodeBranches = (nodes, config) =>
  nodes.map((node) => encodeBranch(config, node));

/**
 * @template Layout
 * @param {API.EncoderSettings<Layout>} config
 * @param {Queue.LinkedNode} node
 * @param {UnixFS.Metadata} [metadata]
 * @returns {Task.Task<API.EncodedFile>}
 */
const encodeBranch = function* (config, { id, links }, metadata) {
  const bytes = config.fileEncoder.encode({
    type: NodeType.File,
    layout: "advanced",
    parts: links,
    metadata,
  });
  const hash = yield* wait(Promise.resolve(config.hasher.digest(bytes)));
  const cid = config.linker.createLink(config.fileEncoder.code, hash);
  const block = { bytes, cid };
  const link = /** @type {UnixFS.FileLink} */ ({
    cid,
    contentByteLength: cumulativeContentByteLength(links),
    dagByteLength: cumulativeDagByteLength(bytes, links),
  });

  return { id, block, link };
};

/**
 * @param {API.BlockWriter} writer
 * @param {UnixFS.Block} block
 * @returns {Task.Task<void, never>}
 */

const writeBlock = function* (writer, block) {
  if ((writer.desiredSize || 0) <= 0) {
    yield* wait(writer.ready);
  }
  writer.write(block);
};

/**
 *
 * @param {Uint8Array|Chunker.Chunk} buffer
 * @returns
 */

const asUint8Array = (buffer) =>
  buffer instanceof Uint8Array
    ? buffer
    : buffer.copyTo(new Uint8Array(buffer.byteLength), 0);

/**
 * @param {Layout.Node} node
 * @returns {node is Layout.Leaf}
 */
const isLeafNode = (node) => node.children == null;

const name = "fixed";
/**
 * @typedef {Object} FixedSize
 * @property {number} maxChunkSize
 */

/** @type {FixedSize} */
const context = {
  maxChunkSize: 262144,
};

const type = "Stateless";

/**
 * @param {number} maxChunkSize
 * @returns {API.StatelessChunker<FixedSize>}
 */
const withMaxChunkSize = (maxChunkSize) => ({
  type: "Stateless",
  context: { maxChunkSize },
  name,
  cut,
});

/**
 * @param {FixedSize} maxChunkSize
 * @param {API.Chunk} buffer
 * @param {boolean} end
 * @returns {number[]}
 */
const cut = ({ maxChunkSize }, { byteLength }, end) => {
  // number of fixed size chunks that would fit
  const n = (byteLength / maxChunkSize) | 0;
  const chunks = new Array(n).fill(maxChunkSize);
  const lastChunkSize = end ? byteLength - n * maxChunkSize : 0;
  if (lastChunkSize > 0) {
    chunks.push(lastChunkSize);
  }
  return chunks;
};

var FixedSize = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  context: context,
  cut: cut,
  name: name,
  type: type,
  withMaxChunkSize: withMaxChunkSize,
});

/**
 * @param {Uint8Array} aa
 * @param {Uint8Array} bb
 */
const equals$1 = (aa, bb) => {
  if (aa === bb) return true;
  if (aa.byteLength !== bb.byteLength) {
    return false;
  }

  for (let ii = 0; ii < aa.byteLength; ii++) {
    if (aa[ii] !== bb[ii]) {
      return false;
    }
  }

  return true;
};

/**
 * @param {ArrayBufferView|ArrayBuffer|Uint8Array} o
 * @returns {Uint8Array}
 */
const coerce = (o) => {
  if (o instanceof Uint8Array && o.constructor.name === "Uint8Array") return o;
  if (o instanceof ArrayBuffer) return new Uint8Array(o);
  if (ArrayBuffer.isView(o)) {
    return new Uint8Array(o.buffer, o.byteOffset, o.byteLength);
  }
  throw new Error("Unknown type, must be binary type");
};

var encode_1$1 = encode$4;

var MSB$3 = 0x80,
  MSBALL$2 = -128,
  INT$2 = Math.pow(2, 31);

function encode$4(num, out, offset) {
  out = out || [];
  offset = offset || 0;
  var oldOffset = offset;

  while (num >= INT$2) {
    out[offset++] = (num & 0xff) | MSB$3;
    num /= 128;
  }
  while (num & MSBALL$2) {
    out[offset++] = (num & 0xff) | MSB$3;
    num >>>= 7;
  }
  out[offset] = num | 0;

  encode$4.bytes = offset - oldOffset + 1;

  return out;
}

var decode$5 = read$2;

var MSB$1$1 = 0x80,
  REST$1$1 = 0x7f;

function read$2(buf, offset) {
  var res = 0,
    offset = offset || 0,
    shift = 0,
    counter = offset,
    b,
    l = buf.length;

  do {
    if (counter >= l) {
      read$2.bytes = 0;
      throw new RangeError("Could not decode varint");
    }
    b = buf[counter++];
    res +=
      shift < 28
        ? (b & REST$1$1) << shift
        : (b & REST$1$1) * Math.pow(2, shift);
    shift += 7;
  } while (b >= MSB$1$1);

  read$2.bytes = counter - offset;

  return res;
}

var N1$1 = Math.pow(2, 7);
var N2$1 = Math.pow(2, 14);
var N3$1 = Math.pow(2, 21);
var N4$1 = Math.pow(2, 28);
var N5$1 = Math.pow(2, 35);
var N6$1 = Math.pow(2, 42);
var N7$1 = Math.pow(2, 49);
var N8$1 = Math.pow(2, 56);
var N9$1 = Math.pow(2, 63);

var length$1 = function (value) {
  return value < N1$1
    ? 1
    : value < N2$1
    ? 2
    : value < N3$1
    ? 3
    : value < N4$1
    ? 4
    : value < N5$1
    ? 5
    : value < N6$1
    ? 6
    : value < N7$1
    ? 7
    : value < N8$1
    ? 8
    : value < N9$1
    ? 9
    : 10;
};

var varint$1 = {
  encode: encode_1$1,
  decode: decode$5,
  encodingLength: length$1,
};

var _brrp_varint$1 = varint$1;

/**
 * @param {Uint8Array} data
 * @param {number} [offset=0]
 * @returns {[number, number]}
 */
const decode$4 = (data, offset = 0) => {
  const code = _brrp_varint$1.decode(data, offset);
  return [code, _brrp_varint$1.decode.bytes];
};

/**
 * @param {number} int
 * @param {Uint8Array} target
 * @param {number} [offset=0]
 */
const encodeTo$1 = (int, target, offset = 0) => {
  _brrp_varint$1.encode(int, target, offset);
  return target;
};

/**
 * @param {number} int
 * @returns {number}
 */
const encodingLength$1 = (int) => {
  return _brrp_varint$1.encodingLength(int);
};

/**
 * Creates a multihash digest.
 *
 * @template {number} Code
 * @param {Code} code
 * @param {Uint8Array} digest
 */
const create$8 = (code, digest) => {
  const size = digest.byteLength;
  const sizeOffset = encodingLength$1(code);
  const digestOffset = sizeOffset + encodingLength$1(size);

  const bytes = new Uint8Array(digestOffset + size);
  encodeTo$1(code, bytes, 0);
  encodeTo$1(size, bytes, sizeOffset);
  bytes.set(digest, digestOffset);

  return new Digest$1(code, size, digest, bytes);
};

/**
 * Turns bytes representation of multihash digest into an instance.
 *
 * @param {Uint8Array} multihash
 * @returns {MultihashDigest}
 */
const decode$3 = (multihash) => {
  const bytes = coerce(multihash);
  const [code, sizeOffset] = decode$4(bytes);
  const [size, digestOffset] = decode$4(bytes.subarray(sizeOffset));
  const digest = bytes.subarray(sizeOffset + digestOffset);

  if (digest.byteLength !== size) {
    throw new Error("Incorrect length");
  }

  return new Digest$1(code, size, digest, bytes);
};

/**
 * @param {MultihashDigest} a
 * @param {unknown} b
 * @returns {b is MultihashDigest}
 */
const equals = (a, b) => {
  if (a === b) {
    return true;
  } else {
    const data = /** @type {{code?:unknown, size?:unknown, bytes?:unknown}} */ (
      b
    );

    return (
      a.code === data.code &&
      a.size === data.size &&
      data.bytes instanceof Uint8Array &&
      equals$1(a.bytes, data.bytes)
    );
  }
};

/**
 * @typedef {import('./interface.js').MultihashDigest} MultihashDigest
 */

/**
 * Represents a multihash digest which carries information about the
 * hashing algorithm and an actual hash digest.
 *
 * @template {number} Code
 * @template {number} Size
 * @class
 * @implements {MultihashDigest}
 */
let Digest$1 = class Digest {
  /**
   * Creates a multihash digest.
   *
   * @param {Code} code
   * @param {Size} size
   * @param {Uint8Array} digest
   * @param {Uint8Array} bytes
   */
  constructor(code, size, digest, bytes) {
    this.code = code;
    this.size = size;
    this.digest = digest;
    this.bytes = bytes;
  }
};

/**
 * @template {string} Name
 * @template {number} Code
 * @param {object} options
 * @param {Name} options.name
 * @param {Code} options.code
 * @param {(input: Uint8Array) => Await<Uint8Array>} options.encode
 */
const from$6 = ({ name, code, encode }) => new Hasher$1(name, code, encode);

/**
 * Hasher represents a hashing algorithm implementation that produces as
 * `MultihashDigest`.
 *
 * @template {string} Name
 * @template {number} Code
 * @class
 * @implements {MultihashHasher<Code>}
 */
let Hasher$1 = class Hasher {
  /**
   *
   * @param {Name} name
   * @param {Code} code
   * @param {(input: Uint8Array) => Await<Uint8Array>} encode
   */
  constructor(name, code, encode) {
    this.name = name;
    this.code = code;
    this.encode = encode;
  }

  /**
   * @param {Uint8Array} input
   * @returns {Await<Digest.Digest<Code, number>>}
   */
  digest(input) {
    if (input instanceof Uint8Array) {
      const result = this.encode(input);
      return result instanceof Uint8Array
        ? create$8(this.code, result)
        : /* c8 ignore next 1 */
          result.then((digest) => create$8(this.code, digest));
    } else {
      throw Error("Unknown type, must be binary type");
      /* c8 ignore next 1 */
    }
  }
};

/**
 * @template {number} Alg
 * @typedef {import('./interface.js').MultihashHasher} MultihashHasher
 */

/**
 * @template T
 * @typedef {Promise<T>|T} Await
 */

/* global crypto */

/**
 * @param {AlgorithmIdentifier} name
 */
const sha$1 =
  (name) =>
  /**
   * @param {Uint8Array} data
   */
  async (data) =>
    new Uint8Array(await crypto.subtle.digest(name, data));

const sha256$1 = from$6({
  name: "sha2-256",
  code: 0x12,
  encode: sha$1("SHA-256"),
});

// base-x encoding / decoding
// Copyright (c) 2018 base-x contributors
// Copyright (c) 2014-2018 The Bitcoin Core developers (base58.cpp)
// Distributed under the MIT software license, see the accompanying
// file LICENSE or http://www.opensource.org/licenses/mit-license.php.
function base(ALPHABET, name) {
  if (ALPHABET.length >= 255) {
    throw new TypeError("Alphabet too long");
  }
  var BASE_MAP = new Uint8Array(256);
  for (var j = 0; j < BASE_MAP.length; j++) {
    BASE_MAP[j] = 255;
  }
  for (var i = 0; i < ALPHABET.length; i++) {
    var x = ALPHABET.charAt(i);
    var xc = x.charCodeAt(0);
    if (BASE_MAP[xc] !== 255) {
      throw new TypeError(x + " is ambiguous");
    }
    BASE_MAP[xc] = i;
  }
  var BASE = ALPHABET.length;
  var LEADER = ALPHABET.charAt(0);
  var FACTOR = Math.log(BASE) / Math.log(256); // log(BASE) / log(256), rounded up
  var iFACTOR = Math.log(256) / Math.log(BASE); // log(256) / log(BASE), rounded up
  function encode(source) {
    if (source instanceof Uint8Array);
    else if (ArrayBuffer.isView(source)) {
      source = new Uint8Array(
        source.buffer,
        source.byteOffset,
        source.byteLength
      );
    } else if (Array.isArray(source)) {
      source = Uint8Array.from(source);
    }
    if (!(source instanceof Uint8Array)) {
      throw new TypeError("Expected Uint8Array");
    }
    if (source.length === 0) {
      return "";
    }
    // Skip & count leading zeroes.
    var zeroes = 0;
    var length = 0;
    var pbegin = 0;
    var pend = source.length;
    while (pbegin !== pend && source[pbegin] === 0) {
      pbegin++;
      zeroes++;
    }
    // Allocate enough space in big-endian base58 representation.
    var size = ((pend - pbegin) * iFACTOR + 1) >>> 0;
    var b58 = new Uint8Array(size);
    // Process the bytes.
    while (pbegin !== pend) {
      var carry = source[pbegin];
      // Apply "b58 = b58 * 256 + ch".
      var i = 0;
      for (
        var it1 = size - 1;
        (carry !== 0 || i < length) && it1 !== -1;
        it1--, i++
      ) {
        carry += (256 * b58[it1]) >>> 0;
        b58[it1] = carry % BASE >>> 0;
        carry = (carry / BASE) >>> 0;
      }
      if (carry !== 0) {
        throw new Error("Non-zero carry");
      }
      length = i;
      pbegin++;
    }
    // Skip leading zeroes in base58 result.
    var it2 = size - length;
    while (it2 !== size && b58[it2] === 0) {
      it2++;
    }
    // Translate the result into a string.
    var str = LEADER.repeat(zeroes);
    for (; it2 < size; ++it2) {
      str += ALPHABET.charAt(b58[it2]);
    }
    return str;
  }
  function decodeUnsafe(source) {
    if (typeof source !== "string") {
      throw new TypeError("Expected String");
    }
    if (source.length === 0) {
      return new Uint8Array();
    }
    var psz = 0;
    // Skip leading spaces.
    if (source[psz] === " ") {
      return;
    }
    // Skip and count leading '1's.
    var zeroes = 0;
    var length = 0;
    while (source[psz] === LEADER) {
      zeroes++;
      psz++;
    }
    // Allocate enough space in big-endian base256 representation.
    var size = ((source.length - psz) * FACTOR + 1) >>> 0; // log(58) / log(256), rounded up.
    var b256 = new Uint8Array(size);
    // Process the characters.
    while (source[psz]) {
      // Decode character
      var carry = BASE_MAP[source.charCodeAt(psz)];
      // Invalid character
      if (carry === 255) {
        return;
      }
      var i = 0;
      for (
        var it3 = size - 1;
        (carry !== 0 || i < length) && it3 !== -1;
        it3--, i++
      ) {
        carry += (BASE * b256[it3]) >>> 0;
        b256[it3] = carry % 256 >>> 0;
        carry = (carry / 256) >>> 0;
      }
      if (carry !== 0) {
        throw new Error("Non-zero carry");
      }
      length = i;
      psz++;
    }
    // Skip trailing spaces.
    if (source[psz] === " ") {
      return;
    }
    // Skip leading zeroes in b256.
    var it4 = size - length;
    while (it4 !== size && b256[it4] === 0) {
      it4++;
    }
    var vch = new Uint8Array(zeroes + (size - it4));
    var j = zeroes;
    while (it4 !== size) {
      vch[j++] = b256[it4++];
    }
    return vch;
  }
  function decode(string) {
    var buffer = decodeUnsafe(string);
    if (buffer) {
      return buffer;
    }
    throw new Error(`Non-${name} character`);
  }
  return {
    encode: encode,
    decodeUnsafe: decodeUnsafe,
    decode: decode,
  };
}
var src = base;

var _brrp__multiformats_scope_baseX = src;

/**
 * Class represents both BaseEncoder and MultibaseEncoder meaning it
 * can be used to encode to multibase or base encode without multibase
 * prefix.
 *
 * @class
 * @template {string} Base
 * @template {string} Prefix
 * @implements {API.MultibaseEncoder<Prefix>}
 * @implements {API.BaseEncoder}
 */
class Encoder {
  /**
   * @param {Base} name
   * @param {Prefix} prefix
   * @param {(bytes:Uint8Array) => string} baseEncode
   */
  constructor(name, prefix, baseEncode) {
    this.name = name;
    this.prefix = prefix;
    this.baseEncode = baseEncode;
  }

  /**
   * @param {Uint8Array} bytes
   * @returns {API.Multibase<Prefix>}
   */
  encode(bytes) {
    if (bytes instanceof Uint8Array) {
      return `${this.prefix}${this.baseEncode(bytes)}`;
    } else {
      throw Error("Unknown type, must be binary type");
    }
  }
}

/**
 * @template {string} Prefix
 */
/**
 * Class represents both BaseDecoder and MultibaseDecoder so it could be used
 * to decode multibases (with matching prefix) or just base decode strings
 * with corresponding base encoding.
 *
 * @class
 * @template {string} Base
 * @template {string} Prefix
 * @implements {API.MultibaseDecoder<Prefix>}
 * @implements {API.UnibaseDecoder<Prefix>}
 * @implements {API.BaseDecoder}
 */
class Decoder {
  /**
   * @param {Base} name
   * @param {Prefix} prefix
   * @param {(text:string) => Uint8Array} baseDecode
   */
  constructor(name, prefix, baseDecode) {
    this.name = name;
    this.prefix = prefix;
    /* c8 ignore next 3 */
    if (prefix.codePointAt(0) === undefined) {
      throw new Error("Invalid prefix character");
    }
    /** @private */
    this.prefixCodePoint = /** @type {number} */ (prefix.codePointAt(0));
    this.baseDecode = baseDecode;
  }

  /**
   * @param {string} text
   */
  decode(text) {
    if (typeof text === "string") {
      if (text.codePointAt(0) !== this.prefixCodePoint) {
        throw Error(
          `Unable to decode multibase string ${JSON.stringify(text)}, ${
            this.name
          } decoder only supports inputs prefixed with ${this.prefix}`
        );
      }
      return this.baseDecode(text.slice(this.prefix.length));
    } else {
      throw Error("Can only multibase decode strings");
    }
  }

  /**
   * @template {string} OtherPrefix
   * @param {API.UnibaseDecoder<OtherPrefix>|ComposedDecoder<OtherPrefix>} decoder
   * @returns {ComposedDecoder<Prefix|OtherPrefix>}
   */
  or(decoder) {
    return or$2(this, decoder);
  }
}

/**
 * @template {string} Prefix
 * @typedef {Record<Prefix, API.UnibaseDecoder<Prefix>>} Decoders
 */

/**
 * @template {string} Prefix
 * @implements {API.MultibaseDecoder<Prefix>}
 * @implements {API.CombobaseDecoder<Prefix>}
 */
class ComposedDecoder {
  /**
   * @param {Decoders<Prefix>} decoders
   */
  constructor(decoders) {
    this.decoders = decoders;
  }

  /**
   * @template {string} OtherPrefix
   * @param {API.UnibaseDecoder<OtherPrefix>|ComposedDecoder<OtherPrefix>} decoder
   * @returns {ComposedDecoder<Prefix|OtherPrefix>}
   */
  or(decoder) {
    return or$2(this, decoder);
  }

  /**
   * @param {string} input
   * @returns {Uint8Array}
   */
  decode(input) {
    const prefix = /** @type {Prefix} */ (input[0]);
    const decoder = this.decoders[prefix];
    if (decoder) {
      return decoder.decode(input);
    } else {
      throw RangeError(
        `Unable to decode multibase string ${JSON.stringify(
          input
        )}, only inputs prefixed with ${Object.keys(
          this.decoders
        )} are supported`
      );
    }
  }
}

/**
 * @template {string} L
 * @template {string} R
 * @param {API.UnibaseDecoder<L>|API.CombobaseDecoder<L>} left
 * @param {API.UnibaseDecoder<R>|API.CombobaseDecoder<R>} right
 * @returns {ComposedDecoder<L|R>}
 */
const or$2 = (left, right) =>
  new ComposedDecoder(
    /** @type {Decoders<L|R>} */ ({
      ...(left.decoders || {
        [/** @type API.UnibaseDecoder<L> */ (left).prefix]: left,
      }),
      ...(right.decoders || {
        [/** @type API.UnibaseDecoder<R> */ (right).prefix]: right,
      }),
    })
  );

/**
 * @class
 * @template {string} Base
 * @template {string} Prefix
 * @implements {API.MultibaseCodec<Prefix>}
 * @implements {API.MultibaseEncoder<Prefix>}
 * @implements {API.MultibaseDecoder<Prefix>}
 * @implements {API.BaseCodec}
 * @implements {API.BaseEncoder}
 * @implements {API.BaseDecoder}
 */
class Codec {
  /**
   * @param {Base} name
   * @param {Prefix} prefix
   * @param {(bytes:Uint8Array) => string} baseEncode
   * @param {(text:string) => Uint8Array} baseDecode
   */
  constructor(name, prefix, baseEncode, baseDecode) {
    this.name = name;
    this.prefix = prefix;
    this.baseEncode = baseEncode;
    this.baseDecode = baseDecode;
    this.encoder = new Encoder(name, prefix, baseEncode);
    this.decoder = new Decoder(name, prefix, baseDecode);
  }

  /**
   * @param {Uint8Array} input
   */
  encode(input) {
    return this.encoder.encode(input);
  }

  /**
   * @param {string} input
   */
  decode(input) {
    return this.decoder.decode(input);
  }
}

/**
 * @template {string} Base
 * @template {string} Prefix
 * @param {object} options
 * @param {Base} options.name
 * @param {Prefix} options.prefix
 * @param {(bytes:Uint8Array) => string} options.encode
 * @param {(input:string) => Uint8Array} options.decode
 * @returns {Codec<Base, Prefix>}
 */
const from$5 = ({ name, prefix, encode, decode }) =>
  new Codec(name, prefix, encode, decode);

/**
 * @template {string} Base
 * @template {string} Prefix
 * @param {object} options
 * @param {Base} options.name
 * @param {Prefix} options.prefix
 * @param {string} options.alphabet
 * @returns {Codec<Base, Prefix>}
 */
const baseX = ({ prefix, name, alphabet }) => {
  const { encode, decode } = _brrp__multiformats_scope_baseX(alphabet, name);
  return from$5({
    prefix,
    name,
    encode,
    /**
     * @param {string} text
     */
    decode: (text) => coerce(decode(text)),
  });
};

/**
 * @param {string} string
 * @param {string} alphabet
 * @param {number} bitsPerChar
 * @param {string} name
 * @returns {Uint8Array}
 */
const decode$2 = (string, alphabet, bitsPerChar, name) => {
  // Build the character lookup table:
  /** @type {Record<string, number>} */
  const codes = {};
  for (let i = 0; i < alphabet.length; ++i) {
    codes[alphabet[i]] = i;
  }

  // Count the padding bytes:
  let end = string.length;
  while (string[end - 1] === "=") {
    --end;
  }

  // Allocate the output:
  const out = new Uint8Array(((end * bitsPerChar) / 8) | 0);

  // Parse the data:
  let bits = 0; // Number of bits currently in the buffer
  let buffer = 0; // Bits waiting to be written out, MSB first
  let written = 0; // Next byte to write
  for (let i = 0; i < end; ++i) {
    // Read one character from the string:
    const value = codes[string[i]];
    if (value === undefined) {
      throw new SyntaxError(`Non-${name} character`);
    }

    // Append the bits to the buffer:
    buffer = (buffer << bitsPerChar) | value;
    bits += bitsPerChar;

    // Write out some bits if the buffer has a byte's worth:
    if (bits >= 8) {
      bits -= 8;
      out[written++] = 0xff & (buffer >> bits);
    }
  }

  // Verify that we have received just enough bits:
  if (bits >= bitsPerChar || 0xff & (buffer << (8 - bits))) {
    throw new SyntaxError("Unexpected end of data");
  }

  return out;
};

/**
 * @param {Uint8Array} data
 * @param {string} alphabet
 * @param {number} bitsPerChar
 * @returns {string}
 */
const encode$3 = (data, alphabet, bitsPerChar) => {
  const pad = alphabet[alphabet.length - 1] === "=";
  const mask = (1 << bitsPerChar) - 1;
  let out = "";

  let bits = 0; // Number of bits currently in the buffer
  let buffer = 0; // Bits waiting to be written out, MSB first
  for (let i = 0; i < data.length; ++i) {
    // Slurp data into the buffer:
    buffer = (buffer << 8) | data[i];
    bits += 8;

    // Write out as much as we can:
    while (bits > bitsPerChar) {
      bits -= bitsPerChar;
      out += alphabet[mask & (buffer >> bits)];
    }
  }

  // Partial character:
  if (bits) {
    out += alphabet[mask & (buffer << (bitsPerChar - bits))];
  }

  // Add padding characters until we hit a byte boundary:
  if (pad) {
    while ((out.length * bitsPerChar) & 7) {
      out += "=";
    }
  }

  return out;
};

/**
 * RFC4648 Factory
 *
 * @template {string} Base
 * @template {string} Prefix
 * @param {object} options
 * @param {Base} options.name
 * @param {Prefix} options.prefix
 * @param {string} options.alphabet
 * @param {number} options.bitsPerChar
 */
const rfc4648 = ({ name, prefix, bitsPerChar, alphabet }) => {
  return from$5({
    prefix,
    name,
    encode(input) {
      return encode$3(input, alphabet, bitsPerChar);
    },
    decode(input) {
      return decode$2(input, alphabet, bitsPerChar, name);
    },
  });
};

const base58btc = baseX({
  name: "base58btc",
  prefix: "z",
  alphabet: "123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz",
});

baseX({
  name: "base58flickr",
  prefix: "Z",
  alphabet: "123456789abcdefghijkmnopqrstuvwxyzABCDEFGHJKLMNPQRSTUVWXYZ",
});

const base32 = rfc4648({
  prefix: "b",
  name: "base32",
  alphabet: "abcdefghijklmnopqrstuvwxyz234567",
  bitsPerChar: 5,
});

rfc4648({
  prefix: "B",
  name: "base32upper",
  alphabet: "ABCDEFGHIJKLMNOPQRSTUVWXYZ234567",
  bitsPerChar: 5,
});

rfc4648({
  prefix: "c",
  name: "base32pad",
  alphabet: "abcdefghijklmnopqrstuvwxyz234567=",
  bitsPerChar: 5,
});

rfc4648({
  prefix: "C",
  name: "base32padupper",
  alphabet: "ABCDEFGHIJKLMNOPQRSTUVWXYZ234567=",
  bitsPerChar: 5,
});

rfc4648({
  prefix: "v",
  name: "base32hex",
  alphabet: "0123456789abcdefghijklmnopqrstuv",
  bitsPerChar: 5,
});

rfc4648({
  prefix: "V",
  name: "base32hexupper",
  alphabet: "0123456789ABCDEFGHIJKLMNOPQRSTUV",
  bitsPerChar: 5,
});

rfc4648({
  prefix: "t",
  name: "base32hexpad",
  alphabet: "0123456789abcdefghijklmnopqrstuv=",
  bitsPerChar: 5,
});

rfc4648({
  prefix: "T",
  name: "base32hexpadupper",
  alphabet: "0123456789ABCDEFGHIJKLMNOPQRSTUV=",
  bitsPerChar: 5,
});

rfc4648({
  prefix: "h",
  name: "base32z",
  alphabet: "ybndrfg8ejkmcpqxot1uwisza345h769",
  bitsPerChar: 5,
});

/**
 * @template {API.Link<unknown, number, number, API.Version>} T
 * @template {string} Prefix
 * @param {T} link
 * @param {API.MultibaseEncoder<Prefix>} [base]
 * @returns {API.ToString<T, Prefix>}
 */
const format = (link, base) => {
  const { bytes, version } = link;
  switch (version) {
    case 0:
      return toStringV0(
        bytes,
        baseCache(link),
        /** @type {API.MultibaseEncoder<"z">} */ (base) || base58btc.encoder
      );
    default:
      return toStringV1(
        bytes,
        baseCache(link),
        /** @type {API.MultibaseEncoder<Prefix>} */ (base || base32.encoder)
      );
  }
};

/** @type {WeakMap<API.UnknownLink, Map<string, string>>} */
const cache$1 = new WeakMap();

/**
 * @param {API.UnknownLink} cid
 * @returns {Map<string, string>}
 */
const baseCache = (cid) => {
  const baseCache = cache$1.get(cid);
  if (baseCache == null) {
    const baseCache = new Map();
    cache$1.set(cid, baseCache);
    return baseCache;
  }
  return baseCache;
};

/**
 * @template {unknown} [Data=unknown]
 * @template {number} [Format=number]
 * @template {number} [Alg=number]
 * @template {API.Version} [Version=API.Version]
 * @implements {API.Link<Data, Format, Alg, Version>}
 */

class CID {
  /**
   * @param {Version} version - Version of the CID
   * @param {Format} code - Code of the codec content is encoded in, see https://github.com/multiformats/multicodec/blob/master/table.csv
   * @param {API.MultihashDigest<Alg>} multihash - (Multi)hash of the of the content.
   * @param {Uint8Array} bytes
   *
   */
  constructor(version, code, multihash, bytes) {
    /** @readonly */
    this.code = code;
    /** @readonly */
    this.version = version;
    /** @readonly */
    this.multihash = multihash;
    /** @readonly */
    this.bytes = bytes;

    // flag to serializers that this is a CID and
    // should be treated specially
    /** @readonly */
    this["/"] = bytes;
  }

  /**
   * Signalling `cid.asCID === cid` has been replaced with `cid['/'] === cid.bytes`
   * please either use `CID.asCID(cid)` or switch to new signalling mechanism
   *
   * @deprecated
   */
  get asCID() {
    return this;
  }

  // ArrayBufferView
  get byteOffset() {
    return this.bytes.byteOffset;
  }

  // ArrayBufferView
  get byteLength() {
    return this.bytes.byteLength;
  }

  /**
   * @returns {CID<Data, API.DAG_PB, API.SHA_256, 0>}
   */
  toV0() {
    switch (this.version) {
      case 0: {
        return /** @type {CID<Data, API.DAG_PB, API.SHA_256, 0>} */ (this);
      }
      case 1: {
        const { code, multihash } = this;

        if (code !== DAG_PB_CODE) {
          throw new Error("Cannot convert a non dag-pb CID to CIDv0");
        }

        // sha2-256
        if (multihash.code !== SHA_256_CODE) {
          throw new Error("Cannot convert non sha2-256 multihash CID to CIDv0");
        }

        return /** @type {CID<Data, API.DAG_PB, API.SHA_256, 0>} */ (
          CID.createV0(
            /** @type {API.MultihashDigest<API.SHA_256>} */ (multihash)
          )
        );
      }
      default: {
        throw Error(
          `Can not convert CID version ${this.version} to version 0. This is a bug please report`
        );
      }
    }
  }

  /**
   * @returns {CID<Data, Format, Alg, 1>}
   */
  toV1() {
    switch (this.version) {
      case 0: {
        const { code, digest } = this.multihash;
        const multihash = create$8(code, digest);
        return /** @type {CID<Data, Format, Alg, 1>} */ (
          CID.createV1(this.code, multihash)
        );
      }
      case 1: {
        return /** @type {CID<Data, Format, Alg, 1>} */ (this);
      }
      default: {
        throw Error(
          `Can not convert CID version ${this.version} to version 1. This is a bug please report`
        );
      }
    }
  }

  /**
   * @param {unknown} other
   * @returns {other is CID<Data, Format, Alg, Version>}
   */
  equals(other) {
    return CID.equals(this, other);
  }

  /**
   * @template {unknown} Data
   * @template {number} Format
   * @template {number} Alg
   * @template {API.Version} Version
   * @param {API.Link<Data, Format, Alg, Version>} self
   * @param {unknown} other
   * @returns {other is CID}
   */
  static equals(self, other) {
    const unknown =
      /** @type {{code?:unknown, version?:unknown, multihash?:unknown}} */ (
        other
      );
    return (
      unknown &&
      self.code === unknown.code &&
      self.version === unknown.version &&
      equals(self.multihash, unknown.multihash)
    );
  }

  /**
   * @param {API.MultibaseEncoder<string>} [base]
   * @returns {string}
   */
  toString(base) {
    return format(this, base);
  }

  toJSON() {
    return { "/": format(this) };
  }

  link() {
    return this;
  }

  get [Symbol.toStringTag]() {
    return "CID";
  }

  // Legacy

  [Symbol.for("nodejs.util.inspect.custom")]() {
    return `CID(${this.toString()})`;
  }

  /**
   * Takes any input `value` and returns a `CID` instance if it was
   * a `CID` otherwise returns `null`. If `value` is instanceof `CID`
   * it will return value back. If `value` is not instance of this CID
   * class, but is compatible CID it will return new instance of this
   * `CID` class. Otherwise returns null.
   *
   * This allows two different incompatible versions of CID library to
   * co-exist and interop as long as binary interface is compatible.
   *
   * @template {unknown} Data
   * @template {number} Format
   * @template {number} Alg
   * @template {API.Version} Version
   * @template {unknown} U
   * @param {API.Link<Data, Format, Alg, Version>|U} input
   * @returns {CID<Data, Format, Alg, Version>|null}
   */
  static asCID(input) {
    if (input == null) {
      return null;
    }

    const value = /** @type {any} */ (input);
    if (value instanceof CID) {
      // If value is instance of CID then we're all set.
      return value;
    } else if (
      (value["/"] != null && value["/"] === value.bytes) ||
      value.asCID === value
    ) {
      // If value isn't instance of this CID class but `this.asCID === this` or
      // `value['/'] === value.bytes` is true it is CID instance coming from a
      // different implementation (diff version or duplicate). In that case we
      // rebase it to this `CID` implementation so caller is guaranteed to get
      // instance with expected API.
      const { version, code, multihash, bytes } = value;
      return new CID(
        version,
        code,
        /** @type {API.MultihashDigest<Alg>} */ (multihash),
        bytes || encodeCID(version, code, multihash.bytes)
      );
    } else if (value[cidSymbol] === true) {
      // If value is a CID from older implementation that used to be tagged via
      // symbol we still rebase it to the this `CID` implementation by
      // delegating that to a constructor.
      const { version, multihash, code } = value;
      const digest =
        /** @type {API.MultihashDigest<Alg>} */
        (decode$3(multihash));
      return CID.create(version, code, digest);
    } else {
      // Otherwise value is not a CID (or an incompatible version of it) in
      // which case we return `null`.
      return null;
    }
  }

  /**
   *
   * @template {unknown} Data
   * @template {number} Format
   * @template {number} Alg
   * @template {API.Version} Version
   * @param {Version} version - Version of the CID
   * @param {Format} code - Code of the codec content is encoded in, see https://github.com/multiformats/multicodec/blob/master/table.csv
   * @param {API.MultihashDigest<Alg>} digest - (Multi)hash of the of the content.
   * @returns {CID<Data, Format, Alg, Version>}
   */
  static create(version, code, digest) {
    if (typeof code !== "number") {
      throw new Error("String codecs are no longer supported");
    }

    if (!(digest.bytes instanceof Uint8Array)) {
      throw new Error("Invalid digest");
    }

    switch (version) {
      case 0: {
        if (code !== DAG_PB_CODE) {
          throw new Error(
            `Version 0 CID must use dag-pb (code: ${DAG_PB_CODE}) block encoding`
          );
        } else {
          return new CID(version, code, digest, digest.bytes);
        }
      }
      case 1: {
        const bytes = encodeCID(version, code, digest.bytes);
        return new CID(version, code, digest, bytes);
      }
      default: {
        throw new Error("Invalid version");
      }
    }
  }

  /**
   * Simplified version of `create` for CIDv0.
   *
   * @template {unknown} [T=unknown]
   * @param {API.MultihashDigest<typeof SHA_256_CODE>} digest - Multihash.
   * @returns {CID<T, typeof DAG_PB_CODE, typeof SHA_256_CODE, 0>}
   */
  static createV0(digest) {
    return CID.create(0, DAG_PB_CODE, digest);
  }

  /**
   * Simplified version of `create` for CIDv1.
   *
   * @template {unknown} Data
   * @template {number} Code
   * @template {number} Alg
   * @param {Code} code - Content encoding format code.
   * @param {API.MultihashDigest<Alg>} digest - Miltihash of the content.
   * @returns {CID<Data, Code, Alg, 1>}
   */
  static createV1(code, digest) {
    return CID.create(1, code, digest);
  }

  /**
   * Decoded a CID from its binary representation. The byte array must contain
   * only the CID with no additional bytes.
   *
   * An error will be thrown if the bytes provided do not contain a valid
   * binary representation of a CID.
   *
   * @template {unknown} Data
   * @template {number} Code
   * @template {number} Alg
   * @template {API.Version} Ver
   * @param {API.ByteView<API.Link<Data, Code, Alg, Ver>>} bytes
   * @returns {CID<Data, Code, Alg, Ver>}
   */
  static decode(bytes) {
    const [cid, remainder] = CID.decodeFirst(bytes);
    if (remainder.length) {
      throw new Error("Incorrect length");
    }
    return cid;
  }

  /**
   * Decoded a CID from its binary representation at the beginning of a byte
   * array.
   *
   * Returns an array with the first element containing the CID and the second
   * element containing the remainder of the original byte array. The remainder
   * will be a zero-length byte array if the provided bytes only contained a
   * binary CID representation.
   *
   * @template {unknown} T
   * @template {number} C
   * @template {number} A
   * @template {API.Version} V
   * @param {API.ByteView<API.Link<T, C, A, V>>} bytes
   * @returns {[CID<T, C, A, V>, Uint8Array]}
   */
  static decodeFirst(bytes) {
    const specs = CID.inspectBytes(bytes);
    const prefixSize = specs.size - specs.multihashSize;
    const multihashBytes = coerce(
      bytes.subarray(prefixSize, prefixSize + specs.multihashSize)
    );
    if (multihashBytes.byteLength !== specs.multihashSize) {
      throw new Error("Incorrect length");
    }
    const digestBytes = multihashBytes.subarray(
      specs.multihashSize - specs.digestSize
    );
    const digest = new Digest$1(
      specs.multihashCode,
      specs.digestSize,
      digestBytes,
      multihashBytes
    );
    const cid =
      specs.version === 0
        ? CID.createV0(/** @type {API.MultihashDigest<API.SHA_256>} */ (digest))
        : CID.createV1(specs.codec, digest);
    return [/** @type {CID<T, C, A, V>} */ (cid), bytes.subarray(specs.size)];
  }

  /**
   * Inspect the initial bytes of a CID to determine its properties.
   *
   * Involves decoding up to 4 varints. Typically this will require only 4 to 6
   * bytes but for larger multicodec code values and larger multihash digest
   * lengths these varints can be quite large. It is recommended that at least
   * 10 bytes be made available in the `initialBytes` argument for a complete
   * inspection.
   *
   * @template {unknown} T
   * @template {number} C
   * @template {number} A
   * @template {API.Version} V
   * @param {API.ByteView<API.Link<T, C, A, V>>} initialBytes
   * @returns {{ version:V, codec:C, multihashCode:A, digestSize:number, multihashSize:number, size:number }}
   */
  static inspectBytes(initialBytes) {
    let offset = 0;
    const next = () => {
      const [i, length] = decode$4(initialBytes.subarray(offset));
      offset += length;
      return i;
    };

    let version = /** @type {V} */ (next());
    let codec = /** @type {C} */ (DAG_PB_CODE);
    if (/** @type {number} */ (version) === 18) {
      // CIDv0
      version = /** @type {V} */ (0);
      offset = 0;
    } else {
      codec = /** @type {C} */ (next());
    }

    if (version !== 0 && version !== 1) {
      throw new RangeError(`Invalid CID version ${version}`);
    }

    const prefixSize = offset;
    const multihashCode = /** @type {A} */ (next()); // multihash code
    const digestSize = next(); // multihash length
    const size = offset + digestSize;
    const multihashSize = size - prefixSize;

    return { version, codec, multihashCode, digestSize, multihashSize, size };
  }

  /**
   * Takes cid in a string representation and creates an instance. If `base`
   * decoder is not provided will use a default from the configuration. It will
   * throw an error if encoding of the CID is not compatible with supplied (or
   * a default decoder).
   *
   * @template {string} Prefix
   * @template {unknown} Data
   * @template {number} Code
   * @template {number} Alg
   * @template {API.Version} Ver
   * @param {API.ToString<API.Link<Data, Code, Alg, Ver>, Prefix>} source
   * @param {API.MultibaseDecoder<Prefix>} [base]
   * @returns {CID<Data, Code, Alg, Ver>}
   */
  static parse(source, base) {
    const [prefix, bytes] = parseCIDtoBytes(source, base);

    const cid = CID.decode(bytes);

    if (cid.version === 0 && source[0] !== "Q") {
      throw Error("Version 0 CID string must not include multibase prefix");
    }

    // Cache string representation to avoid computing it on `this.toString()`
    baseCache(cid).set(prefix, source);

    return cid;
  }
}

/**
 * @template {string} Prefix
 * @template {unknown} Data
 * @template {number} Code
 * @template {number} Alg
 * @template {API.Version} Ver
 * @param {API.ToString<API.Link<Data, Code, Alg, Ver>, Prefix>} source
 * @param {API.MultibaseDecoder<Prefix>} [base]
 * @returns {[Prefix, API.ByteView<API.Link<Data, Code, Alg, Ver>>]}
 */
const parseCIDtoBytes = (source, base) => {
  switch (source[0]) {
    // CIDv0 is parsed differently
    case "Q": {
      const decoder = base || base58btc;
      return [
        /** @type {Prefix} */ (base58btc.prefix),
        decoder.decode(`${base58btc.prefix}${source}`),
      ];
    }
    case base58btc.prefix: {
      const decoder = base || base58btc;
      return [/** @type {Prefix} */ (base58btc.prefix), decoder.decode(source)];
    }
    case base32.prefix: {
      const decoder = base || base32;
      return [/** @type {Prefix} */ (base32.prefix), decoder.decode(source)];
    }
    default: {
      if (base == null) {
        throw Error(
          "To parse non base32 or base58btc encoded CID multibase decoder must be provided"
        );
      }
      return [/** @type {Prefix} */ (source[0]), base.decode(source)];
    }
  }
};

/**
 *
 * @param {Uint8Array} bytes
 * @param {Map<string, string>} cache
 * @param {API.MultibaseEncoder<'z'>} base
 */
const toStringV0 = (bytes, cache, base) => {
  const { prefix } = base;
  if (prefix !== base58btc.prefix) {
    throw Error(`Cannot string encode V0 in ${base.name} encoding`);
  }

  const cid = cache.get(prefix);
  if (cid == null) {
    const cid = base.encode(bytes).slice(1);
    cache.set(prefix, cid);
    return cid;
  } else {
    return cid;
  }
};

/**
 * @template {string} Prefix
 * @param {Uint8Array} bytes
 * @param {Map<string, string>} cache
 * @param {API.MultibaseEncoder<Prefix>} base
 */
const toStringV1 = (bytes, cache, base) => {
  const { prefix } = base;
  const cid = cache.get(prefix);
  if (cid == null) {
    const cid = base.encode(bytes);
    cache.set(prefix, cid);
    return cid;
  } else {
    return cid;
  }
};

const DAG_PB_CODE = 0x70;
const SHA_256_CODE = 0x12;

/**
 * @param {API.Version} version
 * @param {number} code
 * @param {Uint8Array} multihash
 * @returns {Uint8Array}
 */
const encodeCID = (version, code, multihash) => {
  const codeOffset = encodingLength$1(version);
  const hashOffset = codeOffset + encodingLength$1(code);
  const bytes = new Uint8Array(hashOffset + multihash.byteLength);
  encodeTo$1(version, bytes, 0);
  encodeTo$1(code, bytes, codeOffset);
  bytes.set(multihash, hashOffset);
  return bytes;
};

const cidSymbol = Symbol.for("@ipld/js-cid/CID");

/**
 * Type representing a state of the balanced tree. First row hold leaves coming
 * into a builder, once number of leaves in the stack reaches `maxChildren` they
 * are moved into `RootNode` instance which is pushed into the next row of nodes.
 * If next row now contains `maxChildren` nodes from there are again moved into
 * a new `RootNode` and pushed into next row etc...
 *
 * For illustration let's assume we have `maxChildren: 3`, after 3 leafs were
 * added tree will have following layout
 *
 * ```
 *           (root1)
 *              |
 *    ----------------------
 *    |         |          |
 * (leaf1)   (leaf2)    (leaf3)
 * ```
 *
 * Which in our model before flushing is represented as follows:
 *
 * ```js
 * {
 *    width: 3
 *    leafIndex: [leaf1, leaf2, leaf3]
 *    nodeIndex: []
 *    nodes: []
 * }
 * ```
 *
 * After flushing 3 leaves (which is width) are moved into a `RootNode` that
 * is added to `nodes` array (and returned so that caller can create a block).
 * Additionally position of the added node is captured in the `index` at an
 * appropriate depth `0` (that is because we don't count leaves into depth).
 *
 * ```js
 * {
 *    width: 3
 *    leafIndex: []
 *    nodeIndex: [[0]]
 *    nodes: [new RootNode([leaf1, leaf2, leaf3])]
 * }
 * ```
 *
 * Increasing number of leaves to 10 would produce following tree layout
 *
 *```
 *                                                         (root7)
 *                                                           |
 *                                    ------------------------------------------
 *                                    |                                        |
 *                                 (root4)                                  (root6)
 *                                    |                                        |
 *            -------------------------------------------------                |
 *            |                       |                       |                |
 *         (root1)                 (root2)                 (root3)          (root5)
 *            |                       |                       |                |
 *    --------|--------       --------|--------       --------|--------        |
 *    |       |       |       |       |       |       |       |       |        |
 * (leaf1) (leaf2) (leaf3) (leaf4) (leaf5) (leaf6) (leaf7) (leaf8) (leaf9) (leaf10)
 * ```
 *
 * Which in our model will look as follows (note we do not have root5 - root7
 * in model because they are build once width is reached or once builder is
 * closed)
 *
 * ```js
 * {
 *    width: 3
 *    leafIndex: [leaf10]
 *    nodeIndex: [
 *      [0, 1, 2], // [r1, r2, r3]
 *      [3]        // [r4]
 *     ]
 *    nodes: [
 *      new Node([leaf1, leaf2, leaf3]), // r1
 *      new Node([leaf4, leaf5, leaf6]), // r2
 *      new Node([leaf7, leaf8, leaf9]), // r3
 *      new Node([ // r4
 *         new Node([leaf1, leaf2, leaf3]), // r1
 *         new Node([leaf4, leaf5, leaf6]), // r2
 *         new Node([leaf7, leaf8, leaf9]), // r3
 *      ])
 *    ]
 * }
 * ```
 *
 * @typedef {{
 * width: number
 * head: Chunker.Chunk | null
 * leafIndex: number[]
 * nodeIndex: number[][]
 * lastID: number
 * }} Balanced
 */

class Node {
  /**
   *
   * @param {number} id
   * @param {number[]} children
   * @param {Layout.Metadata} [metadata]
   */
  constructor(id, children, metadata) {
    this.id = id;
    this.children = children;
    this.metadata = metadata;
  }
}

/**
 * @typedef Options
 * @property {number} width - Max children per node.
 *
 * @param {number} width
 * @returns {Layout.LayoutEngine<Balanced>}
 */
const withWidth = (width) => ({
  open: () => open({ width }),
  write: write$1,
  close: close$4,
});

const defaults$3 = { width: 174 };

/**
 * @param {Options} options
 * @returns {Balanced}
 */
const open = ({ width } = defaults$3) => ({
  width,

  head: null,
  leafIndex: [],
  nodeIndex: [],
  lastID: 0,
});

/**
 *
 * @param {Balanced} layout
 * @param {Chunker.Chunk[]} chunks
 * @returns {Layout.WriteResult<Balanced>}
 */
const write$1 = (layout, chunks) => {
  if (chunks.length === 0) {
    return { layout, nodes: EMPTY, leaves: EMPTY };
  } else {
    let { lastID } = layout;
    // We need to hold on to the first chunk until we either get a second chunk
    // (at which point we know our layout will have branches) or until we close
    // (at which point our layout will be single leaf or node depneding on
    // metadata)
    const [head, slices] = layout.head
      ? // If we had a head we have more then two chunks (we already checked
        // chunks weren't empty) so we process head along with other chunks.
        [null, (chunks.unshift(layout.head), chunks)]
      : // If we have no head no leaves and got only one chunk we have to save it
      // until we can decide what to do with it.
      chunks.length === 1 && layout.leafIndex.length === 0
      ? [chunks[0], EMPTY]
      : // Otherwise we have no head but got enough chunks to know we'll have a
        // node.
        [null, chunks];

    if (slices.length === 0) {
      return { layout: { ...layout, head }, nodes: EMPTY, leaves: EMPTY };
    } else {
      const leafIndex = [...layout.leafIndex];
      const leaves = [];
      for (const chunk of slices) {
        const leaf = { id: ++lastID, content: chunk };
        leaves.push(leaf);
        leafIndex.push(leaf.id);
      }

      if (leafIndex.length > layout.width) {
        return flush({ ...layout, leafIndex, head, lastID }, leaves);
      } else {
        return {
          layout: { ...layout, head, leafIndex, lastID },
          leaves,
          nodes: EMPTY,
        };
      }
    }
  }
};

/**
 * @param {Balanced} state
 * @param {Layout.Leaf[]} leaves
 * @param {Layout.Branch[]} [nodes]
 * @param {boolean} [close]
 * @returns {Layout.WriteResult<Balanced>}
 */
const flush = (state, leaves = EMPTY, nodes = [], close = false) => {
  let { lastID } = state;
  const nodeIndex = state.nodeIndex.map((row) => [...row]);
  const leafIndex = [...state.leafIndex];
  const { width } = state;

  // Move leaves into nodes
  while (leafIndex.length > width || (leafIndex.length > 0 && close)) {
    grow(nodeIndex, 1);
    const node = new Node(++lastID, leafIndex.splice(0, width));
    nodeIndex[0].push(node.id);
    nodes.push(node);
  }

  let depth = 0;
  while (depth < nodeIndex.length) {
    const row = nodeIndex[depth];
    depth++;

    while (
      row.length > width ||
      (row.length > 0 && close && depth < nodeIndex.length)
    ) {
      const node = new Node(++lastID, row.splice(0, width));
      grow(nodeIndex, depth + 1);
      nodeIndex[depth].push(node.id);
      nodes.push(node);
    }
  }

  return { layout: { ...state, lastID, leafIndex, nodeIndex }, leaves, nodes };
};

/**
 * @param {Balanced} layout
 * @param {Layout.Metadata} [metadata]
 * @returns {Layout.CloseResult}
 */
const close$4 = (layout, metadata) => {
  const state = layout;
  if (layout.head) {
    return {
      root: { id: 1, content: layout.head, metadata },
      leaves: EMPTY,
      nodes: EMPTY,
    };
  } else if (layout.leafIndex.length === 0) {
    return {
      root: { id: 1, metadata },
      leaves: EMPTY,
      nodes: EMPTY,
    };
  } else {
    // Flush with width 1 so all the items will be propagate up the tree
    // and height of `depth-1` so we propagate nodes all but from the top
    // most level
    const { nodes, layout } = flush(state, EMPTY, [], true);

    const { nodeIndex } = layout;
    const height = nodeIndex.length - 1;

    const top = nodeIndex[height];
    if (top.length === 1) {
      const root = nodes[nodes.length - 1];
      nodes.length = nodes.length - 1;
      return { root, nodes, leaves: EMPTY };
    } else {
      const root = new Node(layout.lastID + 1, top, metadata);
      return { root, nodes, leaves: EMPTY };
    }
  }
};

/**
 * @template T
 * @param {T[][]} index
 * @param {number} length
 */
const grow = (index, length) => {
  while (index.length < length) {
    index.push([]);
  }
  return index;
};

/** @type {never[]} */
const EMPTY = [];

/**
 * @returns {API.EncoderSettings}
 */
const defaults$2 = () => ({
  chunker: FixedSize,
  fileChunkEncoder: UnixFSLeaf,
  smallFileEncoder: UnixFSLeaf,
  fileEncoder: UnixFS,
  fileLayout: withWidth(174),
  hasher: sha256$1,
  linker: { createLink: CID.createV1 },
});

/**
 * @template {unknown} Layout
 * @param {Partial<API.EncoderSettings<Layout>>} config
 * @returns {API.EncoderSettings<Layout>}
 */
const configure$4 = (config) => ({
  ...defaults$2(),
  ...config,
});

const UnixFSLeaf = {
  code: code$1,
  name: name$1,
  encode: encodeFileChunk,
};

/**
 * @template Layout
 * @param {API.Options<Layout>} options
 * @returns {API.View<Layout>}
 */
const create$7 = ({ writer, metadata = {}, settings = defaults$2() }) =>
  new FileWriterView(init(writer, metadata, configure$4(settings)));

/**
 * @template T
 * @param {API.View<T>} view
 * @param {Uint8Array} bytes
 * @return {Promise<API.View<T>>}
 */

const write = async (view, bytes) => {
  await perform(view, send({ type: "write", bytes }));
  return view;
};

/**
 * @template T
 * @param {API.View<T>} view
 * @param {API.CloseOptions} options
 */
const close$3 = async (
  view,
  { releaseLock = false, closeWriter = false } = {}
) => {
  await perform(view, send({ type: "close" }));
  const { state } = view;
  if (state.status === "linked") {
    if (closeWriter) {
      await view.state.writer.close();
    } else if (releaseLock) {
      view.state.writer.releaseLock();
    }
    return state.link;
    /* c8 ignore next 5 */
  } else {
    panic(
      `Expected writer to be in 'linked' state after close, but it is in "${state.status}" instead`
    );
  }
};

/**
 * @template T
 * @param {API.View<T>} view
 * @param {Task.Effect<Writer.Message>} effect
 */
const perform = (view, effect) =>
  fork$3(
    loop(effect, (message) => {
      const { state, effect } = update(message, view.state);
      view.state = state;
      return effect;
    })
  );

/**
 * @template Layout
 * @implements {API.View<Layout>}
 */
class FileWriterView {
  /**
   * @param {Writer.State<Layout>} state
   */
  constructor(state) {
    this.state = state;
  }
  get writer() {
    return this.state.writer;
  }
  get settings() {
    return this.state.config;
  }
  /**
   * @param {Uint8Array} bytes
   * @returns {Promise<API.View<Layout>>}
   */
  write(bytes) {
    return write(this, bytes);
  }
  /**
   * @param {API.CloseOptions} [options]
   * @returns {Promise<UnixFS.FileLink>}
   */
  close(options) {
    return close$3(this, options);
  }
}

const defaults$1 = defaults$2;

/**
 * @template [Layout=unknown]
 * @param {API.Options<Layout>} config
 * @returns {API.View<Layout>}
 */
const create$6 = ({ writer, settings = defaults$1(), metadata = {} }) =>
  new DirectoryWriter({
    writer,
    metadata,
    settings,
    entries: new Map(),
    closed: false,
  });

/**
 * @template {unknown} L
 * @template {{ state: API.State<L> }} View
 * @param {View} view
 * @param {string} name
 * @param {API.EntryLink} link
 * @param {API.WriteOptions} options
 */
const set$4 = (view, name, link, { overwrite = false } = {}) => {
  const writable = asWritable$1(view.state);
  if (name.includes("/")) {
    throw new Error(
      `Directory entry name "${name}" contains forbidden "/" character`
    );
  }
  if (!overwrite && writable.entries.has(name)) {
    throw new Error(`Directory already contains entry with name "${name}"`);
  } else {
    writable.entries.set(name, link);
    return view;
  }
};

/**
 * @template {unknown} L
 * @template {{ state: API.State<L> }} View
 * @param {View} view
 * @param {string} name
 */
const remove$1 = (view, name) => {
  const writer = asWritable$1(view.state);
  writer.entries.delete(name);
  return view;
};

/**
 * @template {API.State} Writer
 * @param {Writer} writer
 * @returns {Writer}
 */
const asWritable$1 = (writer) => {
  if (!writer.closed) {
    return writer;
  } else {
    throw new Error(
      `Can not change written directory, but you can .fork() and make changes to it`
    );
  }
};

/**
 * @template {unknown} Layout
 * @param {{ state: API.State<Layout> }} view
 * @param {API.CloseOptions} options
 * @returns {Promise<UnixFS.DirectoryLink>}
 */
const close$2 = async (
  view,
  { closeWriter = false, releaseLock = false } = {}
) => {
  const { writer, settings, metadata } = asWritable$1(view.state);
  view.state.closed = true;
  const entries = [...links$1(view)];
  const node = createFlatDirectory(entries, metadata);
  const bytes = encodeDirectory(node);
  const digest = await settings.hasher.digest(bytes);
  /** @type {UnixFS.Link<UnixFS.Directory>} */
  const cid = settings.linker.createLink(code$1, digest);

  // we make sure that writer has some capacity for this write. If it
  // does not we await.
  if ((writer.desiredSize || 0) <= 0) {
    await writer.ready;
  }
  // once writer has some capacity we write a block, however we do not
  // await completion as we don't care when it's taken off the stream.
  writer.write({ cid, bytes });

  if (closeWriter) {
    await writer.close();
  } else if (releaseLock) {
    writer.releaseLock();
  }

  return {
    cid,
    dagByteLength: cumulativeDagByteLength(bytes, entries),
  };
};

/**
 * @template {unknown} Layout
 * @param {{ state: API.State<Layout> }} view
 * @returns {IterableIterator<UnixFS.DirectoryEntryLink>}
 */
const links$1 = function* ({ state }) {
  for (const [name, { dagByteLength, cid }] of state.entries) {
    yield /** @type {UnixFS.DirectoryEntryLink} */ ({
      name,
      dagByteLength,
      cid,
    });
  }
};

/**
 * @template L1, L2
 * @param {API.View<L1>} state
 * @param {Partial<API.Options<L1|L2>>} options
 * @returns {API.View<L1|L2>}
 */
const fork$2 = (
  { state },
  {
    writer = state.writer,
    metadata = state.metadata,
    settings = state.settings,
  } = {}
) =>
  new DirectoryWriter({
    writer,
    metadata,
    settings,
    entries: new Map(state.entries.entries()),
    closed: false,
  });

/**
 * @template [Layout=unknown]
 * @implements {API.View<Layout>}
 */
class DirectoryWriter {
  /**
   * @param {API.State<Layout>} state
   */
  constructor(state) {
    this.state = state;
  }
  get writer() {
    return this.state.writer;
  }
  get settings() {
    return this.state.settings;
  }

  links() {
    return links$1(this);
  }

  /**
   * @param {string} name
   * @param {UnixFS.FileLink | UnixFS.DirectoryLink} link
   * @param {API.WriteOptions} [options]
   */

  set(name, link, options) {
    return set$4(this, name, link, options);
  }

  /**
   * @param {string} name
   */
  remove(name) {
    return remove$1(this, name);
  }

  /**
   * @template L
   * @param {Partial<API.Options<L>>} [options]
   * @returns {API.View<Layout|L>}
   */
  fork(options) {
    return fork$2(this, options);
  }

  /**
   * @param {API.CloseOptions} [options]
   * @returns {Promise<UnixFS.DirectoryLink>}
   */
  close(options) {
    return close$2(this, options);
  }

  entries() {
    return this.state.entries.entries();
  }
  /**
   * @param {string} name
   */
  has(name) {
    return this.state.entries.has(name);
  }
  get size() {
    return this.state.entries.size;
  }
}

var api = /*#__PURE__*/ Object.freeze({
  __proto__: null,
});

/**
 * @param {API.Uint32} size
 */
const empty$1 = (size = 32) => {
  // We could support < 32, but it seems impractical and would negatively affect
  // performance as we would have to do extra bound checks.
  if (size !== 32) {
    throw new Error(`Uint32 BitField does not support size: ${size}`);
  }

  return 0;
};

/**
 * @param  {API.Uint32[]} bits
 * @param {API.Uint32} [size]
 */
const from$4 = (bits, size) => {
  let bitfield = empty$1(size);
  for (const bit of bits) {
    bitfield = set$3(bitfield, bit);
  }
  return bitfield;
};

/**
 * @param {API.Uint32} _bitField
 */
const size$1 = (_bitField) => 32;

/**
 * Reads out 5 bits at the given bit offset.
 *
 * @param {API.Uint32} bitField - Bitfield in Uint32 representation.
 * @param {API.Uint32} index - Index with-in `bitField` to read bits from.
 * @returns {API.Uint32}
 */
const mask = (bitField, index) => (bitField >>> index) & 0b11111;

/**
 * Creates mask that can be used to check a bit in nodes bitmap for the give
 * key (hash) at given depth.
 *
 * @param {API.Uint32} bitField - Key hash as 32 bit integer.
 * @param {API.Uint32} index - Index with-in the 32bit bitfield
 */
const offset = (bitField, index) => 1 << mask(bitField, index);

/**
 * Maps numbers [0, 31] to powers of two. Creates mask that can be used
 * to check a bit in nodes bitmap for the give key (hash) at given depth.
 *
 * @param {API.Uint32} bitField - Key hash as 32 bit integer.
 * @param {API.Uint32} index - Index with-in the 32bit bitfield
 */
const popcount$1 = (bitField, index = 31) =>
  bitCount(bitField & (offset(index, 0) - 1));

/**
 * @param {API.Uint32} bitField
 * @param {API.Uint32} index
 */
const set$3 = (bitField, index) => bitField | (1 << index);

/**
 * @param {API.Uint32} bitField
 * @param {API.Uint32} index
 */
const unset$1 = (bitField, index) => bitField & (0xff ^ (1 << index));

/**
 * @param {API.Uint32} bitField
 * @param {API.Uint32} index
 */
const get$5 = (bitField, index) => ((bitField >> index) & 0x1) !== 0;

/**
 * Counts the number of bits set in n
 * @param {API.Uint32} bitField
 */
const bitCount = (bitField) => {
  const n1 = bitField - ((bitField >> 1) & 0x55555555);
  const n2 = (n1 & 0x33333333) + ((n1 >> 2) & 0x33333333);
  const n3 = ((n2 + (n2 >> 4)) & 0xf0f0f0f) * 0x1010101;
  return n3 >> 24;
};

/**
 * @param {API.Uint32} left
 * @param {API.Uint32} right
 * @returns {API.Uint32}
 */
const and$1 = (left, right) => left & right;

/**
 * @param {API.Uint32} left
 * @param {API.Uint32} right
 * @returns {API.Uint32}
 */
const or$1 = (left, right) => left | right;

/**
 * Counts the number of bits set in n
 * @param {API.Uint32} bitField
 * @returns {Uint8Array}
 */
const toBytes$1 = (bitField) =>
  Uint8Array.of(
    (bitField >> 24) & 0b1111_1111,
    (bitField >> 16) & 0b1111_1111,
    (bitField >> 8) & 0b1111_1111,
    bitField & 0b1111_1111
  );

/**
 *
 * @param {Uint8Array} bytes
 * @returns {API.Uint32}
 */
const fromBytes$2 = (bytes) => {
  if (bytes.length !== 4) {
    throw new Error(`Expected 4 bytes instead got ${bytes.length}`);
  }
  return (bytes[0] << 24) + (bytes[1] << 16) + (bytes[2] << 8) + bytes[3];
};

var Uint32BitField = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  API: api,
  and: and$1,
  bitCount: bitCount,
  empty: empty$1,
  from: from$4,
  fromBytes: fromBytes$2,
  get: get$5,
  or: or$1,
  popcount: popcount$1,
  set: set$3,
  size: size$1,
  toBytes: toBytes$1,
  unset: unset$1,
});

var murmurHash3js = { exports: {} };

/* jshint -W086: true */

var hasRequiredMurmurHash3js;

function requireMurmurHash3js() {
  if (hasRequiredMurmurHash3js) return murmurHash3js.exports;
  hasRequiredMurmurHash3js = 1;
  (function (module, exports) {
    (function (root, undefined$1) {
      // Create a local object that'll be exported or referenced globally.
      var library = {
        version: "3.0.0",
        x86: {},
        x64: {},
        inputValidation: true,
      };

      // PRIVATE FUNCTIONS
      // -----------------

      function _validBytes(bytes) {
        // check the input is an array or a typed array
        if (!Array.isArray(bytes) && !ArrayBuffer.isView(bytes)) {
          return false;
        }

        // check all bytes are actually bytes
        for (var i = 0; i < bytes.length; i++) {
          if (!Number.isInteger(bytes[i]) || bytes[i] < 0 || bytes[i] > 255) {
            return false;
          }
        }
        return true;
      }

      function _x86Multiply(m, n) {
        //
        // Given two 32bit ints, returns the two multiplied together as a
        // 32bit int.
        //

        return (m & 0xffff) * n + ((((m >>> 16) * n) & 0xffff) << 16);
      }

      function _x86Rotl(m, n) {
        //
        // Given a 32bit int and an int representing a number of bit positions,
        // returns the 32bit int rotated left by that number of positions.
        //

        return (m << n) | (m >>> (32 - n));
      }

      function _x86Fmix(h) {
        //
        // Given a block, returns murmurHash3's final x86 mix of that block.
        //

        h ^= h >>> 16;
        h = _x86Multiply(h, 0x85ebca6b);
        h ^= h >>> 13;
        h = _x86Multiply(h, 0xc2b2ae35);
        h ^= h >>> 16;

        return h;
      }

      function _x64Add(m, n) {
        //
        // Given two 64bit ints (as an array of two 32bit ints) returns the two
        // added together as a 64bit int (as an array of two 32bit ints).
        //

        m = [m[0] >>> 16, m[0] & 0xffff, m[1] >>> 16, m[1] & 0xffff];
        n = [n[0] >>> 16, n[0] & 0xffff, n[1] >>> 16, n[1] & 0xffff];
        var o = [0, 0, 0, 0];

        o[3] += m[3] + n[3];
        o[2] += o[3] >>> 16;
        o[3] &= 0xffff;

        o[2] += m[2] + n[2];
        o[1] += o[2] >>> 16;
        o[2] &= 0xffff;

        o[1] += m[1] + n[1];
        o[0] += o[1] >>> 16;
        o[1] &= 0xffff;

        o[0] += m[0] + n[0];
        o[0] &= 0xffff;

        return [(o[0] << 16) | o[1], (o[2] << 16) | o[3]];
      }

      function _x64Multiply(m, n) {
        //
        // Given two 64bit ints (as an array of two 32bit ints) returns the two
        // multiplied together as a 64bit int (as an array of two 32bit ints).
        //

        m = [m[0] >>> 16, m[0] & 0xffff, m[1] >>> 16, m[1] & 0xffff];
        n = [n[0] >>> 16, n[0] & 0xffff, n[1] >>> 16, n[1] & 0xffff];
        var o = [0, 0, 0, 0];

        o[3] += m[3] * n[3];
        o[2] += o[3] >>> 16;
        o[3] &= 0xffff;

        o[2] += m[2] * n[3];
        o[1] += o[2] >>> 16;
        o[2] &= 0xffff;

        o[2] += m[3] * n[2];
        o[1] += o[2] >>> 16;
        o[2] &= 0xffff;

        o[1] += m[1] * n[3];
        o[0] += o[1] >>> 16;
        o[1] &= 0xffff;

        o[1] += m[2] * n[2];
        o[0] += o[1] >>> 16;
        o[1] &= 0xffff;

        o[1] += m[3] * n[1];
        o[0] += o[1] >>> 16;
        o[1] &= 0xffff;

        o[0] += m[0] * n[3] + m[1] * n[2] + m[2] * n[1] + m[3] * n[0];
        o[0] &= 0xffff;

        return [(o[0] << 16) | o[1], (o[2] << 16) | o[3]];
      }

      function _x64Rotl(m, n) {
        //
        // Given a 64bit int (as an array of two 32bit ints) and an int
        // representing a number of bit positions, returns the 64bit int (as an
        // array of two 32bit ints) rotated left by that number of positions.
        //

        n %= 64;

        if (n === 32) {
          return [m[1], m[0]];
        } else if (n < 32) {
          return [
            (m[0] << n) | (m[1] >>> (32 - n)),
            (m[1] << n) | (m[0] >>> (32 - n)),
          ];
        } else {
          n -= 32;
          return [
            (m[1] << n) | (m[0] >>> (32 - n)),
            (m[0] << n) | (m[1] >>> (32 - n)),
          ];
        }
      }

      function _x64LeftShift(m, n) {
        //
        // Given a 64bit int (as an array of two 32bit ints) and an int
        // representing a number of bit positions, returns the 64bit int (as an
        // array of two 32bit ints) shifted left by that number of positions.
        //

        n %= 64;

        if (n === 0) {
          return m;
        } else if (n < 32) {
          return [(m[0] << n) | (m[1] >>> (32 - n)), m[1] << n];
        } else {
          return [m[1] << (n - 32), 0];
        }
      }

      function _x64Xor(m, n) {
        //
        // Given two 64bit ints (as an array of two 32bit ints) returns the two
        // xored together as a 64bit int (as an array of two 32bit ints).
        //

        return [m[0] ^ n[0], m[1] ^ n[1]];
      }

      function _x64Fmix(h) {
        //
        // Given a block, returns murmurHash3's final x64 mix of that block.
        // (`[0, h[0] >>> 1]` is a 33 bit unsigned right shift. This is the
        // only place where we need to right shift 64bit ints.)
        //

        h = _x64Xor(h, [0, h[0] >>> 1]);
        h = _x64Multiply(h, [0xff51afd7, 0xed558ccd]);
        h = _x64Xor(h, [0, h[0] >>> 1]);
        h = _x64Multiply(h, [0xc4ceb9fe, 0x1a85ec53]);
        h = _x64Xor(h, [0, h[0] >>> 1]);

        return h;
      }

      // PUBLIC FUNCTIONS
      // ----------------

      library.x86.hash32 = function (bytes, seed) {
        //
        // Given a string and an optional seed as an int, returns a 32 bit hash
        // using the x86 flavor of MurmurHash3, as an unsigned int.
        //
        if (library.inputValidation && !_validBytes(bytes)) {
          return undefined$1;
        }
        seed = seed || 0;

        var remainder = bytes.length % 4;
        var blocks = bytes.length - remainder;

        var h1 = seed;

        var k1 = 0;

        var c1 = 0xcc9e2d51;
        var c2 = 0x1b873593;

        for (var i = 0; i < blocks; i = i + 4) {
          k1 =
            bytes[i] |
            (bytes[i + 1] << 8) |
            (bytes[i + 2] << 16) |
            (bytes[i + 3] << 24);

          k1 = _x86Multiply(k1, c1);
          k1 = _x86Rotl(k1, 15);
          k1 = _x86Multiply(k1, c2);

          h1 ^= k1;
          h1 = _x86Rotl(h1, 13);
          h1 = _x86Multiply(h1, 5) + 0xe6546b64;
        }

        k1 = 0;

        switch (remainder) {
          case 3:
            k1 ^= bytes[i + 2] << 16;

          case 2:
            k1 ^= bytes[i + 1] << 8;

          case 1:
            k1 ^= bytes[i];
            k1 = _x86Multiply(k1, c1);
            k1 = _x86Rotl(k1, 15);
            k1 = _x86Multiply(k1, c2);
            h1 ^= k1;
        }

        h1 ^= bytes.length;
        h1 = _x86Fmix(h1);

        return h1 >>> 0;
      };

      library.x86.hash128 = function (bytes, seed) {
        //
        // Given a string and an optional seed as an int, returns a 128 bit
        // hash using the x86 flavor of MurmurHash3, as an unsigned hex.
        //
        if (library.inputValidation && !_validBytes(bytes)) {
          return undefined$1;
        }

        seed = seed || 0;
        var remainder = bytes.length % 16;
        var blocks = bytes.length - remainder;

        var h1 = seed;
        var h2 = seed;
        var h3 = seed;
        var h4 = seed;

        var k1 = 0;
        var k2 = 0;
        var k3 = 0;
        var k4 = 0;

        var c1 = 0x239b961b;
        var c2 = 0xab0e9789;
        var c3 = 0x38b34ae5;
        var c4 = 0xa1e38b93;

        for (var i = 0; i < blocks; i = i + 16) {
          k1 =
            bytes[i] |
            (bytes[i + 1] << 8) |
            (bytes[i + 2] << 16) |
            (bytes[i + 3] << 24);
          k2 =
            bytes[i + 4] |
            (bytes[i + 5] << 8) |
            (bytes[i + 6] << 16) |
            (bytes[i + 7] << 24);
          k3 =
            bytes[i + 8] |
            (bytes[i + 9] << 8) |
            (bytes[i + 10] << 16) |
            (bytes[i + 11] << 24);
          k4 =
            bytes[i + 12] |
            (bytes[i + 13] << 8) |
            (bytes[i + 14] << 16) |
            (bytes[i + 15] << 24);

          k1 = _x86Multiply(k1, c1);
          k1 = _x86Rotl(k1, 15);
          k1 = _x86Multiply(k1, c2);
          h1 ^= k1;

          h1 = _x86Rotl(h1, 19);
          h1 += h2;
          h1 = _x86Multiply(h1, 5) + 0x561ccd1b;

          k2 = _x86Multiply(k2, c2);
          k2 = _x86Rotl(k2, 16);
          k2 = _x86Multiply(k2, c3);
          h2 ^= k2;

          h2 = _x86Rotl(h2, 17);
          h2 += h3;
          h2 = _x86Multiply(h2, 5) + 0x0bcaa747;

          k3 = _x86Multiply(k3, c3);
          k3 = _x86Rotl(k3, 17);
          k3 = _x86Multiply(k3, c4);
          h3 ^= k3;

          h3 = _x86Rotl(h3, 15);
          h3 += h4;
          h3 = _x86Multiply(h3, 5) + 0x96cd1c35;

          k4 = _x86Multiply(k4, c4);
          k4 = _x86Rotl(k4, 18);
          k4 = _x86Multiply(k4, c1);
          h4 ^= k4;

          h4 = _x86Rotl(h4, 13);
          h4 += h1;
          h4 = _x86Multiply(h4, 5) + 0x32ac3b17;
        }

        k1 = 0;
        k2 = 0;
        k3 = 0;
        k4 = 0;

        switch (remainder) {
          case 15:
            k4 ^= bytes[i + 14] << 16;

          case 14:
            k4 ^= bytes[i + 13] << 8;

          case 13:
            k4 ^= bytes[i + 12];
            k4 = _x86Multiply(k4, c4);
            k4 = _x86Rotl(k4, 18);
            k4 = _x86Multiply(k4, c1);
            h4 ^= k4;

          case 12:
            k3 ^= bytes[i + 11] << 24;

          case 11:
            k3 ^= bytes[i + 10] << 16;

          case 10:
            k3 ^= bytes[i + 9] << 8;

          case 9:
            k3 ^= bytes[i + 8];
            k3 = _x86Multiply(k3, c3);
            k3 = _x86Rotl(k3, 17);
            k3 = _x86Multiply(k3, c4);
            h3 ^= k3;

          case 8:
            k2 ^= bytes[i + 7] << 24;

          case 7:
            k2 ^= bytes[i + 6] << 16;

          case 6:
            k2 ^= bytes[i + 5] << 8;

          case 5:
            k2 ^= bytes[i + 4];
            k2 = _x86Multiply(k2, c2);
            k2 = _x86Rotl(k2, 16);
            k2 = _x86Multiply(k2, c3);
            h2 ^= k2;

          case 4:
            k1 ^= bytes[i + 3] << 24;

          case 3:
            k1 ^= bytes[i + 2] << 16;

          case 2:
            k1 ^= bytes[i + 1] << 8;

          case 1:
            k1 ^= bytes[i];
            k1 = _x86Multiply(k1, c1);
            k1 = _x86Rotl(k1, 15);
            k1 = _x86Multiply(k1, c2);
            h1 ^= k1;
        }

        h1 ^= bytes.length;
        h2 ^= bytes.length;
        h3 ^= bytes.length;
        h4 ^= bytes.length;

        h1 += h2;
        h1 += h3;
        h1 += h4;
        h2 += h1;
        h3 += h1;
        h4 += h1;

        h1 = _x86Fmix(h1);
        h2 = _x86Fmix(h2);
        h3 = _x86Fmix(h3);
        h4 = _x86Fmix(h4);

        h1 += h2;
        h1 += h3;
        h1 += h4;
        h2 += h1;
        h3 += h1;
        h4 += h1;

        return (
          ("00000000" + (h1 >>> 0).toString(16)).slice(-8) +
          ("00000000" + (h2 >>> 0).toString(16)).slice(-8) +
          ("00000000" + (h3 >>> 0).toString(16)).slice(-8) +
          ("00000000" + (h4 >>> 0).toString(16)).slice(-8)
        );
      };

      library.x64.hash128 = function (bytes, seed) {
        //
        // Given a string and an optional seed as an int, returns a 128 bit
        // hash using the x64 flavor of MurmurHash3, as an unsigned hex.
        //
        if (library.inputValidation && !_validBytes(bytes)) {
          return undefined$1;
        }
        seed = seed || 0;

        var remainder = bytes.length % 16;
        var blocks = bytes.length - remainder;

        var h1 = [0, seed];
        var h2 = [0, seed];

        var k1 = [0, 0];
        var k2 = [0, 0];

        var c1 = [0x87c37b91, 0x114253d5];
        var c2 = [0x4cf5ad43, 0x2745937f];

        for (var i = 0; i < blocks; i = i + 16) {
          k1 = [
            bytes[i + 4] |
              (bytes[i + 5] << 8) |
              (bytes[i + 6] << 16) |
              (bytes[i + 7] << 24),
            bytes[i] |
              (bytes[i + 1] << 8) |
              (bytes[i + 2] << 16) |
              (bytes[i + 3] << 24),
          ];
          k2 = [
            bytes[i + 12] |
              (bytes[i + 13] << 8) |
              (bytes[i + 14] << 16) |
              (bytes[i + 15] << 24),
            bytes[i + 8] |
              (bytes[i + 9] << 8) |
              (bytes[i + 10] << 16) |
              (bytes[i + 11] << 24),
          ];

          k1 = _x64Multiply(k1, c1);
          k1 = _x64Rotl(k1, 31);
          k1 = _x64Multiply(k1, c2);
          h1 = _x64Xor(h1, k1);

          h1 = _x64Rotl(h1, 27);
          h1 = _x64Add(h1, h2);
          h1 = _x64Add(_x64Multiply(h1, [0, 5]), [0, 0x52dce729]);

          k2 = _x64Multiply(k2, c2);
          k2 = _x64Rotl(k2, 33);
          k2 = _x64Multiply(k2, c1);
          h2 = _x64Xor(h2, k2);

          h2 = _x64Rotl(h2, 31);
          h2 = _x64Add(h2, h1);
          h2 = _x64Add(_x64Multiply(h2, [0, 5]), [0, 0x38495ab5]);
        }

        k1 = [0, 0];
        k2 = [0, 0];

        switch (remainder) {
          case 15:
            k2 = _x64Xor(k2, _x64LeftShift([0, bytes[i + 14]], 48));

          case 14:
            k2 = _x64Xor(k2, _x64LeftShift([0, bytes[i + 13]], 40));

          case 13:
            k2 = _x64Xor(k2, _x64LeftShift([0, bytes[i + 12]], 32));

          case 12:
            k2 = _x64Xor(k2, _x64LeftShift([0, bytes[i + 11]], 24));

          case 11:
            k2 = _x64Xor(k2, _x64LeftShift([0, bytes[i + 10]], 16));

          case 10:
            k2 = _x64Xor(k2, _x64LeftShift([0, bytes[i + 9]], 8));

          case 9:
            k2 = _x64Xor(k2, [0, bytes[i + 8]]);
            k2 = _x64Multiply(k2, c2);
            k2 = _x64Rotl(k2, 33);
            k2 = _x64Multiply(k2, c1);
            h2 = _x64Xor(h2, k2);

          case 8:
            k1 = _x64Xor(k1, _x64LeftShift([0, bytes[i + 7]], 56));

          case 7:
            k1 = _x64Xor(k1, _x64LeftShift([0, bytes[i + 6]], 48));

          case 6:
            k1 = _x64Xor(k1, _x64LeftShift([0, bytes[i + 5]], 40));

          case 5:
            k1 = _x64Xor(k1, _x64LeftShift([0, bytes[i + 4]], 32));

          case 4:
            k1 = _x64Xor(k1, _x64LeftShift([0, bytes[i + 3]], 24));

          case 3:
            k1 = _x64Xor(k1, _x64LeftShift([0, bytes[i + 2]], 16));

          case 2:
            k1 = _x64Xor(k1, _x64LeftShift([0, bytes[i + 1]], 8));

          case 1:
            k1 = _x64Xor(k1, [0, bytes[i]]);
            k1 = _x64Multiply(k1, c1);
            k1 = _x64Rotl(k1, 31);
            k1 = _x64Multiply(k1, c2);
            h1 = _x64Xor(h1, k1);
        }

        h1 = _x64Xor(h1, [0, bytes.length]);
        h2 = _x64Xor(h2, [0, bytes.length]);

        h1 = _x64Add(h1, h2);
        h2 = _x64Add(h2, h1);

        h1 = _x64Fmix(h1);
        h2 = _x64Fmix(h2);

        h1 = _x64Add(h1, h2);
        h2 = _x64Add(h2, h1);

        return (
          ("00000000" + (h1[0] >>> 0).toString(16)).slice(-8) +
          ("00000000" + (h1[1] >>> 0).toString(16)).slice(-8) +
          ("00000000" + (h2[0] >>> 0).toString(16)).slice(-8) +
          ("00000000" + (h2[1] >>> 0).toString(16)).slice(-8)
        );
      };

      // INITIALIZATION
      // --------------

      // Export murmurHash3 for CommonJS, either as an AMD module or just as part
      // of the global object.
      {
        if (module.exports) {
          exports = module.exports = library;
        }

        exports.murmurHash3 = library;
      }
    })();
  })(murmurHash3js, murmurHash3js.exports);
  return murmurHash3js.exports;
}

var murmurhash3jsRevisited;
var hasRequiredMurmurhash3jsRevisited;

function requireMurmurhash3jsRevisited() {
  if (hasRequiredMurmurhash3jsRevisited) return murmurhash3jsRevisited;
  hasRequiredMurmurhash3jsRevisited = 1;
  murmurhash3jsRevisited = requireMurmurHash3js();
  return murmurhash3jsRevisited;
}

var murmurhash3jsRevisitedExports = requireMurmurhash3jsRevisited();
var mur = /*@__PURE__*/ getDefaultExportFromCjs(murmurhash3jsRevisitedExports);

const utf8$2 = new TextEncoder();

/**
 * @typedef {(bytes:Uint8Array) => API.Uint32} Hasher
 * @type {Hasher}
 */
const hash32 = mur.x64.hash126;

/**
 * @param {Partial<API.Options<API.Uint32>>} options
 * @returns {API.Path<API.Uint32>}
 */
/* c8 ignore next 45 */
const configure$3 = ({ bitWidth = 5, hash = hash32 }) => {
  const hashSize = 4;
  if (bitWidth > hashSize * 8) {
    throw new RangeError(
      `Can not use bitWidth ${bitWidth} which exceeds the hashSize ${hashSize}`
    );
  }

  // Mask for reading `bitWidth` number of bits from the end.
  const mask = 0xffffffff >>> (32 - bitWidth);

  /**
   * Determines bit position for the path entry at the given `depth`.
   * ```js
   * const key = hash("result") // 0b00011010010110010101111100110010
   * // Which is following path (in reverse as we read from the right)
   * // 10010/11001/10111/10010/00101/01101/00000 -> [ 18, 25, 23, 18, 5, 13, 0 ]
   * at(key, 0) // 0b10010 -> 18
   * at(key, 1) // 0b11001 -> 25
   * at(key, 2) // 0b10111 -> 23
   * at(key, 3) // 0b10010 -> 18
   * at(key, 4) // 0b00101 -> 5
   * at(key, 5) // 0b01101 -> 13
   * at(key, 6) // 0b00000 -> 0
   * ```
   *
   * @param {API.Uint32} path
   * @param {number} depth
   */
  const at = (path, depth) => (path >>> (depth * bitWidth)) & mask;

  /**
   * @param {string} key
   * @returns {API.Uint32}
   */
  const from = (key) => hash(utf8$2.encode(key));

  return { at, from, size: Math.ceil((hashSize * 8) / bitWidth) };
};

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @implements {API.BitmapIndexedNode<T, K, C>}
 */
class BitmapIndexedNode {
  /**
   * @param {API.Edit|null} edit
   * @param {ReturnType<C['BitField']['empty']>} datamap
   * @param {ReturnType<C['BitField']['empty']>} nodemap
   * @param {API.Children<T, K, C>} children
   * @param {C} config
   */
  constructor(edit, datamap, nodemap, children, config) {
    this.edit = edit;
    this.config = config;
    this.datamap = datamap;
    this.nodemap = nodemap;
    this.children = children;
  }

  get nodeArity() {
    return this.config.BitField.popcount(this.nodemap);
  }
  get dataArity() {
    return this.config.BitField.popcount(this.datamap);
  }

  /**
   * @returns {API.BitmapIndexedNode<T, K, C>}
   */
  /* c8 ignore next 3 */
  empty() {
    return create$5(this.config);
  }

  /**
   * @template X
   * @param {API.Uint32} depth
   * @param {ReturnType<C['Path']['from']>} path
   * @param {K} key
   * @param {X} notFound
   * @returns {T|X}
   */

  lookup(depth, path, key, notFound) {
    return lookup(this, depth, path, key, notFound);
  }

  /**
   * @template {string} R
   * @param {API.Edit|null} edit
   * @param {API.Uint32} depth
   * @param {ReturnType<C['Path']['from']>} path
   * @param {K|R} key
   * @param {T} value
   * @param {{value:boolean}} addedLeaf
   * @returns {API.BitmapIndexedNode<T, K | R, C>}
   */
  associate(edit, depth, path, key, value, addedLeaf) {
    return associate(this, edit, depth, path, key, value, addedLeaf);
  }

  /**
   * @param {API.Edit|null} edit
   * @param {API.Uint32} depth
   * @param {ReturnType<C['Path']['from']>} path
   * @param {K} key
   * @param {{value:boolean}} removedLeaf
   * @returns {API.BitmapIndexedNode<T, K, C>}
   */
  dissociate(edit, depth, path, key, removedLeaf) {
    return dissociate(this, edit, depth, path, key, removedLeaf);
  }

  /**
   * @param {API.Edit|null} edit
   * @returns {API.BitmapIndexedNode<T, K, C>}
   */
  fork(edit = null) {
    return fork$1(this, edit);
  }

  /**
   * @returns {IterableIterator<[K, T]>}
   */
  entries() {
    return entries(this);
  }

  /**
   * @returns {IterableIterator<K>}
   */
  keys() {
    return keys(this);
  }

  /**
   * @returns {IterableIterator<T>}
   */
  values() {
    return values(this);
  }
}

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @implements {API.HashCollisionNode<T, K, C>}
 */
class HashCollisionNode {
  /**
   * @param {API.Edit|null} edit
   * @param {number} count
   * @param {API.CollisionEntries<T, K>} children
   * @param {C} config
   */
  /* c8 ignore next 12 */
  constructor(edit, count, children, config) {
    this.edit = edit;
    this.count = count;
    this.children = children;
    this.config = config;
  }
  get nodeArity() {
    return /** @type {0} */ (0);
  }
  get dataArity() {
    return this.count;
  }

  /**
   * @template X
   * @param {API.Uint32} _shift
   * @param {unknown} _path
   * @param {K} key
   * @param {X} notFound
   * @returns {T|X}
   */
  /* c8 ignore next 3 */
  lookup(_shift, _path, key, notFound) {
    return lookupCollision(this, key, notFound);
  }

  /**
   * @template {string} R
   * @param {API.Edit|null} edit
   * @param {API.Uint32} _shift
   * @param {ReturnType<C['Path']['from']>} path
   * @param {K|R} key
   * @param {T} value
   * @param {{value:boolean}} addedLeaf
   * @returns {API.HashCollisionNode<T, K | R, C>}
   */
  /* c8 ignore next 3 */
  associate(edit, _shift, path, key, value, addedLeaf) {
    return associateCollision(this, edit, path, key, value, addedLeaf);
  }

  /**
   * @param {API.Edit|null} edit
   * @param {API.Uint32} _shift
   * @param {ReturnType<C['Path']['from']>} path
   * @param {K} key
   * @param {{value:boolean}} removedLeaf
   * @returns {API.Node<T, K, C>}
   */
  /* c8 ignore next 3 */
  dissociate(edit, _shift, path, key, removedLeaf) {
    return dissociateCollision(this, edit, path, key, removedLeaf);
  }

  /**
   * @param {API.Edit|null} edit
   * @returns {this}
   */
  /* c8 ignore next 3 */
  fork(edit = null) {
    return /** @type {this} */ (forkCollision(this, edit));
  }

  /**
   * @returns {IterableIterator<[K, T]>}
   */
  /* c8 ignore next 3 */
  entries() {
    return entries(this);
  }

  /**
   * @returns {IterableIterator<K>}
   */
  /* c8 ignore next 3 */
  keys() {
    return keys(this);
  }

  /**
   * @returns {IterableIterator<T>}
   */
  /* c8 ignore next 3 */
  values() {
    return values(this);
  }
}

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @template X
 * @param {API.HashCollisionNode<T, K, C>} node
 * @param {K} name
 * @param {X} notFound
 * @returns {T|X}
 */
/* c8 ignore next 8 */
const lookupCollision = (node, name, notFound) => {
  const { children: entries, count } = node;
  // find where entry with this name belongs
  const n = findHashCollisionNodeIndex(entries, count, name);
  // if entry name at this index matches given name return the value otherwise
  // return `notFound` as we have no such entry.
  return entries[n] === name ? /** @type {T} */ (entries[n + 1]) : notFound;
};

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @template {string} R
 * @param {API.HashCollisionNode<T, K, C>} node
 * @param {API.Edit|null} edit
 * @param {ReturnType<C['Path']['from']>} key
 * @param {K|R} name
 * @param {T} value
 * @param {{value:boolean}} addedLeaf
 * @returns {API.HashCollisionNode<T, K | R, C>}
 */
/* c8 ignore next 26 */
const associateCollision = (node, edit, key, name, value, addedLeaf) => {
  const { children, count } = node;

  const index = findHashCollisionNodeIndex(children, count, name);
  // If entry at this index has a different name we fork the node and
  // add a new entry.
  if (children[index] !== name) {
    const newNode = node.fork(edit);
    addedLeaf.value = true;
    newNode.count += 1;
    newNode.children.splice(index, key, value);
    return newNode;
  }
  // If name is the same but value is not we fork the node and update
  // the value
  else if (children[index + 1] !== value) {
    const newNode = node.fork(edit);
    newNode.children[index + 1] = value;
    return newNode;
  }
  // If we got this far entry with this exact name and value is already
  // present making this a noop, so we return this node back.
  else {
    return node;
  }
};

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {API.HashCollisionNode<T, K, C>} node
 * @param {API.Edit|null} edit
 * @param {ReturnType<C['Path']['from']>} hash
 * @param {K} name
 * @param {{value:boolean}} removedLeaf
 * @returns {API.Node<T, K, C>}
 */
/* c8 ignore next 37 */
const dissociateCollision = (node, edit, hash, name, removedLeaf) => {
  const { children: entries, count, config } = node;
  const index = findHashCollisionNodeIndex(entries, count, name);
  // If there is no entry with a the given name this is noop so we just
  // return back this node.
  if (entries[index] !== name) {
    return node;
  } else {
    removedLeaf.value = true;
    // If conflict contained only two entries removing one of them would
    // leave us with no conflict which is why we create a new node with a
    // an entry other than one that would correspond provided name
    if (count === 2) {
      const offset = index === 0 ? 2 : 0;
      return /** @type {API.BitmapIndexedNode<T, K, C>} */ (
        associate(
          create$5(config),
          edit,
          0,
          hash,
          /** @type {K} */ (entries[offset]),
          /** @type {T} */ (entries[offset + 1]),
          removedLeaf
        )
      );
    }
    // otherwise we got this far we have more than two colliding entries in
    // which case we simply remove one corresponding to given `name`.
    //
    else {
      const newNode = node.fork(edit);
      newNode.children.splice(index, 2);
      newNode.count -= 1;
      return newNode;
    }
  }
};

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {API.HashCollisionNode<T, K, C>} node
 * @param {API.Edit|null} edit
 * @returns {API.HashCollisionNode<T, K, C>}
 */
/* c8 ignore next 12 */
const forkCollision = (node, edit = null) => {
  if (canEdit(node.edit, edit)) {
    return node;
  } else {
    return new HashCollisionNode(
      edit,
      node.count,
      /** @type {API.CollisionEntries<T, K>} */ (node.children.slice()),
      node.config
    );
  }
};

/**
 * Finds the index inside collision entries where given `key` belongs, which is
 * index where `key <= entries[index]` is `true`. If no index satisfies this
 * constraint index will be `entries.length` indicating that key belongs in the
 * last position.
 *
 * @template T
 * @template {string} K
 * @param {API.CollisionEntries<T, K>} entries
 * @param {number} count
 * @param {K} key
 */
/* c8 ignore next 8 */
const findHashCollisionNodeIndex = (entries, count, key) => {
  let index = 0;
  // increase index until we find a index where key <= entries[index]
  while (index < count && entries[index] > key) {
    index += 2;
  }
  return index;
};
/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {API.Edit|null} [edit]
 * @param {C} config
 * @returns {API.BitmapIndexedNode<T, K, C>}
 */
const create$5 = (config, edit = null) =>
  new BitmapIndexedNode(
    edit,
    config.BitField.empty(Math.pow(2, config.bitWidth)),
    config.BitField.empty(Math.pow(2, config.bitWidth)),
    /** @type {API.Children<T, K, C>} */ ([]),
    config
  );

/**
 * @template T, U
 * @template {string} K
 * @param {API.BitmapIndexedNode<T, K>} node
 * @param {K} key
 * @param {U} notFound
 */
const get$4 = (node, key, notFound) =>
  lookup(node, 0, node.config.Path.from(key), key, notFound);

/**
 * @template T, U
 * @template {string} K
 * @template Bits, BitMap
 * @param {API.BitmapIndexedNode<T, K, API.Config<Bits, BitMap>>} node
 * @param {API.Uint32} depth
 * @param {Bits} path
 * @param {K} key
 * @param {U} notFound
 * @returns {T|U}
 */
const lookup = (node, depth, path, key, notFound) => {
  const { datamap, nodemap, config } = node;
  const { Path, BitField } = config;
  const offset = Path.at(path, depth);

  // If bit is set in the data bitmap we have some key, value under the
  // matching hash segment.
  if (BitField.get(datamap, offset)) {
    const index = BitField.popcount(datamap, offset);
    // If key matches actual key in the map we found the the value
    // otherwise we did not.
    if (keyAt(node, index) === key) {
      return valueAt(node, index);
    } else {
      return notFound;
    }
  }
  // If bit is set in the node bitmapt we have a node under the
  // matching hash segment.
  else if (BitField.get(nodemap, offset)) {
    // Resolve node and continue lookup within it.
    const child = resolveNode(node, offset);
    return child.lookup(depth + 1, path, key, notFound);
  }
  // If we have neither node nor key-pair for this hash segment
  // we return notFound.
  else {
    return notFound;
  }
};

/**
 * @template T, U
 * @template {string} K
 * @template {string} R
 * @template {API.Config} C
 * @param {API.BitmapIndexedNode<T, K, C>} node
 * @param {API.Edit|null} edit
 * @param {R} key
 * @param {T} value
 * @param {{ value: boolean }} addedLeaf
 * @returns {API.BitmapIndexedNode<T, K|R, C>}
 */
const set$2 = (node, edit, key, value, addedLeaf) =>
  associate(node, edit, 0, node.config.Path.from(key), key, value, addedLeaf);

/**
 * @template T
 * @template {string} K
 * @template {string} R
 * @template {API.Config} C
 * @param {API.BitmapIndexedNode<T, K, C>} node
 * @param {API.Edit|null} edit
 * @param {API.Uint32} depth
 * @param {ReturnType<C['Path']['from']>} path
 * @param {K|R} key
 * @param {T} value
 * @param {{value:boolean}} addedLeaf
 * @returns {API.BitmapIndexedNode<T, K | R, C>}
 */
const associate = (node, edit, depth, path, key, value, addedLeaf) => {
  const { datamap, nodemap, config } = node;
  const { Path, BitField } = config;
  const offset = Path.at(path, depth);
  // If bit is set in the data bitmap we have some key, value under the
  // matching hash segment.
  if (BitField.get(datamap, offset)) {
    const index = BitField.popcount(datamap, offset);
    const found = keyAt(node, index);
    // If we have entry with given name and value is the same return node
    // as is, otherwise fork node and set the value.
    if (key === found) {
      return valueAt(node, index) === value
        ? node
        : forkAndSet(node, edit, index, value);
    }
    // Otherwise we need to create a branch to contain current key, value and
    // one been passed.
    else {
      const branch = mergeTwoLeaves(
        config,
        edit,
        depth + 1,
        Path.from(found),
        found,
        valueAt(node, index),
        path,
        key,
        value
      );
      addedLeaf.value = true;

      return migrateLeafToBranch(node, edit, offset, branch);
    }
  }
  // If bit is set in the node bitmap we have a branch under the current
  // hash slice.
  else if (BitField.get(nodemap, offset)) {
    const child = resolveNode(node, offset);
    const newChild = child.associate(
      edit,
      depth + 1,
      path,
      key,
      value,
      addedLeaf
    );

    if (child === newChild) {
      return node;
    } else {
      return copyAndSetChild(node, edit, offset, newChild);
    }
  }
  // If we have neither node nor a key-value for this hash segment. We copy
  // current children and add new key-value pair
  else {
    const index = BitField.popcount(datamap, offset);
    addedLeaf.value = true;

    /** @type {API.BitmapIndexedNode<T, K|R, C>} */
    const newNode = node.fork(edit);

    // Capture new entry in the data bitmap
    newNode.datamap = BitField.set(datamap, offset);
    newNode.children.splice(keyPosition(index), 0, key, value);
    return newNode;
  }
};

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {API.BitmapIndexedNode<T, K, C>} node
 * @param {API.Edit|null} edit
 * @param {K} key
 * @param {{ value: boolean }} removedLeaf
 * @returns {API.BitmapIndexedNode<T, K, C>}
 */
const remove = (node, edit, key, removedLeaf) =>
  dissociate(node, edit, 0, node.config.Path.from(key), key, removedLeaf);

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {API.BitmapIndexedNode<T, K, C>} source
 * @param {API.Edit|null} edit
 * @param {API.Uint32} depth
 * @param {ReturnType<C['Path']['from']>} path
 * @param {K} key
 * @param {{value:boolean}} removedLeaf
 * @returns {API.BitmapIndexedNode<T, K, C>}
 */
const dissociate = (source, edit, depth, path, key, removedLeaf) => {
  const { datamap, nodemap, config } = source;
  const { BitField, Path } = config;
  const offset = Path.at(path, depth);
  // If bit is set in the data bitmap we have an entry under the
  // matching hash segment.
  if (BitField.get(datamap, offset)) {
    const index = BitField.popcount(datamap, offset);
    // If key at a given index matches given `name` we fork a node and remove
    // the entry
    if (key === keyAt(source, index)) {
      removedLeaf.value = true;
      const node = fork$1(source, edit);
      // Update the bitmap
      node.datamap = BitField.unset(source.datamap, offset);
      // remove the child
      node.children.splice(keyPosition(index), 2);
      return node;
    }
    // otherwise we don't have such entry so we return node back as is.
    else {
      return source;
    }
  }
  // If bit is set in the node bitmapt we have a node under the
  // matching hash segment.
  else if (BitField.get(nodemap, offset)) {
    const node = resolveNode(source, offset);
    const child = node.dissociate(edit, depth + 1, path, key, removedLeaf);
    // if child has a single element we need to canonicalize
    if (hasSingleLeaf(child)) {
      // if source has a single child, we collapse and return the child
      // otherwise we inline the child.
      return hasSingleNode(source)
        ? child
        : inlineChild(source, edit, offset, child);
    } else if (node === child) {
      return source;
    } else {
      return copyAndSetChild(source, edit, offset, child);
    }
  }
  // If we have neither node nor a key-value for this hash segment this is a
  // noop.
  else {
    return source;
  }
};

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {API.Node<T, K, C>} node
 * @returns {IterableIterator<[K, T]>}
 */
const entries = function* ({ children }) {
  let offset = 0;
  const count = children.length;
  while (offset < count) {
    const key = children[offset];
    if (typeof key === "string") {
      offset += 1;
      const value = children[offset];
      yield /** @type {[K, T]} */ ([key, value]);
      offset += 1;
    } else {
      break;
    }
  }

  while (offset < count) {
    const node = /** @type {API.BitmapIndexedNode<T, K, C>} */ (
      children[offset]
    );
    yield* node.entries();
    offset += 1;
  }
};

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {API.BitmapIndexedNode<T, K, C>} node
 * @param {API.Edit|null} edit
 * @returns {API.BitmapIndexedNode<T, K, C>}
 */
const fork$1 = (node, edit) => {
  if (canEdit(node.edit, edit)) {
    return node;
  } else {
    const newNode = new BitmapIndexedNode(
      edit,
      node.datamap,
      node.nodemap,
      node.children.slice(),
      node.config
    );
    return newNode;
  }
};

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {API.Node<T, K, C>} node
 * @returns {IterableIterator<K>}
 */
const keys = function* ({ children }) {
  let offset = 0;
  const count = children.length;
  while (offset < count) {
    const key = children[offset];
    if (typeof key === "string") {
      yield /** @type {K} */ (key);
      offset += 2;
    } else {
      break;
    }
  }

  while (offset < count) {
    const node = /** @type {API.BitmapIndexedNode<T, K>} */ (children[offset]);
    yield* node.keys();
    offset += 1;
  }
};

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {API.Node<T, K, C>} node
 * @returns {IterableIterator<T>}
 */
const values = function* ({ children }) {
  let offset = 0;
  const count = children.length;
  while (offset < count) {
    const key = children[offset];
    if (typeof key === "string") {
      offset += 1;
      yield /** @type {T} */ (children[offset]);
      offset += 1;
    } else {
      break;
    }
  }

  while (offset < count) {
    const node = /** @type {API.BitmapIndexedNode<T, K>} */ (children[offset]);
    yield* node.values();
    offset += 1;
  }
};

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {API.BitmapIndexedNode<T, K, C>} node
 * @param {API.Edit|null} edit
 * @param {number} offset
 * @param {T} value
 */
const forkAndSet = (node, edit, offset, value) => {
  const newNode = node.fork(edit);
  newNode.children[valuePosition(offset)] = value;
  return newNode;
};

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {API.BitmapIndexedNode<T, K, C>} source
 * @param {API.Edit|null} edit
 * @param {number} offset
 * @param {API.Node<T, K, C>} child
 * @returns {API.BitmapIndexedNode<T, K, C>}
 */
const inlineChild = (source, edit, offset, child) => {
  const { datamap, nodemap, config } = source;
  const { BitField } = config;
  const node = fork$1(source, edit);

  // remove the node that we are inlining
  node.children.splice(nodePosition(source, offset), 1);
  // add key-value pair where it wolud fall
  node.children.splice(
    keyPosition(BitField.popcount(datamap, offset)),
    0,
    child.children[0],
    child.children[1]
  );

  node.datamap = BitField.set(datamap, offset);
  node.nodemap = BitField.unset(nodemap, offset);

  return node;
};

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {API.BitmapIndexedNode<T, K, C>} node
 * @param {API.Edit|null} edit
 * @param {number} offset
 * @param {API.Node<T, K, C>} child
 * @returns {API.BitmapIndexedNode<T, K, C>}
 */
const copyAndSetChild = (node, edit, offset, child) => {
  const newNode = fork$1(node, edit);
  newNode.children[nodePosition(node, offset)] = child;
  return newNode;
};

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {API.BitmapIndexedNode<T, K, C>} source
 * @param {API.Edit|null} edit
 * @param {number} offset
 * @param {API.Node<T, K, C>} branch
 * @returns {API.BitmapIndexedNode<T, K, C>}
 */
const migrateLeafToBranch = (source, edit, offset, branch) => {
  const { nodemap, datamap, config } = source;
  const { BitField } = config;
  const index = BitField.popcount(datamap, offset);
  // Previous id corresponds to the key position
  const oldId = keyPosition(index);
  const newId = nodePosition(source, offset);

  const node = fork$1(source, edit);

  // remove an old leaf
  node.datamap = BitField.unset(datamap, offset);
  node.children.splice(oldId, 2);

  // add a new branch
  node.nodemap = BitField.set(nodemap, offset);
  node.children.splice(newId - 1, 0, branch);

  return node;
};

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {C} config
 * @param {API.Edit|null} edit
 * @param {number} depth
 * @param {ReturnType<C['Path']['from']>} oldPath
 * @param {K} oldKey
 * @param {T} oldValue
 * @param {ReturnType<C['Path']['from']>} newPath
 * @param {K} newKey
 * @param {T} newValue
 * @returns {API.Node<T, K, C>}
 */
const mergeTwoLeaves = (
  config,
  edit,
  depth,
  oldPath,
  oldKey,
  oldValue,
  newPath,
  newKey,
  newValue
) => {
  const { BitField, Path } = config;
  // If we have reached end of the path we can no longer create another
  // `BitmapIndexedNode`, instead we create a node containing (hash) colliding
  // entries
  /* c8 ignore next 7 */
  if (Path.size < depth) {
    return new HashCollisionNode(
      edit,
      2,
      [oldKey, oldValue, newKey, newValue],
      config
    );
  } else {
    const oldOffset = Path.at(oldPath, depth);
    const newOffset = Path.at(newPath, depth);
    // If offsets still match create another intermediery node and merge these
    // two nodes at next depth level.
    if (oldOffset === newOffset) {
      return new BitmapIndexedNode(
        edit,
        BitField.empty(Math.pow(2, config.bitWidth)),
        BitField.from([oldOffset], Math.pow(2, config.bitWidth)),
        [
          mergeTwoLeaves(
            config,
            edit,
            depth + 1,
            oldPath,
            oldKey,
            oldValue,
            newPath,
            newKey,
            newValue
          ),
        ],
        config
      );
    }
    // otherwise create new node with both key-value pairs as it's children
    else {
      return new BitmapIndexedNode(
        edit,
        BitField.from([oldOffset, newOffset], Math.pow(2, config.bitWidth)),
        BitField.empty(Math.pow(2, config.bitWidth)),
        /** @type {API.Children<T, K, C>} */
        (
          // We insert child with a lower index first so that we can derive it's
          // index on access via popcount
          oldOffset < newOffset
            ? [oldKey, oldValue, newKey, newValue]
            : [newKey, newValue, oldKey, oldValue]
        ),
        config
      );
    }
  }
};

/**
 * @template {string} K
 * @param {API.BitmapIndexedNode<unknown, K>} node
 * @param {number} index
 */
const keyAt = ({ children }, index) =>
  /** @type {K} */ (children[keyPosition(index)]);

/**
 * @param {number} index
 */
const keyPosition = (index) => index * 2;

/**
 * @template T
 * @param {API.BitmapIndexedNode<T>} node
 * @param {number} index
 */
const valueAt = ({ children }, index) =>
  /** @type {T} */ (children[valuePosition(index)]);

/**
 * @param {number} index
 */
const valuePosition = (index) => index * 2 + 1;

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {API.BitmapIndexedNode<T, K, C>} node
 * @param {number} offset
 * @returns {API.BitmapIndexedNode<T, K, C>|API.HashCollisionNode<T, K, C>}
 */
const resolveNode = (node, offset) =>
  /** @type {API.BitmapIndexedNode<T, K, C>|API.HashCollisionNode<T, K, C>} */ (
    node.children[nodePosition(node, offset)]
  );

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {API.BitmapIndexedNode<T, K, C>} node
 * @param {number} offset
 */
const nodePosition = ({ children, nodemap, config }, offset) =>
  children.length - 1 - config.BitField.popcount(nodemap, offset);

/**
 * @param {API.Edit|null} owner
 * @param {API.Edit|null} editor
 */
const canEdit = (owner, editor) => owner != null && owner === editor;

/**
 * Returns `true` if node has a single entry. It also refines type to
 * `BitmapIndexedNode` because `HashCollisionNode` is normalized to
 * `BitmapIndexedNode` when it contains only a single entry.
 *
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {API.Node<T, K, C>} node
 * @returns {node is API.BitmapIndexedNode<T, K, C>}
 */
const hasSingleLeaf = (node) => node.nodeArity === 0 && node.dataArity === 1;

/**
 * Returns `true` if node has a single childe node and 0 child leaves.
 *
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @param {API.BitmapIndexedNode<T, K, C>} node
 * @returns {node is API.BitmapIndexedNode<T, K, C>}
 */
const hasSingleNode = ({ config: { BitField }, datamap, nodemap }) =>
  BitField.popcount(datamap) === 0 && BitField.popcount(nodemap) === 1;

// A special-use 0x22 that truncates 64 bits, specifically for use in the UnixFS HAMT
const murmur364 = from$f({
  name: "murmur3-x64-64",
  code: 0x22,
  encode: (input) => fromHex(mur.x64.hash128(input)).subarray(0, 8),
});

const utf8$1 = new TextEncoder();

/**
 * @param {Uint8Array} bytes
 */
/* c8 ignore next 2 */
const hash64$1 = (bytes) => /** @type {Uint8Array} */ (murmur364.encode(bytes));

/**
 * @param {Partial<API.Options<Uint8Array>>} options
 * @returns {API.Path<Uint8Array>}
 */
/* c8 ignore next 25 */
const configure$2 = ({ bitWidth = 8, hash = hash64$1 } = {}) => {
  const hashSize = hash(new Uint8Array()).byteLength;

  /**
   * @param {Uint8Array} path
   * @param {number} depth
   * @returns {API.Uint32}
   */
  const at = (path, depth) => {
    const offset = depth * bitWidth;
    if (offset > hashSize) {
      throw new RangeError(`Out of bounds`);
    }

    return toInt(path, offset, bitWidth);
  };

  /**
   * @param {string} key
   * @returns {Uint8Array}
   */
  const from = (key) => hash(utf8$1.encode(key));

  return { from, at, size: Math.ceil((hashSize * 8) / bitWidth) };
};

/**
 * @param {Uint8Array} bytes
 * @param {number} offset - bit offset
 * @param {number} count - number of bits to consume
 */
const toInt = (bytes, offset, count) => {
  let byteOffset = (offset / 8) | 0;
  let bitOffset = offset % 8;
  let desired = count;
  let bits = 0;
  while (desired > 0 && byteOffset < bytes.byteLength) {
    const byte = bytes[byteOffset];
    const available = 8 - bitOffset;

    const taking = available < desired ? available : desired;
    const bitsLeft = 8 - bitOffset - taking;
    // mask to turn of bits before bitOffset
    const mask = 0xff >> bitOffset;
    // turn off offset bits and shift to drop remaining bit on the right
    const value = (mask & byte) >> bitsLeft;
    bits = (bits << taking) + value;

    desired -= taking;
    byteOffset++;
    bitOffset = 0;
  }

  return bits;
};

/**
 * @param {number} size
 */
const empty = (size = 256) => {
  if (size % 8 !== 0) {
    throw new Error(`Must be multiple of 8`);
  }

  return new Uint8Array(size / 8);
};

/**
 * Creates bitfield with specific bits set.
 *
 * @param {number[]} bits
 * @param {number} [size]
 * @returns {Uint8Array}
 */
const from$3 = (bits, size) => {
  let bitfield = empty(size);
  for (const index of bits) {
    const { byte, byteOffset, bitOffset } = at(bitfield, index);
    bitfield[byteOffset] = byte | (1 << bitOffset);
  }
  return bitfield;
};

/**
 * @param {Uint8Array} bitfield
 */
const size = (bitfield) => bitfield.byteLength * 8;

/**
 * Compute offset for the given index
 *
 * @param {Uint8Array} bitfield
 * @param {number} index
 */
const at = (bitfield, index) => {
  const byteOffset = bitfield.byteLength - 1 - ((index / 8) | 0);
  const bitOffset = index % 8;
  const byte = bitfield[byteOffset];

  return { byte, byteOffset, bitOffset };
};

/**
 * Set a particular bit.
 *
 * @param {Uint8Array} bytes
 * @param {number} index
 * @param {number} byte
 * @returns {Uint8Array}
 */
const setByte = (bytes, index, byte) => {
  if (bytes[index] !== byte) {
    const result = bytes.slice(0);
    result[index] = byte;
    return result;
  }
  return bytes;
};

/**
 * Set a particular bit.
 *
 * @param {Uint8Array} bitfield
 * @param {number} index
 * @returns {Uint8Array}
 */
const set$1 = (bitfield, index) => {
  const { byte, byteOffset, bitOffset } = at(bitfield, index);
  return setByte(bitfield, byteOffset, byte | (1 << bitOffset));
};

/**
 * Unsets a particular bit.

 * @param {Uint8Array} bitfield
 * @param {number} index
 * @returns {Uint8Array}
 */
const unset = (bitfield, index) => {
  const { byte, byteOffset, bitOffset } = at(bitfield, index);
  return setByte(bitfield, byteOffset, byte & (0xff ^ (1 << bitOffset)));
};

/**
 * Returns `true` if bit at given index is set.
 *
 * @param {Uint8Array} bitfield
 * @param {number} index
 */
const get$3 = (bitfield, index) => {
  var { byte, bitOffset } = at(bitfield, index);
  return ((byte >> bitOffset) & 0x1) !== 0;
};

/**
 * @param {Uint8Array} bitfield
 */
const toBytes = (bitfield) => bitfield;

/**
 * @param {Uint8Array} bytes
 */
const fromBytes$1 = (bytes) => bytes;

/**
 * @param {Uint8Array} bitfield
 * @param {number} index
 */
const popcount = (bitfield, index = bitfield.byteLength * 8) => {
  const { byteOffset, bitOffset, byte } = at(bitfield, index);

  let count = popcount$1(byte, bitOffset);
  let offset = bitfield.byteLength - 1;
  while (offset > byteOffset) {
    const byte = bitfield[offset];
    count += bitCount(byte);
    offset--;
  }

  return count;
};

/**
 * @param {Uint8Array} left
 * @param {Uint8Array} right
 */
const or = (left, right) => {
  const result = left.slice();
  let offset = 0;
  while (offset < left.length) {
    result[offset] |= right[offset];
    offset++;
  }
  return result;
};

/**
 * @param {Uint8Array} left
 * @param {Uint8Array} right
 */
const and = (left, right) => {
  const result = left.slice();
  let offset = 0;
  while (offset < left.length) {
    result[offset] &= right[offset];
    offset++;
  }
  return result;
};

var Uint8ArrayBitField = /*#__PURE__*/ Object.freeze({
  __proto__: null,
  API: api,
  and: and,
  empty: empty,
  from: from$3,
  fromBytes: fromBytes$1,
  get: get$3,
  or: or,
  popcount: popcount,
  set: set$1,
  size: size,
  toBytes: toBytes,
  unset: unset,
});

const NOT_FOUND = new RangeError("Not Found");

/**
 * @template {API.Config} [C=API.Config<API.Uint32>]
 * @param {Partial<C>} config
 * @returns {C}
 */
const configure$1 = ({
  bitWidth = 5,
  /* c8 ignore next 4 */
  BitField = bitWidth === 5 ? Uint32BitField : Uint8ArrayBitField,
  Path = bitWidth === 5 ? configure$3({ bitWidth }) : configure$2({ bitWidth }),
} = {}) => /** @type {C} */ ({ bitWidth, BitField, Path });

/**
 * Creates HashMap from the provided entries.
 *
 * @template [V=unknown]
 * @template {string} [K=string]
 * @template {API.Config} [C=API.Config<API.Uint32>]
 * @param {Iterable<[K, V]>} entries
 * @param {Partial<C>} [options]
 * @returns {API.PersistentHashMap<V, K, C>}
 */
const from$2 = (entries, options) => {
  const node = /** @type {API.HashMapBuilder<V, K, C>} */ (builder$1(options));
  for (const [key, value] of entries) {
    node.set(key, value);
  }

  return node.build();
};

/**
 * @template T
 * @template {string} K
 * @param {API.HAMT<T, K>} hamt
 * @param {K} key
 */
const has = (hamt, key) => get$4(hamt.root, key, NOT_FOUND) !== NOT_FOUND;

/**
 * @template T
 * @template {string} K
 * @template [U=undefined]
 * @param {API.HAMT<T, K>} hamt
 * @param {K} key
 * @param {U} notFound
 * @returns {T|U}
 */
const get$2 = (hamt, key, notFound = /** @type {U} */ (undefined)) =>
  get$4(hamt.root, key, notFound);

/**
 * @template {string} K
 * @template T
 * @template {API.Config} C
 * @param {Partial<C>} [options]
 * @returns {API.HashMapBuilder<T, K, C>}
 */
const builder$1 = (options) => {
  const edit = {};
  const config = configure$1(options);
  return new HashMapBuilder(edit, 0, create$5(config, edit), config);
};

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 * @implements {API.PersistentHashMap<T, K, C>}
 */
class PersistentHashMap {
  /**
   *
   * @param {number} count
   * @param {API.BitmapIndexedNode<T, K, C>} root
   * @param {C} config
   */
  constructor(count = 0, root, config) {
    this.count = count;
    this.root = root;
    this.config = config;
  }

  get size() {
    return this.count;
  }

  clone() {
    return new PersistentHashMap(this.count, this.root, this.config);
  }

  /**
   * @returns {API.PersistentHashMap<T, K, C>}
   */
  empty() {
    return new PersistentHashMap(0, create$5(this.config, null), this.config);
  }
  /**
   * @param {K} key
   * @returns {boolean}
   */
  has(key) {
    return has(this, key);
  }
  /**
   * @param {K} key
   * @returns {T|undefined}
   */
  get(key) {
    return get$4(this.root, key, undefined);
  }
  /**
   * @template {string} R
   * @param {R} key
   * @param {T} value
   * @returns {PersistentHashMap<T, K|R, C>}
   */
  set(key, value) {
    const addedLeaf = { value: false };
    const root = set$2(this.root, null, key, value, addedLeaf);
    if (root === this.root) {
      return this;
    } else {
      return new PersistentHashMap(
        addedLeaf.value ? this.count + 1 : this.count,
        root,
        this.config
      );
    }
  }
  /**
   * @param {K} key
   */
  delete(key) {
    const root = remove(this.root, null, key, { value: false });

    if (root === this.root) {
      return this;
    } else {
      return new PersistentHashMap(this.count - 1, root, this.config);
    }
  }

  /* c8 ignore next 3 */
  get bitField() {
    return this.config.BitField.or(this.root.datamap, this.root.nodemap);
  }

  [Symbol.iterator]() {
    return this.entries();
  }

  entries() {
    return this.root.entries();
  }
  keys() {
    return this.root.keys();
  }
  values() {
    return this.root.values();
  }

  /**
   * @returns {API.HashMapBuilder<T, K, C>}
   */

  createBuilder() {
    return new HashMapBuilder({}, this.count, this.root, this.config);
  }
}

/**
 * @template T
 * @template {string} K
 * @template {API.Config} C
 */
class HashMapBuilder {
  /**
   * @param {API.Edit} edit
   * @param {number} count
   * @param {API.BitmapIndexedNode<T, K, C>} root
   * @param {C} config
   */
  constructor(edit, count, root, config) {
    /**
     * @type {API.Edit|null}
     * @private
     */
    this.edit = edit;
    /**
     * @private
     */
    this.count = count;
    this.root = root;
    this.config = config;
  }

  get size() {
    if (this.edit) {
      return this.count;
    } else {
      throw new Error(`.size was accessed on the finalized builder`);
    }
  }
  /**
   * @template {string} R
   * @param {R} key
   * @param {T} value
   * @returns {HashMapBuilder<T, K|R, C>}
   */
  set(key, value) {
    if (this.edit) {
      const addedLeaf = { value: false };
      const root = set$2(this.root, this.edit, key, value, addedLeaf);

      if (this.root !== root) {
        this.root = /** @type {API.BitmapIndexedNode<T, K, C>} */ (root);
      }

      if (addedLeaf.value) {
        this.count += 1;
      }

      return this;
    } else {
      throw new Error(`.set was called on the finalized builder`);
    }
  }
  /**
   * @param {K} key
   */
  delete(key) {
    if (this.edit) {
      if (this.count === 0) {
        return this;
      }
      const removedLeaf = { value: false };
      const root = remove(this.root, this.edit, key, removedLeaf);

      if (root !== this.root) {
        this.root = root;
      }
      if (removedLeaf.value) {
        this.count -= 1;
      }
      return this;
    } else {
      throw new Error(`.delete was called on the finalized builder`);
    }
  }

  build() {
    if (this.edit) {
      this.edit = null;
      return new PersistentHashMap(this.count, this.root, this.config);
    } else {
      throw new Error(`.build was called on the finalized builder`);
    }
  }
}

const utf8 = new TextEncoder();

/**
 * @param {Uint8Array} bytes
 */
const hash64 = (bytes) => /** @type {Uint8Array} */ (murmur364.encode(bytes));

/**
 * @param {Partial<API.Options<Uint8Array>>} options
 * @returns {API.Path<Uint8Array>}
 */
const configure = ({ bitWidth = 8, hash = hash64 }) => {
  const hashSize = hash(new Uint8Array()).byteLength;
  const options = { bitWidth, hash, hashSize };

  /**
   * @param {Uint8Array} path
   * @param {number} depth
   * @returns {API.Uint32}
   */
  const at = (path, depth) => read$1(path, depth, options);

  /**
   * @param {string} key
   */
  const from = (key) => utf8.encode(key);

  return { at, from, size: Infinity };
};

/**
 * @param {Uint8Array} key
 * @param {number} depth
 * @param {object} options
 * @param {number} [options.bitWidth]
 * @param {number} options.hashSize
 * @param {(input:Uint8Array) => Uint8Array} options.hash
 */
const read$1 = (key, depth = 0, { bitWidth = 8, hash, hashSize }) => {
  // key digest consists of infinite number of hash frames that are computed
  // from key + frame n which looks like
  // [hash(key), hash([key, 1]), hash([key, n])]
  // You can think of the hash as concatination of all frames. Here we calculate
  // frame bit size from hash size as we going to use that several times.
  const frameBitSize = hashSize * 8;

  // We start with 0 digest and required `bitCount` corresponding to `bitWith`.
  // In the loop we'll going to consume `bitCount` hash bits.
  let digest = 0;
  let bitCount = bitWidth;
  // Calculate absolute bit offset within the key digest.
  let bitOffset = bitWidth * depth;
  while (bitCount > 0) {
    // We derive frame number based on current bit offset.
    const frameOffset = (bitOffset / frameBitSize) >> 0;
    // Then we compute that hash frame
    const frame =
      frameOffset === 0 ? hash(key) : hash(appendByte(key, frameOffset));

    // compute bit offset within the current frame
    const offset =
      frameBitSize <= bitOffset ? bitOffset % frameBitSize : bitOffset;
    // calculate number of bits remaining in this frame
    const maxBits = frameBitSize - offset;
    // we will consume all required bits from frame if enough are available
    // otherwise we consume whatever's available and continue rest in the next
    // cycle(s).
    const count = maxBits < bitCount ? maxBits : bitCount;
    digest = (digest << count) + toInt(frame, offset, count);
    bitCount -= count;
    bitOffset += count;
  }

  return digest;
};

/**
 * @param {Uint8Array} source
 * @param {number} byte
 */
const appendByte = (source, byte) => {
  const bytes = new Uint8Array(source.byteLength + 1).fill(
    byte,
    source.byteLength
  );
  bytes.set(source);
  return bytes;
};

const bitWidth = 8;
const config = {
  bitWidth,
  Path: configure({ bitWidth }),
};

/**
 * @param {HAMT.PersistentHashMap} hamt
 */
const tableSize = (hamt) => Math.pow(2, hamt.config.bitWidth);

/**
 * @template [T=unknown]
 * @template {string} [K=string]
 * @template {HAMT.Config} [C=HAMT.Config<Uint8Array>]
 * @param {Partial<C>} options
 * @returns {HAMT.HashMapBuilder<T, K, C>}
 */
const builder = (options = /** @type {C} */ (config)) => builder$1(options);

/**
 * @template [V=unknown]
 * @template {string} [K=string]
 * @template {HAMT.Config} [C=HAMT.Config<Uint8Array>]
 * @param {Iterable<[K, V]>} entries
 * @param {Partial<C>} options
 */
const from$1 = (entries, options = /** @type {C} */ (config)) =>
  from$2(entries, options);

/**
 * @template T
 * @template {string} K
 * @template Bits, Bitmap
 * @param {HAMT.BitmapIndexedNode<T, K, HAMT.Config<Bits, Bitmap>>} hamt
 */
const bitField = ({ datamap, nodemap, config: { BitField } }) =>
  withoutLeadingZeros(BitField.toBytes(BitField.or(datamap, nodemap)));

/**
 * @param {Uint8Array} bytes
 */
const withoutLeadingZeros = (bytes) => {
  let offset = 0;
  while (offset < bytes.byteLength) {
    if (bytes[offset] !== 0) {
      return bytes.subarray(offset);
    }
    offset += 1;
  }
  return bytes.subarray(offset);
};

/**
 * Maps HAMT node into IPFS UnixFS compatible format.
 *
 * @template T
 * @template {string} K
 * @template {HAMT.Config} C
 * @param {HAMT.BitmapIndexedNode<T, K, C>} root
 * @returns {IterableIterator<{prefix:string, key:K, value:T, node?:void}|{prefix:string, node:HAMT.BitmapIndexedNode<T, K, C>}>}
 */
const iterate = function* (root) {
  const { config, datamap, nodemap } = root;
  const { BitField: bitfield } = config;
  const size = bitfield.size(datamap);
  let bitOffset = 0;
  let dataCount = 0;
  while (bitOffset < size) {
    const prefix = bitOffset.toString(16).toUpperCase().padStart(2, "0");
    if (bitfield.get(datamap, bitOffset)) {
      const key = keyAt(root, dataCount);
      yield {
        prefix,
        key,
        value: valueAt(root, dataCount),
      };
      dataCount++;
    } else if (bitfield.get(nodemap, bitOffset)) {
      yield {
        prefix,
        // UnixFS never contains hash collision nodes because it uses
        // inifinite hashes
        node: /** @type {HAMT.BitmapIndexedNode<T, K, C>} */ (
          resolveNode(root, bitOffset)
        ),
      };
    }
    bitOffset++;
  }
};

function readonly({ enumerable = true, configurable = false } = {}) {
  return { enumerable, configurable, writable: false };
}

/**
 * @param {[string|number, string]} path
 * @param {any} value
 * @returns {Iterable<[string, CID]>}
 */
function* linksWithin(path, value) {
  if (value != null && typeof value === "object") {
    if (Array.isArray(value)) {
      for (const [index, element] of value.entries()) {
        const elementPath = [...path, index];
        const cid = CID.asCID(element);
        if (cid) {
          yield [elementPath.join("/"), cid];
        } else if (typeof element === "object") {
          yield* links(element, elementPath);
        }
      }
    } else {
      const cid = CID.asCID(value);
      if (cid) {
        yield [path.join("/"), cid];
      } else {
        yield* links(value, path);
      }
    }
  }
}

/**
 * @template T
 * @param {T} source
 * @param {Array<string|number>} base
 * @returns {Iterable<[string, CID]>}
 */
function* links(source, base) {
  if (source == null || source instanceof Uint8Array) {
    return;
  }
  const cid = CID.asCID(source);
  if (cid) {
    yield [base.join("/"), cid];
  }
  for (const [key, value] of Object.entries(source)) {
    const path = /** @type {[string|number, string]} */ ([...base, key]);
    yield* linksWithin(path, value);
  }
}

/**
 * @param {[string|number, string]} path
 * @param {any} value
 * @returns {Iterable<string>}
 */
function* treeWithin(path, value) {
  if (Array.isArray(value)) {
    for (const [index, element] of value.entries()) {
      const elementPath = [...path, index];
      yield elementPath.join("/");
      if (typeof element === "object" && !CID.asCID(element)) {
        yield* tree(element, elementPath);
      }
    }
  } else {
    yield* tree(value, path);
  }
}

/**
 * @template T
 * @param {T} source
 * @param {Array<string|number>} base
 * @returns {Iterable<string>}
 */
function* tree(source, base) {
  if (source == null || typeof source !== "object") {
    return;
  }
  for (const [key, value] of Object.entries(source)) {
    const path = /** @type {[string|number, string]} */ ([...base, key]);
    yield path.join("/");
    if (
      value != null &&
      !(value instanceof Uint8Array) &&
      typeof value === "object" &&
      !CID.asCID(value)
    ) {
      yield* treeWithin(path, value);
    }
  }
}

/**
 *
 * @template T
 * @param {T} source
 * @param {string[]} path
 * @returns {API.BlockCursorView<unknown>}
 */
function get$1(source, path) {
  let node = /** @type {Record<string, any>} */ (source);
  for (const [index, key] of path.entries()) {
    node = node[key];
    if (node == null) {
      throw new Error(
        `Object has no property at ${path
          .slice(0, index + 1)
          .map((part) => `[${JSON.stringify(part)}]`)
          .join("")}`
      );
    }
    const cid = CID.asCID(node);
    if (cid) {
      return { value: cid, remaining: path.slice(index + 1).join("/") };
    }
  }
  return { value: node };
}

/**
 * @template {unknown} T - Logical type of the data encoded in the block
 * @template {number} C - multicodec code corresponding to codec used to encode the block
 * @template {number} A - multicodec code corresponding to the hashing algorithm used in CID creation.
 * @template {API.Version} V - CID version
 * @implements {API.BlockView<T, C, A, V>}
 */
class Block {
  /**
   * @param {object} options
   * @param {CID<T, C, A, V>} options.cid
   * @param {API.ByteView<T>} options.bytes
   * @param {T} options.value
   */
  constructor({ cid, bytes, value }) {
    if (!cid || !bytes || typeof value === "undefined") {
      throw new Error("Missing required argument");
    }

    this.cid = cid;
    this.bytes = bytes;
    this.value = value;
    this.asBlock = this;

    // Mark all the properties immutable
    Object.defineProperties(this, {
      cid: readonly(),
      bytes: readonly(),
      value: readonly(),
      asBlock: readonly(),
    });
  }

  links() {
    return links(this.value, []);
  }

  tree() {
    return tree(this.value, []);
  }

  /**
   *
   * @param {string} [path]
   * @returns {API.BlockCursorView<unknown>}
   */
  get(path = "/") {
    return get$1(this.value, path.split("/").filter(Boolean));
  }
}

const defaults = defaults$2;

/**
 * @template [Layout=unknown]
 * @param {API.Options<Layout>} config
 * @returns {API.View<Layout>}
 */
const create$4 = ({ writer, settings = defaults(), metadata = {} }) =>
  new HAMTDirectoryWriter({
    writer,
    metadata,
    settings,
    entries: new HashMap(),
    closed: false,
  });

/**
 * @template {API.State} Writer
 * @param {Writer} writer
 * @returns {Writer}
 */
const asWritable = (writer) => {
  if (!writer.closed) {
    return writer;
  } else {
    throw new Error(
      "Can not change written HAMT directory, but you can .fork() and make changes to it"
    );
  }
};

/**
 * @template {unknown} Layout
 * @param {{ state: API.State<Layout> }} view
 * @param {API.CloseOptions} options
 * @returns {Promise<UnixFS.DirectoryLink>}
 */
const close$1 = async (
  view,
  { closeWriter = false, releaseLock = false } = {}
) => {
  const { writer, settings, metadata } = asWritable(view.state);
  view.state.closed = true;

  const { entries } = view.state;
  /* c8 ignore next 3 */
  if (!(entries instanceof HashMap)) {
    throw new Error(`not a HAMT: ${entries}`);
  }

  const hamt = entries.builder.build();
  const blocks = iterateBlocks(hamt, hamt.root, settings);

  /** @type {UnixFS.BlockView<UnixFS.DirectoryShard>?} */
  let root = null;
  for await (const block of blocks) {
    root = block;
    // we make sure that writer has some capacity for this write. If it
    // does not we await.
    if ((writer.desiredSize || 0) <= 0) {
      await writer.ready;
    }
    // once writer has some capacity we write a block, however we do not
    // await completion as we don't care when it's taken off the stream.
    writer.write(block);
  }
  /* c8 ignore next */
  if (root == null) throw new Error("no root block yielded");

  if (closeWriter) {
    await writer.close();
  } else if (releaseLock) {
    writer.releaseLock();
  }

  return {
    cid: root.cid,
    dagByteLength: cumulativeDagByteLength(root.bytes, root.value.entries),
  };
};

/**
 * @template {unknown} Layout
 * @param {UnixFSPermaMap.PersistentHashMap<API.EntryLink>} hamt
 * @param {UnixFSPermaMap.BitmapIndexedNode<API.EntryLink>} node
 * @param {API.EncoderSettings<Layout>} settings
 * @returns {AsyncIterableIterator<UnixFS.BlockView<UnixFS.DirectoryShard>>}
 */
const iterateBlocks = async function* (hamt, node, settings) {
  /** @type {UnixFS.DirectoryEntryLink[]} */
  const entries = [];
  for (const ent of iterate(node)) {
    if ("key" in ent) {
      entries.push(
        /** @type {UnixFS.DirectoryEntryLink} */ ({
          name: `${ent.prefix ?? ""}${ent.key ?? ""}`,
          dagByteLength: ent.value.dagByteLength,
          cid: ent.value.cid,
        })
      );
    } else {
      /** @type {UnixFS.BlockView<UnixFS.DirectoryShard>?} */
      let root = null;
      for await (const block of iterateBlocks(hamt, ent.node, settings)) {
        yield block;
        root = block;
      }
      /* c8 ignore next */
      if (root == null) throw new Error("no root block yielded");

      entries.push(
        /** @type {UnixFS.ShardedDirectoryLink} */ ({
          name: ent.prefix,
          dagByteLength: cumulativeDagByteLength(
            root.bytes,
            root.value.entries
          ),
          cid: root.cid,
        })
      );
    }
  }

  const shard = createDirectoryShard(
    entries,
    bitField(node),
    tableSize(hamt),
    murmur364.code
  );
  yield await encodeHAMTShardBlock(shard, settings);
};

/**
 * @template {unknown} Layout
 * @param {UnixFS.DirectoryShard} shard
 * @param {API.EncoderSettings<Layout>} settings
 * @returns {Promise<UnixFS.BlockView<UnixFS.DirectoryShard>>}
 */
async function encodeHAMTShardBlock(shard, settings) {
  const bytes = encodeHAMTShard(shard);
  const hash = await settings.hasher.digest(bytes);
  const cid = settings.linker.createLink(code$2, hash);
  // @ts-ignore Link is not CID
  return new Block({ cid, bytes, value: shard });
}

/**
 * @template L1, L2
 * @param {API.View<L1>} state
 * @param {Partial<API.Options<L1|L2>>} options
 * @returns {API.View<L1|L2>}
 */
const fork = (
  { state },
  {
    writer = state.writer,
    metadata = state.metadata,
    settings = state.settings,
  } = {}
) =>
  new HAMTDirectoryWriter({
    writer,
    metadata,
    settings,
    entries: new HashMap(from$1(state.entries.entries()).createBuilder()),
    closed: false,
  });

/**
 * @template [Layout=unknown]
 * @implements {API.View<Layout>}
 */
class HAMTDirectoryWriter {
  /**
   * @param {API.State<Layout>} state
   */
  constructor(state) {
    this.state = state;
  }
  get writer() {
    return this.state.writer;
  }
  get settings() {
    return this.state.settings;
  }

  /**
   * @param {string} name
   * @param {UnixFS.FileLink | UnixFS.DirectoryLink} link
   * @param {API.WriteOptions} [options]
   */

  set(name, link, options) {
    return set$4(this, name, link, options);
  }

  /**
   * @param {string} name
   */
  remove(name) {
    return remove$1(this, name);
  }

  /**
   * @template L
   * @param {Partial<API.Options<L>>} [options]
   * @returns {API.View<Layout|L>}
   */
  fork(options) {
    return fork(this, options);
  }

  /**
   * @param {API.CloseOptions} [options]
   * @returns {Promise<UnixFS.DirectoryLink>}
   */
  close(options) {
    return close$1(this, options);
  }

  entries() {
    return this.state.entries.entries();
  }
  /**
   * @param {string} name
   */
  has(name) {
    return this.state.entries.has(name);
  }
  get size() {
    return this.state.entries.size;
  }
}

/**
 * @implements {Map<string, API.EntryLink>}
 */
class HashMap extends Map {
  /**
   * @param {UnixFSPermaMap.HashMapBuilder} [builder]
   */
  constructor(builder$1 = builder()) {
    super();
    /** @type {UnixFSPermaMap.HashMapBuilder} */
    this.builder = builder$1;
  }

  clear() {
    this.builder = builder();
  }

  /**
   * @param {string} key
   */
  delete(key) {
    const { root } = this.builder;
    this.builder.delete(key);
    return this.builder.root !== root;
  }

  /**
   * @param {(value: API.EntryLink, key: string, map: Map<string, API.EntryLink>) => void} callbackfn
   * @param {any} [thisArg]
   */
  forEach(callbackfn, thisArg = this) {
    for (const [k, v] of this.builder.root.entries()) {
      callbackfn.call(thisArg, v, k, this);
    }
  }

  /**
   * @param {string} key
   */
  get(key) {
    return get$2(this.builder, key);
  }

  /**
   * @param {string} key
   */
  has(key) {
    return has(this.builder, key);
  }

  /**
   * @param {string} key
   * @param {API.EntryLink} value
   */
  set(key, value) {
    this.builder.set(key, value);
    return this;
  }

  get size() {
    return this.builder.size;
  }

  [Symbol.iterator]() {
    return this.builder.root.entries();
  }

  entries() {
    return this.builder.root.entries();
  }

  keys() {
    return this.builder.root.keys();
  }

  values() {
    return this.builder.root.values();
  }
}

/**
 * @template [Layout=unknown]
 * @param {API.Options<Layout>} options
 * @returns {API.View<Layout>}
 */
const createWriter = ({ writable, settings = defaults$2() }) =>
  new FileSystemWriter({
    writer: writable.getWriter(),
    settings,
  });

/**
 * @template {{writer:API.BlockWriter}} View
 * @param {View} view
 * @param {API.CloseOptions} options
 */
const close = async (view, { releaseLock = true, closeWriter = true } = {}) => {
  if (closeWriter) {
    await view.writer.close();
  } else if (releaseLock) {
    view.writer.releaseLock();
  }

  return view;
};

/**
 * @template [Layout=unknown]
 * @implemets {API.View<Layout>}
 */
class FileSystemWriter {
  /**
   * @param {object} options
   * @param {API.BlockWriter} options.writer
   * @param {Partial<API.EncoderSettings<Layout>>} options.settings
   */
  constructor({ writer, settings }) {
    this.writer = writer;
    this.settings = configure$4(settings);
  }

  /**
   * @template [L=unknown]
   * @param {API.WriterOptions<L|Layout>} config
   */
  createFileWriter({ settings = this.settings, metadata } = {}) {
    return create$7({
      writer: this.writer,
      settings,
      metadata,
    });
  }

  /**
   * @template [L=unknown]
   * @param {API.WriterOptions<L|Layout>} config
   */
  createDirectoryWriter({ settings = this.settings, metadata } = {}) {
    return create$6({
      writer: this.writer,
      settings,
      metadata,
    });
  }

  /**
   * @param {API.CloseOptions} [options]
   */
  close(options) {
    return close(this, options);
  }
}

// BlockSizeLimit specifies the maximum size an imported block can have.
// @see https://github.com/ipfs/go-unixfs/blob/68c015a6f317ed5e21a4870f7c423a4b38b90a96/importer/helpers/helpers.go#L7-L8
const BLOCK_SIZE_LIMIT = 1048576; // 1 MB
const defaultCapacity = BLOCK_SIZE_LIMIT * 100;

/**
 * Creates `QueuingStrategy` that can fit blocks with total size up to given
 * byteLength.
 *
 * @param {number} byteLength
 * @returns {Required<QueuingStrategy<API.Block>>}
 */
const withCapacity = (byteLength = defaultCapacity) => ({
  highWaterMark: byteLength,
  size: (block) => block.bytes.length,
});

const SHARD_THRESHOLD = 1000; // shard directory after > 1,000 items
const queuingStrategy = withCapacity();
const defaultSettings = configure$4({
  fileChunkEncoder: raw,
  smallFileEncoder: raw,
  chunker: withMaxChunkSize(1024 * 1024),
  fileLayout: withWidth(1024),
});
/**
 * @param {import('./types.js').BlobLike} blob
 * @param {import('./types.js').UnixFSEncoderSettingsOptions} [options]
 * @returns {ReadableStream<import('@ipld/unixfs').Block>}
 */
function createFileEncoderStream(blob, options) {
  /** @type {TransformStream<import('@ipld/unixfs').Block, import('@ipld/unixfs').Block>} */
  const { readable, writable } = new TransformStream({}, queuingStrategy);
  const settings = options?.settings ?? defaultSettings;
  const unixfsWriter = createWriter({ writable, settings });
  const fileBuilder = new UnixFSFileBuilder("", blob);
  void (async () => {
    await fileBuilder.finalize(unixfsWriter);
    await unixfsWriter.close();
  })();
  return readable;
}
class UnixFSFileBuilder {
  #file;
  /**
   * @param {string} name
   * @param {import('./types.js').BlobLike} file
   */
  constructor(name, file) {
    this.name = name;
    this.#file = file;
  }
  /** @param {import('@ipld/unixfs').View} writer */
  async finalize(writer) {
    const unixfsFileWriter = create$7(writer);
    await this.#file.stream().pipeTo(
      new WritableStream({
        async write(chunk) {
          await unixfsFileWriter.write(chunk);
        },
      })
    );
    return await unixfsFileWriter.close();
  }
}
class UnixFSDirectoryBuilder {
  #options;
  /** @type {Map<string, UnixFSFileBuilder | UnixFSDirectoryBuilder>} */
  entries = new Map();
  /**
   * @param {string} name
   * @param {import('./types.js').UnixFSDirectoryEncoderOptions} [options]
   */
  constructor(name, options) {
    this.name = name;
    this.#options = options;
  }
  /** @param {import('@ipld/unixfs').View} writer */
  async finalize(writer) {
    const dirWriter =
      this.entries.size <= SHARD_THRESHOLD
        ? create$6(writer)
        : create$4(writer);
    for (const [name, entry] of this.entries) {
      const link = await entry.finalize(writer);
      if (this.#options?.onDirectoryEntryLink) {
        // @ts-expect-error
        this.#options.onDirectoryEntryLink({ name: entry.name, ...link });
      }
      dirWriter.set(name, link);
    }
    return await dirWriter.close();
  }
}
/**
 * @param {Iterable<import('./types.js').FileLike>} files
 * @param {import('./types.js').UnixFSEncoderSettingsOptions & import('./types.js').UnixFSDirectoryEncoderOptions} [options]
 * @returns {ReadableStream<import('@ipld/unixfs').Block>}
 */
function createDirectoryEncoderStream(files, options) {
  const rootDir = new UnixFSDirectoryBuilder("", options);
  for (const file of files) {
    const path = file.name.split("/");
    if (path[0] === "" || path[0] === ".") {
      path.shift();
    }
    let dir = rootDir;
    for (const [i, name] of path.entries()) {
      if (i === path.length - 1) {
        dir.entries.set(name, new UnixFSFileBuilder(path.join("/"), file));
        break;
      }
      let dirBuilder = dir.entries.get(name);
      if (dirBuilder == null) {
        const dirName = dir === rootDir ? name : `${dir.name}/${name}`;
        dirBuilder = new UnixFSDirectoryBuilder(dirName, options);
        dir.entries.set(name, dirBuilder);
      }
      if (!(dirBuilder instanceof UnixFSDirectoryBuilder)) {
        throw new Error(`"${file.name}" cannot be a file and a directory`);
      }
      dir = dirBuilder;
    }
  }
  /** @type {TransformStream<import('@ipld/unixfs').Block, import('@ipld/unixfs').Block>} */
  const { readable, writable } = new TransformStream({}, queuingStrategy);
  const settings = options?.settings ?? defaultSettings;
  const unixfsWriter = createWriter({ writable, settings });
  void (async () => {
    const link = await rootDir.finalize(unixfsWriter);
    if (options?.onDirectoryEntryLink) {
      options.onDirectoryEntryLink({ name: "", ...link });
    }
    await unixfsWriter.close();
  })();
  return readable;
}

/**
 * @typedef {import('./api').Block} Block
 * @typedef {import('./api').BlockHeader} BlockHeader
 * @typedef {import('./api').BlockIndex} BlockIndex
 * @typedef {import('./coding').BytesReader} BytesReader
 * @typedef {import('./coding').CarHeader} CarHeader
 * @typedef {import('./coding').CarV2Header} CarV2Header
 * @typedef {import('./coding').CarV2FixedHeader} CarV2FixedHeader
 * @typedef {import('./coding').CarDecoder} CarDecoder
 */

/**
 * Reads header data from a `BytesReader`. The header may either be in the form
 * of a `CarHeader` or `CarV2Header` depending on the CAR being read.
 *
 * @name async decoder.readHeader(reader)
 * @param {BytesReader} reader
 * @param {number} [strictVersion]
 * @returns {Promise<CarHeader|CarV2Header>}
 */
async function readHeader(reader, strictVersion) {
  const length = decodeVarint$1(await reader.upTo(8), reader);
  if (length === 0) {
    throw new Error("Invalid CAR header (zero length)");
  }
  const header = await reader.exactly(length, true);
  const block = decode$y(header);
  if (CarV1HeaderOrV2Pragma.toTyped(block) === undefined) {
    throw new Error("Invalid CAR header format");
  }
  if (
    (block.version !== 1 && block.version !== 2) ||
    (strictVersion !== undefined && block.version !== strictVersion)
  ) {
    throw new Error(
      `Invalid CAR version: ${block.version}${
        strictVersion !== undefined ? ` (expected ${strictVersion})` : ""
      }`
    );
  }
  if (block.version === 1) {
    // CarV1HeaderOrV2Pragma makes roots optional, let's make it mandatory
    if (!Array.isArray(block.roots)) {
      throw new Error("Invalid CAR header format");
    }
    return block;
  }
  // version 2
  if (block.roots !== undefined) {
    throw new Error("Invalid CAR header format");
  }
  const v2Header = decodeV2Header(await reader.exactly(V2_HEADER_LENGTH, true));
  reader.seek(v2Header.dataOffset - reader.pos);
  const v1Header = await readHeader(reader, 1);
  return Object.assign(v1Header, v2Header);
}

/**
 * @param {BytesReader} reader
 * @returns {Promise<CID>}
 */
async function readCid(reader) {
  const first = await reader.exactly(2, false);
  if (
    first[0] === CIDV0_BYTES$1.SHA2_256 &&
    first[1] === CIDV0_BYTES$1.LENGTH
  ) {
    // cidv0 32-byte sha2-256
    const bytes = await reader.exactly(34, true);
    const multihash = decode$z(bytes);
    return CID$2.create(0, CIDV0_BYTES$1.DAG_PB, multihash);
  }

  const version = decodeVarint$1(await reader.upTo(8), reader);
  if (version !== 1) {
    throw new Error(`Unexpected CID version (${version})`);
  }
  const codec = decodeVarint$1(await reader.upTo(8), reader);
  const bytes = await reader.exactly(
    getMultihashLength$1(await reader.upTo(8)),
    true
  );
  const multihash = decode$z(bytes);
  return CID$2.create(version, codec, multihash);
}

/**
 * Reads the leading data of an individual block from CAR data from a
 * `BytesReader`. Returns a `BlockHeader` object which contains
 * `{ cid, length, blockLength }` which can be used to either index the block
 * or read the block binary data.
 *
 * @name async decoder.readBlockHead(reader)
 * @param {BytesReader} reader
 * @returns {Promise<BlockHeader>}
 */
async function readBlockHead(reader) {
  // length includes a CID + Binary, where CID has a variable length
  // we have to deal with
  const start = reader.pos;
  let length = decodeVarint$1(await reader.upTo(8), reader);
  if (length === 0) {
    throw new Error("Invalid CAR section (zero length)");
  }
  length += reader.pos - start;
  const cid = await readCid(reader);
  const blockLength = length - Number(reader.pos - start); // subtract CID length

  return { cid, length, blockLength };
}

/**
 * @param {BytesReader} reader
 * @returns {Promise<Block>}
 */
async function readBlock(reader) {
  const { cid, blockLength } = await readBlockHead(reader);
  const bytes = await reader.exactly(blockLength, true);
  return { bytes, cid };
}

/**
 * @param {BytesReader} reader
 * @returns {Promise<BlockIndex>}
 */
async function readBlockIndex(reader) {
  const offset = reader.pos;
  const { cid, length, blockLength } = await readBlockHead(reader);
  const index = { cid, length, blockLength, offset, blockOffset: reader.pos };
  reader.seek(index.blockLength);
  return index;
}

/**
 * Creates a `CarDecoder` from a `BytesReader`. The `CarDecoder` is as async
 * interface that will consume the bytes from the `BytesReader` to yield a
 * `header()` and either `blocks()` or `blocksIndex()` data.
 *
 * @name decoder.createDecoder(reader)
 * @param {BytesReader} reader
 * @returns {CarDecoder}
 */
function createDecoder(reader) {
  const headerPromise = (async () => {
    const header = await readHeader(reader);
    if (header.version === 2) {
      const v1length = reader.pos - header.dataOffset;
      reader = limitReader(reader, header.dataSize - v1length);
    }
    return header;
  })();

  return {
    header: () => headerPromise,

    async *blocks() {
      await headerPromise;
      while ((await reader.upTo(8)).length > 0) {
        yield await readBlock(reader);
      }
    },

    async *blocksIndex() {
      await headerPromise;
      while ((await reader.upTo(8)).length > 0) {
        yield await readBlockIndex(reader);
      }
    },
  };
}

/**
 * Creates a `BytesReader` from a `Uint8Array`.
 *
 * @name decoder.bytesReader(bytes)
 * @param {Uint8Array} bytes
 * @returns {BytesReader}
 */
function bytesReader(bytes) {
  let pos = 0;

  /** @type {BytesReader} */
  return {
    async upTo(length) {
      const out = bytes.subarray(
        pos,
        pos + Math.min(length, bytes.length - pos)
      );
      return out;
    },

    async exactly(length, seek = false) {
      if (length > bytes.length - pos) {
        throw new Error("Unexpected end of data");
      }
      const out = bytes.subarray(pos, pos + length);
      if (seek) {
        pos += length;
      }
      return out;
    },

    seek(length) {
      pos += length;
    },

    get pos() {
      return pos;
    },
  };
}

/**
 * @ignore
 * reusable reader for streams and files, we just need a way to read an
 * additional chunk (of some undetermined size) and a way to close the
 * reader when finished
 * @param {() => Promise<Uint8Array|null>} readChunk
 * @returns {BytesReader}
 */
function chunkReader(readChunk /*, closer */) {
  let pos = 0;
  let have = 0;
  let offset = 0;
  let currentChunk = new Uint8Array(0);

  const read = async (/** @type {number} */ length) => {
    have = currentChunk.length - offset;
    const bufa = [currentChunk.subarray(offset)];
    while (have < length) {
      const chunk = await readChunk();
      if (chunk == null) {
        break;
      }
      /* c8 ignore next 8 */
      // undo this ignore ^ when we have a fd implementation that can seek()
      if (have < 0) {
        // because of a seek()
        /* c8 ignore next 4 */
        // toohard to test the else
        if (chunk.length > have) {
          bufa.push(chunk.subarray(-have));
        } // else discard
      } else {
        bufa.push(chunk);
      }
      have += chunk.length;
    }
    currentChunk = new Uint8Array(bufa.reduce((p, c) => p + c.length, 0));
    let off = 0;
    for (const b of bufa) {
      currentChunk.set(b, off);
      off += b.length;
    }
    offset = 0;
  };

  /** @type {BytesReader} */
  return {
    async upTo(length) {
      if (currentChunk.length - offset < length) {
        await read(length);
      }
      return currentChunk.subarray(
        offset,
        offset + Math.min(currentChunk.length - offset, length)
      );
    },

    async exactly(length, seek = false) {
      if (currentChunk.length - offset < length) {
        await read(length);
      }
      if (currentChunk.length - offset < length) {
        throw new Error("Unexpected end of data");
      }
      const out = currentChunk.subarray(offset, offset + length);
      if (seek) {
        pos += length;
        offset += length;
      }
      return out;
    },

    seek(length) {
      pos += length;
      offset += length;
    },

    get pos() {
      return pos;
    },
  };
}

/**
 * Creates a `BytesReader` from an `AsyncIterable<Uint8Array>`, which allows for
 * consumption of CAR data from a streaming source.
 *
 * @name decoder.asyncIterableReader(asyncIterable)
 * @param {AsyncIterable<Uint8Array>} asyncIterable
 * @returns {BytesReader}
 */
function asyncIterableReader(asyncIterable) {
  const iterator = asyncIterable[Symbol.asyncIterator]();

  async function readChunk() {
    const next = await iterator.next();
    if (next.done) {
      return null;
    }
    return next.value;
  }

  return chunkReader(readChunk);
}

/**
 * Wraps a `BytesReader` in a limiting `BytesReader` which limits maximum read
 * to `byteLimit` bytes. It _does not_ update `pos` of the original
 * `BytesReader`.
 *
 * @name decoder.limitReader(reader, byteLimit)
 * @param {BytesReader} reader
 * @param {number} byteLimit
 * @returns {BytesReader}
 */
function limitReader(reader, byteLimit) {
  let bytesRead = 0;

  /** @type {BytesReader} */
  return {
    async upTo(length) {
      let bytes = await reader.upTo(length);
      if (bytes.length + bytesRead > byteLimit) {
        bytes = bytes.subarray(0, byteLimit - bytesRead);
      }
      return bytes;
    },

    async exactly(length, seek = false) {
      const bytes = await reader.exactly(length, seek);
      if (bytes.length + bytesRead > byteLimit) {
        throw new Error("Unexpected end of data");
      }
      if (seek) {
        bytesRead += length;
      }
      return bytes;
    },

    seek(length) {
      bytesRead += length;
      reader.seek(length);
    },

    get pos() {
      return reader.pos;
    },
  };
}

/**
 * @typedef {import('multiformats').CID} CID
 * @typedef {import('./api').Block} Block
 * @typedef {import('./api').RootsReader} RootsReader
 * @typedef {import('./coding').BytesReader} BytesReader
 */

/**
 * @class
 * @implements {RootsReader}
 * @property {number} version The version number of the CAR referenced by this reader (should be `1`).
 */
class CarIteratorBase {
  /**
   * @param {number} version
   * @param {CID[]} roots
   * @param {AsyncIterable<Block>|void} iterable
   */
  constructor(version, roots, iterable) {
    this._version = version;
    this._roots = roots;
    this._iterable = iterable;
    this._decoded = false;
  }

  get version() {
    return this._version;
  }

  /**
   * @returns {Promise<CID[]>}
   */
  async getRoots() {
    return this._roots;
  }
}

/**
 * Provides an iterator over all of the `Block`s in a CAR. Implements a
 * `BlockIterator` interface, or `AsyncIterable<Block>`. Where a `Block` is
 * a `{ cid:CID, bytes:Uint8Array }` pair.
 *
 * As an implementer of `AsyncIterable`, this class can be used directly in a
 * `for await (const block of iterator) {}` loop. Where the `iterator` is
 * constructed using {@link CarBlockiterator.fromBytes} or
 * {@link CarBlockiterator.fromIterable}.
 *
 * An iteration can only be performce _once_ per instantiation.
 *
 * `CarBlockIterator` also implements the `RootsReader` interface and provides
 * the {@link CarBlockiterator.getRoots `getRoots()`} method.
 *
 * Load this class with either
 * `import { CarBlockIterator } from '@ipld/car/iterator'`
 * (`const { CarBlockIterator } = require('@ipld/car/iterator')`). Or
 * `import { CarBlockIterator } from '@ipld/car'`
 * (`const { CarBlockIterator } = require('@ipld/car')`).
 *
 * @name CarBlockIterator
 * @class
 * @implements {RootsReader}
 * @implements {AsyncIterable<Block>}
 * @property {number} version The version number of the CAR referenced by this
 * iterator (should be `1`).
 */
class CarBlockIterator extends CarIteratorBase {
  // inherited method
  /**
   * Get the list of roots defined by the CAR referenced by this iterator. May be
   * zero or more `CID`s.
   *
   * @function getRoots
   * @memberof CarBlockIterator
   * @instance
   * @async
   * @returns {Promise<CID[]>}
   */

  /**
   * @returns {AsyncIterator<Block>}
   */
  [Symbol.asyncIterator]() {
    if (this._decoded) {
      throw new Error("Cannot decode more than once");
    }
    /* c8 ignore next 3 */
    if (!this._iterable) {
      throw new Error("Block iterable not found");
    }
    this._decoded = true;
    return this._iterable[Symbol.asyncIterator]();
  }

  /**
   * Instantiate a {@link CarBlockIterator} from a `Uint8Array` blob. Rather
   * than decoding the entire byte array prior to returning the iterator, as in
   * {@link CarReader.fromBytes}, only the header is decoded and the remainder
   * of the CAR is parsed as the `Block`s as yielded.
   *
   * @async
   * @static
   * @memberof CarBlockIterator
   * @param {Uint8Array} bytes
   * @returns {Promise<CarBlockIterator>}
   */
  static async fromBytes(bytes) {
    const { version, roots, iterator } = await fromBytes(bytes);
    return new CarBlockIterator(version, roots, iterator);
  }

  /**
   * Instantiate a {@link CarBlockIterator} from a `AsyncIterable<Uint8Array>`,
   * such as a [modern Node.js stream](https://nodejs.org/api/stream.html#stream_streams_compatibility_with_async_generators_and_async_iterators).
   * Rather than decoding the entire byte array prior to returning the iterator,
   * as in {@link CarReader.fromIterable}, only the header is decoded and the
   * remainder of the CAR is parsed as the `Block`s as yielded.
   *
   * @async
   * @static
   * @param {AsyncIterable<Uint8Array>} asyncIterable
   * @returns {Promise<CarBlockIterator>}
   */
  static async fromIterable(asyncIterable) {
    const { version, roots, iterator } = await fromIterable(asyncIterable);
    return new CarBlockIterator(version, roots, iterator);
  }
}

/**
 * @param {Uint8Array} bytes
 * @returns {Promise<{ version:number, roots:CID[], iterator:AsyncIterable<Block>}>}
 */
async function fromBytes(bytes) {
  if (!(bytes instanceof Uint8Array)) {
    throw new TypeError("fromBytes() requires a Uint8Array");
  }
  return decodeIterator(bytesReader(bytes));
}

/**
 * @param {AsyncIterable<Uint8Array>} asyncIterable
 * @returns {Promise<{ version:number, roots:CID[], iterator:AsyncIterable<Block>}>}
 */
async function fromIterable(asyncIterable) {
  if (
    !asyncIterable ||
    !(typeof asyncIterable[Symbol.asyncIterator] === "function")
  ) {
    throw new TypeError("fromIterable() requires an async iterable");
  }
  return decodeIterator(asyncIterableReader(asyncIterable));
}

/**
 * @private
 * @param {BytesReader} reader
 * @returns {Promise<{ version:number, roots:CID[], iterator:AsyncIterable<Block>}>}
 */
async function decodeIterator(reader) {
  const decoder = createDecoder(reader);
  const { version, roots } = await decoder.header();
  return { version, roots, iterator: decoder.blocks() };
}

/**
 * @typedef {import('multiformats').CID} CID
 * @typedef {import('./api').Block} Block
 * @typedef {import('./coding').CarEncoder} CarEncoder
 * @typedef {import('./coding').IteratorChannel_Writer<Uint8Array>} IteratorChannel_Writer
 */

const CAR_V1_VERSION = 1;

/**
 * Create a header from an array of roots.
 *
 * @param {CID[]} roots
 * @returns {Uint8Array}
 */
function createHeader(roots) {
  const headerBytes = encode$q({ version: CAR_V1_VERSION, roots });
  const varintBytes = varint$3.encode(headerBytes.length);
  const header = new Uint8Array(varintBytes.length + headerBytes.length);
  header.set(varintBytes, 0);
  header.set(headerBytes, varintBytes.length);
  return header;
}

/**
 * @param {IteratorChannel_Writer} writer
 * @returns {CarEncoder}
 */
function createEncoder(writer) {
  // none of this is wrapped in a mutex, that needs to happen above this to
  // avoid overwrites

  return {
    /**
     * @param {CID[]} roots
     * @returns {Promise<void>}
     */
    async setRoots(roots) {
      const bytes = createHeader(roots);
      await writer.write(bytes);
    },

    /**
     * @param {Block} block
     * @returns {Promise<void>}
     */
    async writeBlock(block) {
      const { cid, bytes } = block;
      await writer.write(
        new Uint8Array(varint$3.encode(cid.bytes.length + bytes.length))
      );
      await writer.write(cid.bytes);
      if (bytes.length) {
        // zero-length blocks are valid, but it'd be safer if we didn't write them
        await writer.write(bytes);
      }
    },

    /**
     * @returns {Promise<void>}
     */
    async close() {
      await writer.end();
    },

    /**
     * @returns {number}
     */
    version() {
      return CAR_V1_VERSION;
    },
  };
}

/**
 * @template {any} T
 * @typedef {import('./coding').IteratorChannel<T>} IteratorChannel
 */

function noop() {}

/**
 * @template {any} T
 * @returns {IteratorChannel<T>}
 */
function create$3() {
  /** @type {T[]} */
  const chunkQueue = [];
  /** @type {Promise<void> | null} */
  let drainer = null;
  let drainerResolver = noop;
  let ended = false;
  /** @type {Promise<IteratorResult<T>> | null} */
  let outWait = null;
  let outWaitResolver = noop;

  const makeDrainer = () => {
    if (!drainer) {
      drainer = new Promise((resolve) => {
        drainerResolver = () => {
          drainer = null;
          drainerResolver = noop;
          resolve();
        };
      });
    }
    return drainer;
  };

  /**
   * @returns {IteratorChannel<T>}
   */
  const writer = {
    /**
     * @param {T} chunk
     * @returns {Promise<void>}
     */
    write(chunk) {
      chunkQueue.push(chunk);
      const drainer = makeDrainer();
      outWaitResolver();
      return drainer;
    },

    async end() {
      ended = true;
      const drainer = makeDrainer();
      outWaitResolver();
      await drainer;
    },
  };

  /** @type {AsyncIterator<T>} */
  const iterator = {
    /** @returns {Promise<IteratorResult<T>>} */
    async next() {
      const chunk = chunkQueue.shift();
      if (chunk) {
        if (chunkQueue.length === 0) {
          drainerResolver();
        }
        return { done: false, value: chunk };
      }

      if (ended) {
        drainerResolver();
        return { done: true, value: undefined };
      }

      if (!outWait) {
        outWait = new Promise((resolve) => {
          outWaitResolver = () => {
            outWait = null;
            outWaitResolver = noop;
            return resolve(iterator.next());
          };
        });
      }

      return outWait;
    },
  };

  return { writer, iterator };
}

/**
 * @typedef {import('./api').Block} Block
 * @typedef {import('./api').BlockWriter} BlockWriter
 * @typedef {import('./api').WriterChannel} WriterChannel
 * @typedef {import('./coding').CarEncoder} CarEncoder
 * @typedef {import('./coding').IteratorChannel<Uint8Array>} IteratorChannel
 */

/**
 * Provides a writer interface for the creation of CAR files.
 *
 * Creation of a `CarWriter` involves the instatiation of an input / output pair
 * in the form of a `WriterChannel`, which is a
 * `{ writer:CarWriter, out:AsyncIterable<Uint8Array> }` pair. These two
 * components form what can be thought of as a stream-like interface. The
 * `writer` component (an instantiated `CarWriter`), has methods to
 * {@link CarWriter.put `put()`} new blocks and {@link CarWriter.put `close()`}
 * the writing operation (finalising the CAR archive). The `out` component is
 * an `AsyncIterable` that yields the bytes of the archive. This can be
 * redirected to a file or other sink. In Node.js, you can use the
 * [`Readable.from()`](https://nodejs.org/api/stream.html#stream_stream_readable_from_iterable_options)
 * API to convert this to a standard Node.js stream, or it can be directly fed
 * to a
 * [`stream.pipeline()`](https://nodejs.org/api/stream.html#stream_stream_pipeline_source_transforms_destination_callback).
 *
 * The channel will provide a form of backpressure. The `Promise` from a
 * `write()` won't resolve until the resulting data is drained from the `out`
 * iterable.
 *
 * It is also possible to ignore the `Promise` from `write()` calls and allow
 * the generated data to queue in memory. This should be avoided for large CAR
 * archives of course due to the memory costs and potential for memory overflow.
 *
 * Load this class with either
 * `import { CarWriter } from '@ipld/car/writer'`
 * (`const { CarWriter } = require('@ipld/car/writer')`). Or
 * `import { CarWriter } from '@ipld/car'`
 * (`const { CarWriter } = require('@ipld/car')`). The former will likely
 * result in smaller bundle sizes where this is important.
 *
 * @name CarWriter
 * @class
 * @implements {BlockWriter}
 */
class CarWriter {
  /**
   * @param {CID[]} roots
   * @param {CarEncoder} encoder
   */
  constructor(roots, encoder) {
    this._encoder = encoder;
    /** @type {Promise<void>} */
    this._mutex = encoder.setRoots(roots);
    this._ended = false;
  }

  /**
   * Write a `Block` (a `{ cid:CID, bytes:Uint8Array }` pair) to the archive.
   *
   * @function
   * @memberof CarWriter
   * @instance
   * @async
   * @param {Block} block - A `{ cid:CID, bytes:Uint8Array }` pair.
   * @returns {Promise<void>} The returned promise will only resolve once the
   * bytes this block generates are written to the `out` iterable.
   */
  async put(block) {
    if (!(block.bytes instanceof Uint8Array) || !block.cid) {
      throw new TypeError("Can only write {cid, bytes} objects");
    }
    if (this._ended) {
      throw new Error("Already closed");
    }
    const cid = CID$2.asCID(block.cid);
    if (!cid) {
      throw new TypeError("Can only write {cid, bytes} objects");
    }
    this._mutex = this._mutex.then(() =>
      this._encoder.writeBlock({ cid, bytes: block.bytes })
    );
    return this._mutex;
  }

  /**
   * Finalise the CAR archive and signal that the `out` iterable should end once
   * any remaining bytes are written.
   *
   * @function
   * @memberof CarWriter
   * @instance
   * @async
   * @returns {Promise<void>}
   */
  async close() {
    if (this._ended) {
      throw new Error("Already closed");
    }
    await this._mutex;
    this._ended = true;
    return this._encoder.close();
  }

  /**
   * Returns the version number of the CAR file being written
   *
   * @returns {number}
   */
  version() {
    return this._encoder.version();
  }

  /**
   * Create a new CAR writer "channel" which consists of a
   * `{ writer:CarWriter, out:AsyncIterable<Uint8Array> }` pair.
   *
   * @async
   * @static
   * @memberof CarWriter
   * @param {CID[] | CID | void} roots
   * @returns {WriterChannel} The channel takes the form of
   * `{ writer:CarWriter, out:AsyncIterable<Uint8Array> }`.
   */
  static create(roots) {
    roots = toRoots(roots);
    const { encoder, iterator } = encodeWriter();
    const writer = new CarWriter(roots, encoder);
    const out = new CarWriterOut(iterator);
    return { writer, out };
  }

  /**
   * Create a new CAR appender "channel" which consists of a
   * `{ writer:CarWriter, out:AsyncIterable<Uint8Array> }` pair.
   * This appender does not consider roots and does not produce a CAR header.
   * It is designed to append blocks to an _existing_ CAR archive. It is
   * expected that `out` will be concatenated onto the end of an existing
   * archive that already has a properly formatted header.
   *
   * @async
   * @static
   * @memberof CarWriter
   * @returns {WriterChannel} The channel takes the form of
   * `{ writer:CarWriter, out:AsyncIterable<Uint8Array> }`.
   */
  static createAppender() {
    const { encoder, iterator } = encodeWriter();
    encoder.setRoots = () => Promise.resolve();
    const writer = new CarWriter([], encoder);
    const out = new CarWriterOut(iterator);
    return { writer, out };
  }

  /**
   * Update the list of roots in the header of an existing CAR as represented
   * in a Uint8Array.
   *
   * This operation is an _overwrite_, the total length of the CAR will not be
   * modified. A rejection will occur if the new header will not be the same
   * length as the existing header, in which case the CAR will not be modified.
   * It is the responsibility of the user to ensure that the roots being
   * replaced encode as the same length as the new roots.
   *
   * The byte array passed in an argument will be modified and also returned
   * upon successful modification.
   *
   * @async
   * @static
   * @memberof CarWriter
   * @param {Uint8Array} bytes
   * @param {CID[]} roots - A new list of roots to replace the existing list in
   * the CAR header. The new header must take up the same number of bytes as the
   * existing header, so the roots should collectively be the same byte length
   * as the existing roots.
   * @returns {Promise<Uint8Array>}
   */
  static async updateRootsInBytes(bytes, roots) {
    const reader = bytesReader(bytes);
    await readHeader(reader);
    const newHeader = createHeader(roots);
    if (Number(reader.pos) !== newHeader.length) {
      throw new Error(
        `updateRoots() can only overwrite a header of the same length (old header is ${reader.pos} bytes, new header is ${newHeader.length} bytes)`
      );
    }
    bytes.set(newHeader, 0);
    return bytes;
  }
}

/**
 * @class
 * @implements {AsyncIterable<Uint8Array>}
 */
class CarWriterOut {
  /**
   * @param {AsyncIterator<Uint8Array>} iterator
   */
  constructor(iterator) {
    this._iterator = iterator;
  }

  [Symbol.asyncIterator]() {
    if (this._iterating) {
      throw new Error("Multiple iterator not supported");
    }
    this._iterating = true;
    return this._iterator;
  }
}

function encodeWriter() {
  /** @type {IteratorChannel} */
  const iw = create$3();
  const { writer, iterator } = iw;
  const encoder = createEncoder(writer);
  return { encoder, iterator };
}

/**
 * @private
 * @param {CID[] | CID | void} roots
 * @returns {CID[]}
 */
function toRoots(roots) {
  if (roots === undefined) {
    return [];
  }

  if (!Array.isArray(roots)) {
    const cid = CID$2.asCID(roots);
    if (!cid) {
      throw new TypeError("roots must be a single CID or an array of CIDs");
    }
    return [cid];
  }

  const _roots = [];
  for (const root of roots) {
    const _root = CID$2.asCID(root);
    if (!_root) {
      throw new TypeError("roots must be a single CID or an array of CIDs");
    }
    _roots.push(_root);
  }
  return _roots;
}

/**
 * @typedef {import('@ipld/unixfs').Block} Block
 */
const code = 0x0202;
/** Byte length of a CBOR encoded CAR header with zero roots. */
const NO_ROOTS_HEADER_LENGTH = 18;
/** @param {import('./types.js').AnyLink} [root] */
function headerEncodingLength(root) {
  if (!root) return NO_ROOTS_HEADER_LENGTH;
  const headerLength = encode$q({ version: 1, roots: [root] }).length;
  const varintLength = varint$3.encodingLength(headerLength);
  return varintLength + headerLength;
}
/** @param {Block} block */
function blockHeaderEncodingLength(block) {
  const payloadLength = block.cid.bytes.length + block.bytes.length;
  const varintLength = varint$3.encodingLength(payloadLength);
  return varintLength + block.cid.bytes.length;
}
/** @param {Block} block */
function blockEncodingLength(block) {
  return blockHeaderEncodingLength(block) + block.bytes.length;
}
/**
 * @param {Iterable<Block> | AsyncIterable<Block>} blocks
 * @param {import('./types.js').AnyLink} [root]
 * @returns {Promise<import('./types.js').CARFile>}
 */
async function encode$2(blocks, root) {
  // @ts-expect-error
  const { writer, out } = CarWriter.create(root);
  /** @type {Error?} */
  let error;
  void (async () => {
    try {
      for await (const block of blocks) {
        await writer.put(block);
      }
    } catch (/** @type {any} */ err) {
      error = err;
    } finally {
      await writer.close();
    }
  })();
  const chunks = [];
  for await (const chunk of out) chunks.push(chunk);
  // @ts-expect-error
  if (error != null) throw error;
  const roots = root != null ? [root] : [];
  return Object.assign(new Blob(chunks), { version: 1, roots });
}
/** @extends {ReadableStream<Block>} */
class BlockStream extends ReadableStream {
  /** @param {import('./types.js').BlobLike} car */
  constructor(car) {
    /** @type {Promise<CarBlockIterator>?} */
    let blocksPromise = null;
    const getBlocksIterable = () => {
      if (blocksPromise) return blocksPromise;
      blocksPromise = CarBlockIterator.fromIterable(toIterable(car.stream()));
      return blocksPromise;
    };
    /** @type {AsyncIterator<Block>?} */
    let iterator = null;
    super({
      async start() {
        const blocks = await getBlocksIterable();
        iterator = /** @type {AsyncIterator<Block>} */ (
          blocks[Symbol.asyncIterator]()
        );
      },
      async pull(controller) {
        /* c8 ignore next */
        if (!iterator) throw new Error("missing blocks iterator");
        const { value, done } = await iterator.next();
        if (done) return controller.close();
        controller.enqueue(value);
      },
    });
    /** @returns {Promise<import('./types.js').AnyLink[]>} */
    this.getRoots = async () => {
      const blocks = await getBlocksIterable();
      return await blocks.getRoots();
    };
  }
}
/* c8 ignore start */
/**
 * {@link ReadableStream} is an async iterable in newer environments, but it's
 * not standard yet. This function normalizes a {@link ReadableStream} to a
 * definite async iterable.
 *
 * @template T
 * @param {ReadableStream<T> | AsyncIterable<T>} stream
 * @returns {AsyncIterable<T>} An async iterable of the contents of the
 *                             {@link stream} (possibly {@link stream} itself).
 */
function toIterable(stream) {
  return Symbol.asyncIterator in stream
    ? stream
    : (async function* () {
        const reader = stream.getReader();
        try {
          while (true) {
            const { done, value } = await reader.read();
            if (done) return;
            yield value;
          }
        } finally {
          reader.releaseLock();
        }
      })();
}
/* c8 ignore end */

/** @type {WeakMap<Uint8Array, string>} */
const cache = new WeakMap();
/** @param {API.MultihashDigest} digest */
const toBase58String = (digest) => {
  let str = cache.get(digest.bytes);
  if (!str) {
    str = base58btc$2.encode(digest.bytes);
    cache.set(digest.bytes, str);
  }
  return str;
};
/**
 * @template {API.MultihashDigest} Key
 * @template Value
 * @implements {Map<Key, Value>}
 */
class DigestMap {
  /** @type {Map<string, [Key, Value]>} */
  #data;
  /**
   * @param {Array<[Key, Value]>} [entries]
   */
  constructor(entries) {
    this.#data = new Map();
    for (const [k, v] of entries ?? []) {
      this.set(k, v);
    }
  }
  get [Symbol.toStringTag]() {
    return "DigestMap";
  }
  clear() {
    this.#data.clear();
  }
  /**
   * @param {Key} key
   * @returns {boolean}
   */
  delete(key) {
    const mhstr = toBase58String(key);
    return this.#data.delete(mhstr);
  }
  /**
   * @param {(value: Value, key: Key, map: Map<Key, Value>) => void} callbackfn
   * @param {any} [thisArg]
   */
  forEach(callbackfn, thisArg) {
    for (const [k, v] of this.#data.values()) {
      callbackfn.call(thisArg, v, k, this);
    }
  }
  /**
   * @param {Key} key
   * @returns {Value|undefined}
   */
  get(key) {
    const data = this.#data.get(toBase58String(key));
    if (data) return data[1];
  }
  /**
   * @param {Key} key
   * @returns {boolean}
   */
  has(key) {
    return this.#data.has(toBase58String(key));
  }
  /**
   * @param {Key} key
   * @param {Value} value
   */
  set(key, value) {
    this.#data.set(toBase58String(key), [key, value]);
    return this;
  }
  /** @returns {number} */
  get size() {
    return this.#data.size;
  }
  /** @returns */
  [Symbol.iterator]() {
    return this.entries();
  }
  /** @returns {IterableIterator<[Key, Value]>} */
  *entries() {
    yield* this.#data.values();
  }
  /** @returns {IterableIterator<Key>} */
  *keys() {
    for (const [k] of this.#data.values()) {
      yield k;
    }
  }
  /** @returns {IterableIterator<Value>} */
  *values() {
    for (const [, v] of this.#data.values()) {
      yield v;
    }
  }
}

const version = "index/sharded/dag@0.1";
variant({
  [version]: struct({
    /** DAG root. */
    content: match$2(),
    /** Shards the DAG can be found in. */
    shards: array(match$2()),
  }),
});
/** @implements {API.ShardedDAGIndexView} */
class ShardedDAGIndex {
  #content;
  #shards;
  /** @param {API.UnknownLink} content */
  constructor(content) {
    this.#content = content;
    /** @type {DigestMap<API.ShardDigest, API.Position>} */
    this.#shards = new DigestMap();
  }
  get content() {
    return this.#content;
  }
  get shards() {
    return this.#shards;
  }
  /**
   * @param {API.ShardDigest} shard
   * @param {API.SliceDigest} slice
   * @param {API.Position} pos
   */
  setSlice(shard, slice, pos) {
    let index = this.#shards.get(shard);
    if (!index) {
      index = new DigestMap();
      this.#shards.set(shard, index);
    }
    index.set(slice, pos);
  }
  archive() {
    return archive$1(this);
  }
}
/**
 * @param {API.UnknownLink} content
 * @returns {API.ShardedDAGIndexView}
 */
const create$2 = (content) => new ShardedDAGIndex(content);
/**
 * @param {API.ShardedDAGIndex} model
 * @returns {Promise<API.Result<Uint8Array>>}
 */
const archive$1 = async (model) => {
  const blocks = new Map();
  const shards = [...model.shards.entries()].sort((a, b) =>
    compare(a[0].digest, b[0].digest)
  );
  const index = {
    content: model.content,
    shards: /** @type {API.Link[]} */ ([]),
  };
  for (const s of shards) {
    const slices = [...s[1].entries()]
      .sort((a, b) => compare(a[0].digest, b[0].digest))
      .map((e) => [e[0].bytes, e[1]]);
    const bytes = encode$q([s[0].bytes, slices]);
    const digest = await sha256$4.digest(bytes);
    const cid = create$g(code$g, digest);
    blocks.set(cid.toString(), { cid, bytes });
    index.shards.push(cid);
  }
  const bytes = encode$q({ [version]: index });
  const digest = await sha256$4.digest(bytes);
  const cid = create$g(code$g, digest);
  return ok(encode$h({ roots: [{ cid, bytes }], blocks }));
};

/**
 * @packageDocumentation
 *
 * A class that lets you do operations over a list of Uint8Arrays without
 * copying them.
 *
 * ```js
 * import { Uint8ArrayList } from 'uint8arraylist'
 *
 * const list = new Uint8ArrayList()
 * list.append(Uint8Array.from([0, 1, 2]))
 * list.append(Uint8Array.from([3, 4, 5]))
 *
 * list.subarray()
 * // -> Uint8Array([0, 1, 2, 3, 4, 5])
 *
 * list.consume(3)
 * list.subarray()
 * // -> Uint8Array([3, 4, 5])
 *
 * // you can also iterate over the list
 * for (const buf of list) {
 *   // ..do something with `buf`
 * }
 *
 * list.subarray(0, 1)
 * // -> Uint8Array([0])
 * ```
 *
 * ## Converting Uint8ArrayLists to Uint8Arrays
 *
 * There are two ways to turn a `Uint8ArrayList` into a `Uint8Array` - `.slice` and `.subarray` and one way to turn a `Uint8ArrayList` into a `Uint8ArrayList` with different contents - `.sublist`.
 *
 * ### slice
 *
 * Slice follows the same semantics as [Uint8Array.slice](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/TypedArray/slice) in that it creates a new `Uint8Array` and copies bytes into it using an optional offset & length.
 *
 * ```js
 * const list = new Uint8ArrayList()
 * list.append(Uint8Array.from([0, 1, 2]))
 * list.append(Uint8Array.from([3, 4, 5]))
 *
 * list.slice(0, 1)
 * // -> Uint8Array([0])
 * ```
 *
 * ### subarray
 *
 * Subarray attempts to follow the same semantics as [Uint8Array.subarray](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/TypedArray/subarray) with one important different - this is a no-copy operation, unless the requested bytes span two internal buffers in which case it is a copy operation.
 *
 * ```js
 * const list = new Uint8ArrayList()
 * list.append(Uint8Array.from([0, 1, 2]))
 * list.append(Uint8Array.from([3, 4, 5]))
 *
 * list.subarray(0, 1)
 * // -> Uint8Array([0]) - no-copy
 *
 * list.subarray(2, 5)
 * // -> Uint8Array([2, 3, 4]) - copy
 * ```
 *
 * ### sublist
 *
 * Sublist creates and returns a new `Uint8ArrayList` that shares the underlying buffers with the original so is always a no-copy operation.
 *
 * ```js
 * const list = new Uint8ArrayList()
 * list.append(Uint8Array.from([0, 1, 2]))
 * list.append(Uint8Array.from([3, 4, 5]))
 *
 * list.sublist(0, 1)
 * // -> Uint8ArrayList([0]) - no-copy
 *
 * list.sublist(2, 5)
 * // -> Uint8ArrayList([2], [3, 4]) - no-copy
 * ```
 *
 * ## Inspiration
 *
 * Borrows liberally from [bl](https://www.npmjs.com/package/bl) but only uses native JS types.
 */
const symbol = Symbol.for("@achingbrain/uint8arraylist");
function findBufAndOffset(bufs, index) {
  if (index == null || index < 0) {
    throw new RangeError("index is out of bounds");
  }
  let offset = 0;
  for (const buf of bufs) {
    const bufEnd = offset + buf.byteLength;
    if (index < bufEnd) {
      return {
        buf,
        index: index - offset,
      };
    }
    offset = bufEnd;
  }
  throw new RangeError("index is out of bounds");
}
/**
 * Check if object is a CID instance
 *
 * @example
 *
 * ```js
 * import { isUint8ArrayList, Uint8ArrayList } from 'uint8arraylist'
 *
 * isUint8ArrayList(true) // false
 * isUint8ArrayList([]) // false
 * isUint8ArrayList(new Uint8ArrayList()) // true
 * ```
 */
function isUint8ArrayList(value) {
  return Boolean(value?.[symbol]);
}
class Uint8ArrayList {
  bufs;
  length;
  [symbol] = true;
  constructor(...data) {
    this.bufs = [];
    this.length = 0;
    if (data.length > 0) {
      this.appendAll(data);
    }
  }
  *[Symbol.iterator]() {
    yield* this.bufs;
  }
  get byteLength() {
    return this.length;
  }
  /**
   * Add one or more `bufs` to the end of this Uint8ArrayList
   */
  append(...bufs) {
    this.appendAll(bufs);
  }
  /**
   * Add all `bufs` to the end of this Uint8ArrayList
   */
  appendAll(bufs) {
    let length = 0;
    for (const buf of bufs) {
      if (buf instanceof Uint8Array) {
        length += buf.byteLength;
        this.bufs.push(buf);
      } else if (isUint8ArrayList(buf)) {
        length += buf.byteLength;
        this.bufs.push(...buf.bufs);
      } else {
        throw new Error(
          "Could not append value, must be an Uint8Array or a Uint8ArrayList"
        );
      }
    }
    this.length += length;
  }
  /**
   * Add one or more `bufs` to the start of this Uint8ArrayList
   */
  prepend(...bufs) {
    this.prependAll(bufs);
  }
  /**
   * Add all `bufs` to the start of this Uint8ArrayList
   */
  prependAll(bufs) {
    let length = 0;
    for (const buf of bufs.reverse()) {
      if (buf instanceof Uint8Array) {
        length += buf.byteLength;
        this.bufs.unshift(buf);
      } else if (isUint8ArrayList(buf)) {
        length += buf.byteLength;
        this.bufs.unshift(...buf.bufs);
      } else {
        throw new Error(
          "Could not prepend value, must be an Uint8Array or a Uint8ArrayList"
        );
      }
    }
    this.length += length;
  }
  /**
   * Read the value at `index`
   */
  get(index) {
    const res = findBufAndOffset(this.bufs, index);
    return res.buf[res.index];
  }
  /**
   * Set the value at `index` to `value`
   */
  set(index, value) {
    const res = findBufAndOffset(this.bufs, index);
    res.buf[res.index] = value;
  }
  /**
   * Copy bytes from `buf` to the index specified by `offset`
   */
  write(buf, offset = 0) {
    if (buf instanceof Uint8Array) {
      for (let i = 0; i < buf.length; i++) {
        this.set(offset + i, buf[i]);
      }
    } else if (isUint8ArrayList(buf)) {
      for (let i = 0; i < buf.length; i++) {
        this.set(offset + i, buf.get(i));
      }
    } else {
      throw new Error(
        "Could not write value, must be an Uint8Array or a Uint8ArrayList"
      );
    }
  }
  /**
   * Remove bytes from the front of the pool
   */
  consume(bytes) {
    // first, normalize the argument, in accordance with how Buffer does it
    bytes = Math.trunc(bytes);
    // do nothing if not a positive number
    if (Number.isNaN(bytes) || bytes <= 0) {
      return;
    }
    // if consuming all bytes, skip iterating
    if (bytes === this.byteLength) {
      this.bufs = [];
      this.length = 0;
      return;
    }
    while (this.bufs.length > 0) {
      if (bytes >= this.bufs[0].byteLength) {
        bytes -= this.bufs[0].byteLength;
        this.length -= this.bufs[0].byteLength;
        this.bufs.shift();
      } else {
        this.bufs[0] = this.bufs[0].subarray(bytes);
        this.length -= bytes;
        break;
      }
    }
  }
  /**
   * Extracts a section of an array and returns a new array.
   *
   * This is a copy operation as it is with Uint8Arrays and Arrays
   * - note this is different to the behaviour of Node Buffers.
   */
  slice(beginInclusive, endExclusive) {
    const { bufs, length } = this._subList(beginInclusive, endExclusive);
    return concat(bufs, length);
  }
  /**
   * Returns a alloc from the given start and end element index.
   *
   * In the best case where the data extracted comes from a single Uint8Array
   * internally this is a no-copy operation otherwise it is a copy operation.
   */
  subarray(beginInclusive, endExclusive) {
    const { bufs, length } = this._subList(beginInclusive, endExclusive);
    if (bufs.length === 1) {
      return bufs[0];
    }
    return concat(bufs, length);
  }
  /**
   * Returns a allocList from the given start and end element index.
   *
   * This is a no-copy operation.
   */
  sublist(beginInclusive, endExclusive) {
    const { bufs, length } = this._subList(beginInclusive, endExclusive);
    const list = new Uint8ArrayList();
    list.length = length;
    // don't loop, just set the bufs
    list.bufs = [...bufs];
    return list;
  }
  _subList(beginInclusive, endExclusive) {
    beginInclusive = beginInclusive ?? 0;
    endExclusive = endExclusive ?? this.length;
    if (beginInclusive < 0) {
      beginInclusive = this.length + beginInclusive;
    }
    if (endExclusive < 0) {
      endExclusive = this.length + endExclusive;
    }
    if (beginInclusive < 0 || endExclusive > this.length) {
      throw new RangeError("index is out of bounds");
    }
    if (beginInclusive === endExclusive) {
      return { bufs: [], length: 0 };
    }
    if (beginInclusive === 0 && endExclusive === this.length) {
      return { bufs: this.bufs, length: this.length };
    }
    const bufs = [];
    let offset = 0;
    for (let i = 0; i < this.bufs.length; i++) {
      const buf = this.bufs[i];
      const bufStart = offset;
      const bufEnd = bufStart + buf.byteLength;
      // for next loop
      offset = bufEnd;
      if (beginInclusive >= bufEnd) {
        // start after this buf
        continue;
      }
      const sliceStartInBuf =
        beginInclusive >= bufStart && beginInclusive < bufEnd;
      const sliceEndsInBuf = endExclusive > bufStart && endExclusive <= bufEnd;
      if (sliceStartInBuf && sliceEndsInBuf) {
        // slice is wholly contained within this buffer
        if (beginInclusive === bufStart && endExclusive === bufEnd) {
          // requested whole buffer
          bufs.push(buf);
          break;
        }
        // requested part of buffer
        const start = beginInclusive - bufStart;
        bufs.push(buf.subarray(start, start + (endExclusive - beginInclusive)));
        break;
      }
      if (sliceStartInBuf) {
        // slice starts in this buffer
        if (beginInclusive === 0) {
          // requested whole buffer
          bufs.push(buf);
          continue;
        }
        // requested part of buffer
        bufs.push(buf.subarray(beginInclusive - bufStart));
        continue;
      }
      if (sliceEndsInBuf) {
        if (endExclusive === bufEnd) {
          // requested whole buffer
          bufs.push(buf);
          break;
        }
        // requested part of buffer
        bufs.push(buf.subarray(0, endExclusive - bufStart));
        break;
      }
      // slice started before this buffer and ends after it
      bufs.push(buf);
    }
    return { bufs, length: endExclusive - beginInclusive };
  }
  indexOf(search, offset = 0) {
    if (!isUint8ArrayList(search) && !(search instanceof Uint8Array)) {
      throw new TypeError(
        'The "value" argument must be a Uint8ArrayList or Uint8Array'
      );
    }
    const needle = search instanceof Uint8Array ? search : search.subarray();
    offset = Number(offset ?? 0);
    if (isNaN(offset)) {
      offset = 0;
    }
    if (offset < 0) {
      offset = this.length + offset;
    }
    if (offset < 0) {
      offset = 0;
    }
    if (search.length === 0) {
      return offset > this.length ? this.length : offset;
    }
    // https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string-search_algorithm
    const M = needle.byteLength;
    if (M === 0) {
      throw new TypeError("search must be at least 1 byte long");
    }
    // radix
    const radix = 256;
    const rightmostPositions = new Int32Array(radix);
    // position of the rightmost occurrence of the byte c in the pattern
    for (let c = 0; c < radix; c++) {
      // -1 for bytes not in pattern
      rightmostPositions[c] = -1;
    }
    for (let j = 0; j < M; j++) {
      // rightmost position for bytes in pattern
      rightmostPositions[needle[j]] = j;
    }
    // Return offset of first match, -1 if no match
    const right = rightmostPositions;
    const lastIndex = this.byteLength - needle.byteLength;
    const lastPatIndex = needle.byteLength - 1;
    let skip;
    for (let i = offset; i <= lastIndex; i += skip) {
      skip = 0;
      for (let j = lastPatIndex; j >= 0; j--) {
        const char = this.get(i + j);
        if (needle[j] !== char) {
          skip = Math.max(1, j - right[char]);
          break;
        }
      }
      if (skip === 0) {
        return i;
      }
    }
    return -1;
  }
  getInt8(byteOffset) {
    const buf = this.subarray(byteOffset, byteOffset + 1);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    return view.getInt8(0);
  }
  setInt8(byteOffset, value) {
    const buf = allocUnsafe(1);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    view.setInt8(0, value);
    this.write(buf, byteOffset);
  }
  getInt16(byteOffset, littleEndian) {
    const buf = this.subarray(byteOffset, byteOffset + 2);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    return view.getInt16(0, littleEndian);
  }
  setInt16(byteOffset, value, littleEndian) {
    const buf = alloc(2);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    view.setInt16(0, value, littleEndian);
    this.write(buf, byteOffset);
  }
  getInt32(byteOffset, littleEndian) {
    const buf = this.subarray(byteOffset, byteOffset + 4);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    return view.getInt32(0, littleEndian);
  }
  setInt32(byteOffset, value, littleEndian) {
    const buf = alloc(4);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    view.setInt32(0, value, littleEndian);
    this.write(buf, byteOffset);
  }
  getBigInt64(byteOffset, littleEndian) {
    const buf = this.subarray(byteOffset, byteOffset + 8);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    return view.getBigInt64(0, littleEndian);
  }
  setBigInt64(byteOffset, value, littleEndian) {
    const buf = alloc(8);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    view.setBigInt64(0, value, littleEndian);
    this.write(buf, byteOffset);
  }
  getUint8(byteOffset) {
    const buf = this.subarray(byteOffset, byteOffset + 1);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    return view.getUint8(0);
  }
  setUint8(byteOffset, value) {
    const buf = allocUnsafe(1);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    view.setUint8(0, value);
    this.write(buf, byteOffset);
  }
  getUint16(byteOffset, littleEndian) {
    const buf = this.subarray(byteOffset, byteOffset + 2);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    return view.getUint16(0, littleEndian);
  }
  setUint16(byteOffset, value, littleEndian) {
    const buf = alloc(2);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    view.setUint16(0, value, littleEndian);
    this.write(buf, byteOffset);
  }
  getUint32(byteOffset, littleEndian) {
    const buf = this.subarray(byteOffset, byteOffset + 4);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    return view.getUint32(0, littleEndian);
  }
  setUint32(byteOffset, value, littleEndian) {
    const buf = alloc(4);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    view.setUint32(0, value, littleEndian);
    this.write(buf, byteOffset);
  }
  getBigUint64(byteOffset, littleEndian) {
    const buf = this.subarray(byteOffset, byteOffset + 8);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    return view.getBigUint64(0, littleEndian);
  }
  setBigUint64(byteOffset, value, littleEndian) {
    const buf = alloc(8);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    view.setBigUint64(0, value, littleEndian);
    this.write(buf, byteOffset);
  }
  getFloat32(byteOffset, littleEndian) {
    const buf = this.subarray(byteOffset, byteOffset + 4);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    return view.getFloat32(0, littleEndian);
  }
  setFloat32(byteOffset, value, littleEndian) {
    const buf = alloc(4);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    view.setFloat32(0, value, littleEndian);
    this.write(buf, byteOffset);
  }
  getFloat64(byteOffset, littleEndian) {
    const buf = this.subarray(byteOffset, byteOffset + 8);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    return view.getFloat64(0, littleEndian);
  }
  setFloat64(byteOffset, value, littleEndian) {
    const buf = alloc(8);
    const view = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    view.setFloat64(0, value, littleEndian);
    this.write(buf, byteOffset);
  }
  equals(other) {
    if (other == null) {
      return false;
    }
    if (!(other instanceof Uint8ArrayList)) {
      return false;
    }
    if (other.bufs.length !== this.bufs.length) {
      return false;
    }
    for (let i = 0; i < this.bufs.length; i++) {
      if (!equals$4(this.bufs[i], other.bufs[i])) {
        return false;
      }
    }
    return true;
  }
  /**
   * Create a Uint8ArrayList from a pre-existing list of Uint8Arrays.  Use this
   * method if you know the total size of all the Uint8Arrays ahead of time.
   */
  static fromUint8Arrays(bufs, length) {
    const list = new Uint8ArrayList();
    list.bufs = bufs;
    if (length == null) {
      length = bufs.reduce((acc, curr) => acc + curr.byteLength, 0);
    }
    list.length = length;
    return list;
  }
}
/*
function indexOf (needle: Uint8Array, haystack: Uint8Array, offset = 0) {
  for (let i = offset; i < haystack.byteLength; i++) {
    for (let j = 0; j < needle.length; j++) {
      if (haystack[i + j] !== needle[j]) {
        break
      }

      if (j === needle.byteLength -1) {
        return i
      }
    }

    if (haystack.byteLength - i < needle.byteLength) {
      break
    }
  }

  return -1
}
*/

const MSB$2 = 0x80;
const REST = 0x7f;
const MSBALL$1 = -128;
const INT$1 = Math.pow(2, 31);

/**
 * @param {number} num
 */
const encode$1 = (num) => {
  /** @type {number[]} */
  const out = [];
  let offset = 0;

  while (num >= INT$1) {
    out[offset++] = (num & 0xff) | MSB$2;
    num /= 128;
  }
  while (num & MSBALL$1) {
    out[offset++] = (num & 0xff) | MSB$2;
    num >>>= 7;
  }
  out[offset] = num | 0;

  return out;
};

/**
 * @param {import('uint8arraylist').Uint8ArrayList} buf
 * @param {number} [offset]
 */
const decode$1 = (buf, offset) => {
  let res = 0;
  offset = offset || 0;
  let shift = 0;
  let counter = offset;
  let b;
  const l = buf.length;

  do {
    if (counter >= l || shift > 49)
      throw new RangeError("Could not decode varint");
    b = buf.get(counter++);
    res += shift < 28 ? (b & REST) << shift : (b & REST) * Math.pow(2, shift);
    shift += 7;
  } while (b >= MSB$2);

  return [res, counter - offset];
};

/* eslint-env browser */

const State = {
  ReadHeaderLength: 0,
  ReadHeader: 1,
  ReadBlockLength: 2,
  ReadBlock: 3,
};

const CIDV0_BYTES = {
  SHA2_256: 0x12,
  LENGTH: 0x20,
};

/** @extends {TransformStream<Uint8Array, import('./api.js').Block & import('./api.js').Position>} */
class CARReaderStream extends TransformStream {
  /** @type {Promise<import('./api.js').CARHeader>} */
  #headerPromise;

  /**
   * @param {QueuingStrategy<Uint8Array>} [writableStrategy]
   * An object that optionally defines a queuing strategy for the stream.
   * @param {QueuingStrategy<import('./api.js').Block & import('./api.js').Position>} [readableStrategy]
   * An object that optionally defines a queuing strategy for the stream.
   * Defaults to a CountQueuingStrategy with highWaterMark of `1` to allow
   * `getHeader` to be called before the stream is consumed.
   */
  constructor(writableStrategy, readableStrategy) {
    const buffer = new Uint8ArrayList();
    let offset = 0;
    let prevOffset = offset;
    let wanted = 8;
    let state = State.ReadHeaderLength;

    /** @type {(value: import('./api.js').CARHeader) => void} */
    let resolveHeader;
    const headerPromise = new Promise((resolve) => {
      resolveHeader = resolve;
    });

    super(
      {
        transform(chunk, controller) {
          buffer.append(chunk);
          while (true) {
            if (buffer.length < wanted) break;
            if (state === State.ReadHeaderLength) {
              const [length, bytes] = decode$1(buffer);
              buffer.consume(bytes);
              prevOffset = offset;
              offset += bytes;
              state = State.ReadHeader;
              wanted = length;
            } else if (state === State.ReadHeader) {
              const header = decode$y(buffer.slice(0, wanted));
              resolveHeader && resolveHeader(header);
              buffer.consume(wanted);
              prevOffset = offset;
              offset += wanted;
              state = State.ReadBlockLength;
              wanted = 8;
            } else if (state === State.ReadBlockLength) {
              const [length, bytes] = decode$1(buffer);
              buffer.consume(bytes);
              prevOffset = offset;
              offset += bytes;
              state = State.ReadBlock;
              wanted = length;
            } else if (state === State.ReadBlock) {
              const _offset = prevOffset;
              const length = offset - prevOffset + wanted;

              prevOffset = offset;
              /** @type {import('multiformats').UnknownLink} */
              let cid;
              if (
                buffer.get(0) === CIDV0_BYTES.SHA2_256 &&
                buffer.get(1) === CIDV0_BYTES.LENGTH
              ) {
                const bytes = buffer.subarray(0, 34);
                const multihash = decode$z(bytes);
                // @ts-expect-error
                cid = createLegacy(multihash);
                buffer.consume(34);
                offset += 34;
              } else {
                const [version, versionBytes] = decode$1(buffer);
                if (version !== 1)
                  throw new Error(`unexpected CID version (${version})`);
                buffer.consume(versionBytes);
                offset += versionBytes;

                const [codec, codecBytes] = decode$1(buffer);
                buffer.consume(codecBytes);
                offset += codecBytes;

                const multihashBytes = getMultihashLength(buffer);
                const multihash = decode$z(buffer.subarray(0, multihashBytes));
                cid = create$g(codec, multihash);
                buffer.consume(multihashBytes);
                offset += multihashBytes;
              }

              const blockBytes = wanted - (offset - prevOffset);
              const bytes = buffer.subarray(0, blockBytes);
              controller.enqueue({
                cid,
                bytes,
                offset: _offset,
                length,
                blockOffset: offset,
                blockLength: blockBytes,
              });

              buffer.consume(blockBytes);
              prevOffset = offset;
              offset += blockBytes;
              state = State.ReadBlockLength;
              wanted = 8;
            }
          }
        },
        flush(controller) {
          if (state !== State.ReadBlockLength) {
            controller.error(new Error("unexpected end of data"));
          }
        },
      },
      writableStrategy,
      readableStrategy ?? new CountQueuingStrategy({ highWaterMark: 1 })
    );

    this.#headerPromise = headerPromise;
  }

  /**
   * Get the decoded CAR header.
   */
  getHeader() {
    return this.#headerPromise;
  }
}

/** @param {Uint8ArrayList} bytes */
const getMultihashLength = (bytes) => {
  const [, codeBytes] = decode$1(bytes);
  const [length, lengthBytes] = decode$1(bytes, codeBytes);
  return codeBytes + lengthBytes + length;
};

/* eslint-env browser */

/**
 * @param {import('multiformats').UnknownLink[]} roots
 * @returns {Uint8Array}
 */
const encodeHeader = (roots) => {
  const headerBytes = encode$q({ version: 1, roots });
  const varintBytes = encode$1(headerBytes.length);
  const header = new Uint8Array(varintBytes.length + headerBytes.length);
  header.set(varintBytes, 0);
  header.set(headerBytes, varintBytes.length);
  return header;
};

/**
 * @param {import('./api.js').Block} block
 * @returns {Uint8Array}
 */
const encodeBlock = (block) => {
  const varintBytes = encode$1(block.cid.bytes.length + block.bytes.length);
  const bytes = new Uint8Array(
    varintBytes.length + block.cid.bytes.length + block.bytes.length
  );
  bytes.set(varintBytes);
  bytes.set(block.cid.bytes, varintBytes.length);
  bytes.set(block.bytes, varintBytes.length + block.cid.bytes.length);
  return bytes;
};

/** @extends {TransformStream<import('./api.js').Block, Uint8Array>} */
class CARWriterStream extends TransformStream {
  /**
   * @param {import('multiformats').UnknownLink[]} [roots]
   * @param {QueuingStrategy<import('./api.js').Block>} [writableStrategy]
   * @param {QueuingStrategy<Uint8Array>} [readableStrategy]
   */
  constructor(roots = [], writableStrategy, readableStrategy) {
    super(
      {
        start: (controller) => controller.enqueue(encodeHeader(roots)),
        transform: (block, controller) =>
          controller.enqueue(encodeBlock(block)),
      },
      writableStrategy,
      readableStrategy
    );
  }
}

/**
 * Indexes a sharded DAG
 *
 * @param {import('multiformats').Link} root
 * @param {import('@web3-storage/capabilities/types').CARLink[]} shards
 * @param {Array<Map<API.SliceDigest, API.Position>>} shardIndexes
 */
async function indexShardedDAG(root, shards, shardIndexes) {
  const index = create$2(root);
  for (const [i, shard] of shards.entries()) {
    const slices = shardIndexes[i];
    index.shards.set(shard.multihash, slices);
  }
  return await index.archive();
}

/**
 * @typedef {import('./types.js').FileLike} FileLike
 */
// https://observablehq.com/@gozala/w3up-shard-size
const SHARD_SIZE = 133169152;
/**
 * Shard a set of blocks into a set of CAR files. By default the last block
 * received is assumed to be the DAG root and becomes the CAR root CID for the
 * last CAR output. Set the `rootCID` option to override.
 *
 * @extends {TransformStream<import('@ipld/unixfs').Block, import('./types.js').IndexedCARFile>}
 */
class ShardingStream extends TransformStream {
  /**
   * @param {import('./types.js').ShardingOptions} [options]
   */
  constructor(options = {}) {
    const shardSize = options.shardSize ?? SHARD_SIZE;
    const maxBlockLength = shardSize - headerEncodingLength();
    /** @type {import('@ipld/unixfs').Block[]} */
    let blocks = [];
    /** @type {import('@ipld/unixfs').Block[] | null} */
    let readyBlocks = null;
    /** @type {Map<import('./types.js').SliceDigest, import('./types.js').Position>} */
    let slices = new DigestMap();
    /** @type {Map<import('./types.js').SliceDigest, import('./types.js').Position> | null} */
    let readySlices = null;
    let currentLength = 0;
    super({
      async transform(block, controller) {
        if (readyBlocks != null && readySlices != null) {
          controller.enqueue(await encodeCAR(readyBlocks, readySlices));
          readyBlocks = null;
          readySlices = null;
        }
        const blockHeaderLength = blockHeaderEncodingLength(block);
        const blockLength = blockHeaderLength + block.bytes.length;
        if (blockLength > maxBlockLength) {
          throw new Error(
            `block will cause CAR to exceed shard size: ${block.cid}`
          );
        }
        if (blocks.length && currentLength + blockLength > maxBlockLength) {
          readyBlocks = blocks;
          readySlices = slices;
          blocks = [];
          slices = new DigestMap();
          currentLength = 0;
        }
        blocks.push(block);
        slices.set(block.cid.multihash, [
          headerEncodingLength() + currentLength + blockHeaderLength,
          block.bytes.length,
        ]);
        currentLength += blockLength;
      },
      async flush(controller) {
        if (readyBlocks != null && readySlices != null) {
          controller.enqueue(await encodeCAR(readyBlocks, readySlices));
        }
        const rootBlock = blocks.at(-1);
        if (rootBlock == null) return;
        const rootCID = options.rootCID ?? rootBlock.cid;
        const headerLength = headerEncodingLength(rootCID);
        // if adding CAR root overflows the shard limit we move overflowing
        // blocks into another CAR.
        if (headerLength + currentLength > shardSize) {
          const overage = headerLength + currentLength - shardSize;
          const overflowBlocks = [];
          let overflowCurrentLength = 0;
          while (overflowCurrentLength < overage) {
            const block = blocks[blocks.length - 1];
            blocks.pop();
            slices.delete(block.cid.multihash);
            overflowBlocks.unshift(block);
            overflowCurrentLength += blockEncodingLength(block);
            // need at least 1 block in original shard
            if (blocks.length < 1)
              throw new Error(
                `block will cause CAR to exceed shard size: ${block.cid}`
              );
          }
          controller.enqueue(await encodeCAR(blocks, slices));
          // Finally, re-calc block positions from blocks we moved out of the
          // CAR that was too big.
          overflowCurrentLength = 0;
          /** @type {Map<import('./types.js').SliceDigest, import('./types.js').Position>} */
          const overflowSlices = new DigestMap();
          for (const block of overflowBlocks) {
            const overflowBlockHeaderLength = blockHeaderEncodingLength(block);
            overflowSlices.set(block.cid.multihash, [
              headerLength + overflowCurrentLength + overflowBlockHeaderLength,
              block.bytes.length,
            ]);
            overflowCurrentLength +=
              overflowBlockHeaderLength + block.bytes.length;
          }
          controller.enqueue(
            await encodeCAR(overflowBlocks, overflowSlices, rootCID)
          );
        } else {
          // adjust offsets for longer header in final shard
          const diff = headerLength - headerEncodingLength();
          for (const slice of slices.values()) {
            slice[0] += diff;
          }
          controller.enqueue(await encodeCAR(blocks, slices, rootCID));
        }
      },
    });
  }
}
/**
 * Default comparator for FileLikes. Sorts by file name in ascending order.
 *
 * @param {FileLike} a
 * @param {FileLike} b
 * @param {(file: FileLike) => string} getComparedValue - given a file being sorted, return the value by which its order should be determined, if it is different than the file object itself (e.g. file.name)
 */
const defaultFileComparator = (
  a,
  b,
  getComparedValue = (file) => file.name
) => {
  return ascending(a, b, getComparedValue);
};
/**
 * a comparator for sorting in ascending order. Use with Sorted or Array#sort.
 *
 * @template T
 * @param {T} a
 * @param {T} b
 * @param {(i: T) => any} getComparedValue - given an item being sorted, return the value by which it should be sorted, if it is different than the item
 */
function ascending(a, b, getComparedValue) {
  const ask = getComparedValue(a);
  const bsk = getComparedValue(b);
  if (ask === bsk) return 0;
  else if (ask < bsk) return -1;
  return 1;
}
/**
 * @param {Iterable<import('@ipld/unixfs').Block>} blocks
 * @param {Map<import('./types.js').SliceDigest, import('./types.js').Position>} slices
 * @param {import('./types.js').AnyLink} [root]
 * @returns {Promise<import('./types.js').IndexedCARFile>}
 */
const encodeCAR = async (blocks, slices, root) =>
  Object.assign(await encode$2(blocks, root), { slices });

/**
 * Uploads a file to the service and returns the root data CID for the
 * generated DAG.
 *
 * Required delegated capability proofs: `blob/add`, `index/add`,
 * `filecoin/offer`, `upload/add`
 *
 * @param {import('./types.js').InvocationConfig|import('./types.js').InvocationConfigurator} conf Configuration
 * for the UCAN invocation. An object with `issuer`, `with` and `proofs`, or a
 * function that generates this object.
 *
 * The `issuer` is the signing authority that is issuing the UCAN
 * invocation(s). It is typically the user _agent_.
 *
 * The `with` is the resource the invocation applies to. It is typically the
 * DID of a space.
 *
 * The `proofs` are a set of capability delegations that prove the issuer
 * has the capability to perform the action.
 *
 * The issuer needs the `blob/add`, `index/add`, `filecoin/offer` and
 * `upload/add` delegated capability.
 * @param {import('./types.js').BlobLike} file File data.
 * @param {import('./types.js').UploadFileOptions} [options]
 */
async function uploadFile(conf, file, options = {}) {
  return await uploadBlockStream(
    conf,
    createFileEncoderStream(file, options),
    options
  );
}
/**
 * Uploads a directory of files to the service and returns the root data CID
 * for the generated DAG. All files are added to a container directory, with
 * paths in file names preserved.
 *
 * Required delegated capability proofs: `blob/add`, `index/add`,
 * `filecoin/offer`, `upload/add`
 *
 * @param {import('./types.js').InvocationConfig|import('./types.js').InvocationConfigurator} conf Configuration
 * for the UCAN invocation. An object with `issuer`, `with` and `proofs`, or a
 * function that generates this object
 *
 * The `issuer` is the signing authority that is issuing the UCAN
 * invocation(s). It is typically the user _agent_.
 *
 * The `with` is the resource the invocation applies to. It is typically the
 * DID of a space.
 *
 * The `proofs` are a set of capability delegations that prove the issuer
 * has the capability to perform the action.
 *
 * The issuer needs the `blob/add`, `index/add`, `filecoin/offer` and
 * `upload/add` delegated capability.
 * @param {import('./types.js').FileLike[]} files  Files that should be in the directory.
 * To ensure determinism in the IPLD encoding, files are automatically sorted by `file.name`.
 * To retain the order of the files as passed in the array, set `customOrder` option to `true`.
 * @param {import('./types.js').UploadDirectoryOptions} [options]
 */
async function uploadDirectory(conf, files, options = {}) {
  const { customOrder = false } = options;
  const entries = customOrder ? files : [...files].sort(defaultFileComparator);
  return await uploadBlockStream(
    conf,
    createDirectoryEncoderStream(entries, options),
    options
  );
}
/**
 * Uploads a CAR file to the service.
 *
 * The difference between this function and `Store.add` is that the CAR file is
 * automatically sharded and an "upload" is registered, linking the individual
 * shards (see `Upload.add`).
 *
 * Use the `onShardStored` callback to obtain the CIDs of the CAR file shards.
 *
 * Required delegated capability proofs: `blob/add`, `index/add`,
 * `filecoin/offer`, `upload/add`
 *
 * @param {import('./types.js').InvocationConfig|import('./types.js').InvocationConfigurator} conf Configuration
 * for the UCAN invocation. An object with `issuer`, `with` and `proofs`, or a
 * function that generates this object
 *
 * The `issuer` is the signing authority that is issuing the UCAN
 * invocation(s). It is typically the user _agent_.
 *
 * The `with` is the resource the invocation applies to. It is typically the
 * DID of a space.
 *
 * The `proofs` are a set of capability delegations that prove the issuer
 * has the capability to perform the action.
 *
 * The issuer needs the `blob/add`, `index/add`, `filecoin/offer` and `upload/add` delegated capability.
 * @param {import('./types.js').BlobLike} car CAR file.
 * @param {import('./types.js').UploadOptions} [options]
 */
async function uploadCAR(conf, car, options = {}) {
  const blocks = new BlockStream(car);
  options.rootCID = options.rootCID ?? (await blocks.getRoots())[0];
  return await uploadBlockStream(conf, blocks, options);
}
/**
 * @param {import('./types.js').InvocationConfig|import('./types.js').InvocationConfigurator} conf
 * @param {ReadableStream<import('@ipld/unixfs').Block>} blocks
 * @param {import('./types.js').UploadOptions} [options]
 * @returns {Promise<import('./types.js').AnyLink>}
 */
async function uploadBlockStream(
  conf,
  blocks,
  { pieceHasher = PieceHasher, ...options } = {}
) {
  /** @type {import('./types.js').InvocationConfigurator} */
  const configure = typeof conf === "function" ? conf : () => conf;
  /** @type {Array<Map<import('./types.js').SliceDigest, import('./types.js').Position>>} */
  const shardIndexes = [];
  /** @type {import('./types.js').CARLink[]} */
  const shards = [];
  /** @type {import('./types.js').AnyLink?} */
  let root = null;
  await blocks
    .pipeThrough(new ShardingStream(options))
    .pipeThrough(
      /** @type {TransformStream<import('./types.js').IndexedCARFile, import('./types.js').CARMetadata>} */
      (
        new TransformStream({
          async transform(car, controller) {
            const bytes = new Uint8Array(await car.arrayBuffer());
            const digest = await sha256$2.digest(bytes);
            const conf = await configure([
              {
                can: ability$2,
                nb: input$8(digest, bytes.length),
              },
            ]);
            // Invoke blob/add and write bytes to write target
            await add$2(conf, digest, bytes, options);
            const cid = create$9(code, digest);
            let piece;
            if (pieceHasher) {
              const multihashDigest = await pieceHasher.digest(bytes);
              /** @type {import('@web3-storage/capabilities/types').PieceLink} */
              piece = create$9(code$3, multihashDigest);
              const content = create$9(code$3, digest);
              // Invoke filecoin/offer for data
              const result = await filecoinOffer(
                {
                  issuer: conf.issuer,
                  audience: conf.audience,
                  // Resource of invocation is the issuer did for being self issued
                  with: conf.issuer.did(),
                  proofs: conf.proofs,
                },
                content,
                piece,
                options
              );
              if (result.out.error) {
                throw new Error(
                  "failed to offer piece for aggregation into filecoin deal",
                  { cause: result.out.error }
                );
              }
            }
            const { version, roots, size, slices } = car;
            controller.enqueue({ version, roots, size, cid, piece, slices });
          },
        })
      )
    )
    .pipeTo(
      new WritableStream({
        write(meta) {
          root = root || meta.roots[0];
          shards.push(meta.cid);
          // add the CAR shard itself to the slices
          meta.slices.set(meta.cid.multihash, [0, meta.size]);
          shardIndexes.push(meta.slices);
          if (options.onShardStored) options.onShardStored(meta);
        },
      })
    );
  /* c8 ignore next */
  if (!root) throw new Error("missing root CID");
  const indexBytes = await indexShardedDAG(root, shards, shardIndexes);
  /* c8 ignore next 3 */
  if (!indexBytes.ok) {
    throw new Error("failed to archive DAG index", { cause: indexBytes.error });
  }
  const indexDigest = await sha256$2.digest(indexBytes.ok);
  const indexLink = create$9(code, indexDigest);
  const [blobAddConf, indexAddConf, uploadAddConf] = await Promise.all([
    configure([
      {
        can: ability$2,
        nb: input$8(indexDigest, indexBytes.ok.length),
      },
    ]),
    configure([
      {
        can: ability$1,
        nb: input$4(indexLink),
      },
    ]),
    configure([
      {
        can: ability,
        nb: input$3(root, shards),
      },
    ]),
  ]);
  // Store the index in the space
  await add$2(blobAddConf, indexDigest, indexBytes.ok, options);
  // Register the index with the service
  await add$1(indexAddConf, indexLink, options);
  // Register an upload with the service
  await add(uploadAddConf, root, shards, options);
  return root;
}

const accessServiceURL = new URL("https://up.web3.storage");
const accessServicePrincipal = parse$1("did:web:web3.storage");
const accessServiceConnection = connect({
  id: accessServicePrincipal,
  codec: outbound,
  channel: open$2({
    url: accessServiceURL,
    method: "POST",
  }),
});
const uploadServiceURL = new URL("https://up.web3.storage");
const uploadServicePrincipal = parse$1("did:web:web3.storage");
const uploadServiceConnection = connect({
  id: uploadServicePrincipal,
  codec: outbound,
  channel: open$2({
    url: uploadServiceURL,
    method: "POST",
  }),
});
const filecoinServiceURL = new URL("https://up.web3.storage");
const filecoinServicePrincipal = parse$1("did:web:web3.storage");
const filecoinServiceConnection = connect({
  id: filecoinServicePrincipal,
  codec: outbound,
  channel: open$2({
    url: filecoinServiceURL,
    method: "POST",
  }),
});
/** @type {import('./types.js').ServiceConf} */
const serviceConf = {
  access: accessServiceConnection,
  upload: uploadServiceConnection,
  filecoin: filecoinServiceConnection,
};

class Base {
  /**
   * @type {Agent}
   * @protected
   */
  _agent;
  /**
   * @type {import('./types.js').ServiceConf}
   * @protected
   */
  _serviceConf;
  /**
   * @param {import('@web3-storage/access').AgentData} agentData
   * @param {object} [options]
   * @param {import('./types.js').ServiceConf} [options.serviceConf]
   * @param {URL} [options.receiptsEndpoint]
   */
  constructor(agentData, options = {}) {
    this._serviceConf = options.serviceConf ?? serviceConf;
    this._agent = new Agent(agentData, {
      servicePrincipal: this._serviceConf.access.id,
      // @ts-expect-error I know but it will be HTTP for the forseeable.
      url: this._serviceConf.access.channel.url,
      connection: this._serviceConf.access,
    });
    this._receiptsEndpoint = options.receiptsEndpoint ?? receiptsEndpoint;
  }
  /**
   * The current user agent (this device).
   *
   * @type {Agent}
   */
  get agent() {
    return this._agent;
  }
  /**
   * @protected
   * @param {import('./types.js').Ability[]} abilities
   */
  async _invocationConfig(abilities) {
    const resource = this._agent.currentSpace();
    if (!resource) {
      throw new Error(
        "missing current space: use createSpace() or setCurrentSpace()"
      );
    }
    const issuer = this._agent.issuer;
    const proofs = await this._agent.proofs(
      abilities.map((can) => ({ can, with: resource }))
    );
    const audience = this._serviceConf.upload.id;
    return { issuer, with: resource, proofs, audience };
  }
}

/**
 * Returns contained `ok` if result is and throws `error` if result is not ok.
 *
 * @template T
 * @param {API.Result<T, {}>} result
 * @returns {T}
 */
const unwrap = ({ ok, error }) => {
  if (error) {
    throw error;
  } else {
    return /** @type {T} */ (ok);
  }
};

/**
 * Client for interacting with the `access/*` capabilities.
 */
class AccessClient extends Base {
  /* c8 ignore start - testing websocket code is hard */
  /**
   * Authorize the current agent to use capabilities granted to the passed
   * email account.
   *
   * @deprecated Use `request` instead.
   *
   * @param {`${string}@${string}`} email
   * @param {object} [options]
   * @param {AbortSignal} [options.signal]
   * @param {Iterable<{ can: API.Ability }>} [options.capabilities]
   */
  async authorize(email, options) {
    const account = fromEmail(email);
    const authorization = unwrap(await request(this, { account }));
    const access = unwrap(await authorization.claim(options));
    await unwrap(await access.save());
    return access.proofs;
  }
  /* c8 ignore stop */
  /**
   * Claim delegations granted to the account associated with this agent.
   *
   * @param {object} [input]
   * @param {API.DID} [input.audience]
   */
  async claim(input) {
    const access = unwrap(await claim(this, input));
    await unwrap(await access.save());
    return access.proofs;
  }
  /**
   * Requests specified `access` level from the account from the given account.
   *
   * @param {object} input
   * @param {API.AccountDID} input.account
   * @param {API.Access} [input.access]
   * @param {AbortSignal} [input.signal]
   */
  async request(input) {
    return await request(this, input);
  }
  /**
   * Shares access with delegates.
   *
   * @param {object} input
   * @param {API.Delegation[]} input.delegations
   * @param {API.SpaceDID} [input.space]
   * @param {API.Delegation[]} [input.proofs]
   */
  async delegate(input) {
    return await delegate(this, input);
  }
}
/**
 * @param {{agent: API.Agent}} client
 * @param {object} [input]
 * @param {API.DID} [input.audience]
 */
const claim = async ({ agent }, input) => claim$1(agent, input);
/**
 * Requests specified `access` level from specified `account`. It will invoke
 * `access/authorize` capability and keep polling `access/claim` capability
 * until access is granted or request is aborted.
 *
 * @param {{agent: API.Agent}} agent
 * @param {object} input
 * @param {API.AccountDID} input.account
 * @param {API.Access} [input.access]
 * @param {API.DID} [input.audience]
 */
const request = async ({ agent }, input) => request$2(agent, input);
/**
 *
 * @param {{agent: API.Agent}} agent
 * @param {object} input
 * @param {API.Delegation[]} input.delegations
 * @param {API.SpaceDID} [input.space]
 * @param {API.Delegation[]} [input.proofs]
 */
const delegate = async ({ agent }, input) => delegate$1(agent, input);
const { accountAccess } = access;

class PlanClient extends Base {
  /**
   * Required delegated capabilities:
   * - `plan/get`
   *
   * @param {import('@web3-storage/access').AccountDID} account
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async get(account, options) {
    const out = await get({ agent: this.agent }, { ...options, account });
    if (!out.ok) {
      throw new Error(`failed ${get$c.can} invocation`, {
        cause: out.error,
      });
    }
    return out.ok;
  }
  /**
   * Required delegated capabilities:
   * - `plan/set`
   *
   * @param {API.AccountDID} account
   * @param {API.DID} product
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async set(account, product, options) {
    const out = await set(
      { agent: this.agent },
      { ...options, account, product }
    );
    if (!out.ok) {
      throw new Error(`failed ${set$5.can} invocation`, {
        cause: out.error,
      });
    }
    return out.ok;
  }
  /**
   *
   * @param {API.AccountDID} account
   * @param {string} returnURL
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async createAdminSession(account, returnURL, options) {
    const out = await createAdminSession(
      { agent: this.agent },
      { ...options, account, returnURL }
    );
    if (!out.ok) {
      throw new Error(`failed ${createAdminSession$1.can} invocation`, {
        cause: out.error,
      });
    }
    return out.ok;
  }
}
/**
 * Gets the plan currently associated with the account.
 *
 * @param {{agent: API.Agent}} client
 * @param {object} options
 * @param {API.AccountDID} options.account
 * @param {string} [options.nonce]
 * @param {API.Delegation[]} [options.proofs]
 */
const get = async ({ agent }, { account, nonce, proofs = [] }) => {
  const receipt = await agent.invokeAndExecute(get$c, {
    with: account,
    proofs,
    nonce,
  });
  return receipt.out;
};
/**
 * Sets the plan currently associated with the account.
 *
 * @param {{agent: API.Agent}} client
 * @param {object} options
 * @param {API.DID} options.product
 * @param {API.AccountDID} options.account
 * @param {string} [options.nonce]
 * @param {API.Delegation[]} [options.proofs]
 */
const set = async ({ agent }, { account, product, nonce, proofs = [] }) => {
  const receipt = await agent.invokeAndExecute(set$5, {
    with: account,
    nb: { product },
    nonce,
    proofs,
  });
  return receipt.out;
};
/**
 * Creates an admin session for the given account.
 *
 * Returns a URL that a user can resolve to enter the
 * admin billing portal for this account.
 *
 * @param {{agent: API.Agent}} client
 * @param {object} options
 * @param {API.AccountDID} options.account
 * @param {string} options.returnURL
 * @param {string} [options.nonce]
 * @param {API.Delegation[]} [options.proofs]
 */
const createAdminSession = async (
  { agent },
  { account, returnURL, nonce, proofs = [] }
) => {
  const receipt = await agent.invokeAndExecute(createAdminSession$1, {
    with: account,
    proofs,
    nonce,
    nb: {
      returnURL,
    },
  });
  return receipt.out;
};

/**
 * Client for interacting with the `subscription/*` capabilities.
 */
class SubscriptionClient extends Base {
  /**
   * List subscriptions for the passed account.
   *
   * Required delegated capabilities:
   * - `subscription/list`
   *
   * @param {import('@web3-storage/access').AccountDID} account
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  /* c8 ignore next */
  async list(account, options) {
    const out = await list$1({ agent: this.agent }, { ...options, account });
    /* c8 ignore next 8 */
    if (!out.ok) {
      throw new Error(`failed ${list$6.can} invocation`, {
        cause: out.error,
      });
    }
    return out.ok;
  }
}
/**
 * Gets subscriptions associated with the account.
 *
 * @param {{agent: API.Agent}} client
 * @param {object} options
 * @param {API.AccountDID} options.account
 * @param {string} [options.nonce]
 * @param {API.Delegation[]} [options.proofs]
 */
const list$1 = async ({ agent }, { account, nonce, proofs = [] }) => {
  const receipt = await agent.invokeAndExecute(list$6, {
    with: account,
    proofs,
    nb: {},
    nonce,
  });
  return receipt.out;
};

/**
 * @typedef {import('@web3-storage/did-mailto').EmailAddress} EmailAddress
 */
/**
 * List all accounts that agent has stored access to. Returns a dictionary
 * of accounts keyed by their `did:mailto` identifier.
 *
 * @param {{agent: API.Agent}} client
 * @param {object} query
 * @param {API.DID<'mailto'>} [query.account]
 */
const list = ({ agent }, { account } = {}) => {
  const query = /** @type {API.CapabilityQuery} */ ({
    with: account ?? /did:mailto:.*/,
    can: "*",
  });
  const proofs = agent.proofs([query]);
  /** @type {Record<API.DidMailto, Account>} */
  const accounts = {};
  /** @type {Record<string, API.Delegation>} */
  const attestations = {};
  for (const proof of proofs) {
    const access = allows(proof);
    for (const [resource, abilities] of Object.entries(access)) {
      if (AccountDID.is(resource) && abilities["*"]) {
        const id = /** @type {API.DidMailto} */ (resource);
        const account =
          accounts[id] ||
          (accounts[id] = new Account({ id, agent, proofs: [] }));
        account.addProof(proof);
      }
      for (const settings of /** @type {{proof?:API.Link}[]} */ (
        abilities["ucan/attest"] || []
      )) {
        const id = settings.proof;
        if (id) {
          attestations[`${id}`] = proof;
        }
      }
    }
  }
  for (const account of Object.values(accounts)) {
    for (const proof of account.proofs) {
      const attestation = attestations[`${proof.cid}`];
      if (attestation) {
        account.addProof(attestation);
      }
    }
  }
  return accounts;
};
/**
 * Attempts to obtains an account access by performing an authentication with
 * the did:mailto account corresponding to given email. Process involves out
 * of bound email verification, so this function returns a promise that will
 * resolve to an account only after access has been granted by the email owner
 * by clicking on the link in the email. If the link is not clicked within the
 * authorization session time bounds (currently 15 minutes), the promise will
 * resolve to an error.
 *
 * @param {{agent: API.Agent}} client
 * @param {EmailAddress} email
 * @param {object} [options]
 * @param {AbortSignal} [options.signal]
 * @returns {Promise<API.Result<Account, Error>>}
 */
const login = async ({ agent }, email, options = {}) => {
  const account = fromEmail(email);
  // If we already have a session for this account we
  // skip the authentication process, otherwise we will
  // end up adding more UCAN proofs and attestations to
  // the store which we then will be sending when using
  // this account.
  // Note: This is not a robust solution as there may be
  // reasons to re-authenticate e.g. previous session is
  // no longer valid because it was revoked. But dropping
  // revoked UCANs from store is something we should do
  // anyway.
  const session = list({ agent }, { account })[account];
  if (session) {
    return { ok: session };
  }
  const result = await request(
    { agent },
    {
      account,
      access: accountAccess,
    }
  );
  const { ok: access, error } = result;
  /* c8 ignore next 2 - don't know how to test this */
  if (error) {
    return { error };
  } else {
    const { ok, error } = await access.claim({ signal: options.signal });
    /* c8 ignore next 2 - don't know how to test this */
    if (error) {
      return { error };
    } else {
      return { ok: new Account({ id: account, proofs: ok.proofs, agent }) };
    }
  }
};
/**
 * @typedef {object} Model
 * @property {API.DidMailto} id
 * @property {API.Agent} agent
 * @property {API.Delegation[]} proofs
 */
class Account {
  /**
   * @param {Model} model
   */
  constructor(model) {
    this.model = model;
    this.plan = new AccountPlan(model);
  }
  get agent() {
    return this.model.agent;
  }
  get proofs() {
    return this.model.proofs;
  }
  did() {
    return this.model.id;
  }
  toEmail() {
    return toEmail(this.did());
  }
  /**
   * @param {API.Delegation} proof
   */
  addProof(proof) {
    this.proofs.push(proof);
  }
  toJSON() {
    return {
      id: this.did(),
      proofs: this.proofs
        // we sort proofs to get a deterministic JSON representation.
        .sort((a, b) => a.cid.toString().localeCompare(b.cid.toString()))
        .map((proof) => proof.toJSON()),
    };
  }
  /**
   * Provisions given `space` with this account.
   *
   * @param {API.SpaceDID} space
   * @param {object} input
   * @param {API.ProviderDID} [input.provider]
   * @param {API.Agent} [input.agent]
   */
  provision(space, input = {}) {
    return add$7(this.agent, {
      ...input,
      account: this.did(),
      consumer: space,
      proofs: this.proofs,
    });
  }
  /**
   * Saves account in the agent store so it can be accessed across sessions.
   *
   * @param {object} input
   * @param {API.Agent} [input.agent]
   */
  async save({ agent = this.agent } = {}) {
    return await importAuthorization(agent, this);
  }
}
class AccountPlan {
  /**
   * @param {Model} model
   */
  constructor(model) {
    this.model = model;
  }
  /**
   * Gets information about the plan associated with this account.
   *
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async get(options) {
    return await get(this.model, {
      ...options,
      account: this.model.id,
      proofs: this.model.proofs,
    });
  }
  /**
   * Sets the plan associated with this account.
   *
   * @param {import('@ucanto/interface').DID} productDID
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async set(productDID, options) {
    return await set(this.model, {
      ...options,
      account: this.model.id,
      product: productDID,
      proofs: this.model.proofs,
    });
  }
  /**
   * Waits for a payment plan to be selected.
   * This method continuously checks the account's payment plan status
   * at a specified interval until a valid plan is selected, or when the timeout is reached,
   * or when the abort signal is aborted.
   *
   * @param {object} [options]
   * @param {number} [options.interval] - The polling interval in milliseconds (default is 1000ms).
   * @param {number} [options.timeout] - The maximum time to wait in milliseconds before throwing a timeout error (default is 15 minutes).
   * @param {AbortSignal} [options.signal] - An optional AbortSignal to cancel the waiting process.
   * @returns {Promise<import('@web3-storage/access').PlanGetSuccess>} - Resolves once a payment plan is selected within the timeout.
   * @throws {Error} - Throws an error if there is an issue retrieving the payment plan or if the timeout is exceeded.
   */
  async wait(options) {
    const startTime = Date.now();
    const interval = options?.interval || 1000; // 1 second
    const timeout = options?.timeout || 60 * 15 * 1000; // 15 minutes
    // eslint-disable-next-line no-constant-condition
    while (true) {
      const res = await this.get();
      if (res.ok) return res.ok;
      if (res.error) {
        throw new Error(`Error retrieving payment plan: ${res.error}`);
      }
      if (Date.now() - startTime > timeout) {
        throw new Error("Timeout: Payment plan selection took too long.");
      }
      if (options?.signal?.aborted) {
        throw new Error("Aborted: Payment plan selection was aborted.");
      }
      console.log("Waiting for payment plan to be selected...");
      await new Promise((resolve) => setTimeout(resolve, interval));
    }
  }
  /**
   *
   * @param {import('@web3-storage/access').AccountDID} accountDID
   * @param {string} returnURL
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async createAdminSession(accountDID, returnURL, options) {
    return await createAdminSession(this.model, {
      ...options,
      account: accountDID,
      returnURL,
    });
  }
  /**
   *
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async subscriptions(options) {
    return await list$1(this.model, {
      ...options,
      account: this.model.id,
      proofs: this.model.proofs,
    });
  }
}

/**
 * Client for interacting with the `usage/*` capabilities.
 */
class UsageClient extends Base {
  /**
   * Get a usage report for the passed space in the given time period.
   *
   * Required delegated capabilities:
   * - `usage/report`
   *
   * @param {import('../types.js').SpaceDID} space
   * @param {{ from: Date, to: Date }} period
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async report(space, period, options) {
    const out = await report(
      { agent: this.agent },
      { ...options, space, period }
    );
    /* c8 ignore next 7 */
    if (!out.ok) {
      throw new Error(`failed ${report$1.can} invocation`, {
        cause: out.error,
      });
    }
    return out.ok;
  }
}
/**
 * Get a usage report for the period.
 *
 * @param {{agent: API.Agent}} client
 * @param {object} options
 * @param {API.SpaceDID} options.space
 * @param {{ from: Date, to: Date }} options.period
 * @param {string} [options.nonce]
 * @param {API.Delegation[]} [options.proofs]
 * @returns {Promise<API.Result<API.UsageReportSuccess, API.UsageReportFailure>>}
 */
const report = async ({ agent }, { space, period, nonce, proofs = [] }) => {
  const receipt = await agent.invokeAndExecute(report$1, {
    with: space,
    proofs,
    nonce,
    nb: {
      period: {
        from: Math.floor(period.from.getTime() / 1000),
        to: Math.ceil(period.to.getTime() / 1000),
      },
    },
  });
  return receipt.out;
};

/**
 * @typedef {object} Model
 * @property {API.SpaceDID} id
 * @property {{name?:string}} [meta]
 * @property {API.Agent} agent
 */
class Space {
  #model;
  /**
   * @param {Model} model
   */
  constructor(model) {
    this.#model = model;
    this.usage = new StorageUsage(model);
  }
  /**
   * The given space name.
   */
  get name() {
    /* c8 ignore next */
    return String(this.#model.meta?.name ?? "");
  }
  /**
   * The DID of the space.
   */
  did() {
    return this.#model.id;
  }
  /**
   * User defined space metadata.
   */
  meta() {
    return this.#model.meta;
  }
}
class StorageUsage {
  #model;
  /**
   * @param {Model} model
   */
  constructor(model) {
    this.#model = model;
  }
  /**
   * Get the current usage in bytes.
   */
  async get() {
    const { agent } = this.#model;
    const space = this.#model.id;
    const now = new Date();
    const period = {
      // we may not have done a snapshot for this month _yet_, so get report
      // from last month -> now
      from: startOfLastMonth(now),
      to: now,
    };
    const result = await report({ agent }, { space, period });
    /* c8 ignore next */
    if (result.error) return result;
    const provider = /** @type {API.ProviderDID} */ (agent.connection.id.did());
    const report$1 = result.ok[provider];
    return {
      /* c8 ignore next */
      ok:
        report$1?.size.final == null ? undefined : BigInt(report$1.size.final),
    };
  }
}
/** @param {string|number|Date} now */
const startOfMonth = (now) => {
  const d = new Date(now);
  d.setUTCDate(1);
  d.setUTCHours(0);
  d.setUTCMinutes(0);
  d.setUTCSeconds(0);
  d.setUTCMilliseconds(0);
  return d;
};
/** @param {string|number|Date} now */
const startOfLastMonth = (now) => {
  const d = startOfMonth(now);
  d.setUTCMonth(d.getUTCMonth() - 1);
  return d;
};

/* c8 ignore start */
/**
 * @template {import('./types.js').Capabilities} C
 * @extends {Delegation<C>}
 */
class AgentDelegation extends Delegation {
  /* c8 ignore stop */
  /** @type {Record<string, any>} */
  #meta;
  /**
   * @param {import('./types.js').UCANBlock<C>} root
   * @param {Map<string, import('./types.js').Block>} [blocks]
   * @param {Record<string, any>} [meta]
   */
  constructor(root, blocks, meta = {}) {
    super(root, blocks);
    this.#meta = meta;
  }
  /**
   * User defined delegation metadata.
   */
  meta() {
    return this.#meta;
  }
}

var encode_1 = encode;
var MSB = 128,
  MSBALL = -128,
  INT = Math.pow(2, 31);
function encode(num, out, offset) {
  out = out || [];
  offset = offset || 0;
  var oldOffset = offset;
  while (num >= INT) {
    out[offset++] = (num & 255) | MSB;
    num /= 128;
  }
  while (num & MSBALL) {
    out[offset++] = (num & 255) | MSB;
    num >>>= 7;
  }
  out[offset] = num | 0;
  encode.bytes = offset - oldOffset + 1;
  return out;
}
var decode = read;
var MSB$1 = 128,
  REST$1 = 127;
function read(buf, offset) {
  var res = 0,
    offset = offset || 0,
    shift = 0,
    counter = offset,
    b,
    l = buf.length;
  do {
    if (counter >= l) {
      read.bytes = 0;
      throw new RangeError("Could not decode varint");
    }
    b = buf[counter++];
    res +=
      shift < 28 ? (b & REST$1) << shift : (b & REST$1) * Math.pow(2, shift);
    shift += 7;
  } while (b >= MSB$1);
  read.bytes = counter - offset;
  return res;
}
var N1 = Math.pow(2, 7);
var N2 = Math.pow(2, 14);
var N3 = Math.pow(2, 21);
var N4 = Math.pow(2, 28);
var N5 = Math.pow(2, 35);
var N6 = Math.pow(2, 42);
var N7 = Math.pow(2, 49);
var N8 = Math.pow(2, 56);
var N9 = Math.pow(2, 63);
var length = function (value) {
  return value < N1
    ? 1
    : value < N2
    ? 2
    : value < N3
    ? 3
    : value < N4
    ? 4
    : value < N5
    ? 5
    : value < N6
    ? 6
    : value < N7
    ? 7
    : value < N8
    ? 8
    : value < N9
    ? 9
    : 10;
};
var varint = {
  encode: encode_1,
  decode: decode,
  encodingLength: length,
};
var _brrp_varint = varint;

const encodeTo = (int, target, offset = 0) => {
  _brrp_varint.encode(int, target, offset);
  return target;
};
const encodingLength = (int) => {
  return _brrp_varint.encodingLength(int);
};

const create$1 = (code, digest) => {
  const size = digest.byteLength;
  const sizeOffset = encodingLength(code);
  const digestOffset = sizeOffset + encodingLength(size);
  const bytes = new Uint8Array(digestOffset + size);
  encodeTo(code, bytes, 0);
  encodeTo(size, bytes, sizeOffset);
  bytes.set(digest, digestOffset);
  return new Digest(code, size, digest, bytes);
};
class Digest {
  constructor(code, size, digest, bytes) {
    this.code = code;
    this.size = size;
    this.digest = digest;
    this.bytes = bytes;
  }
}

const from = ({ name, code, encode }) => new Hasher(name, code, encode);
class Hasher {
  constructor(name, code, encode) {
    this.name = name;
    this.code = code;
    this.encode = encode;
  }
  digest(input) {
    if (input instanceof Uint8Array) {
      const result = this.encode(input);
      return result instanceof Uint8Array
        ? create$1(this.code, result)
        : result.then((digest) => create$1(this.code, digest));
    } else {
      throw Error("Unknown type, must be binary type");
    }
  }
}

const sha = (name) => async (data) =>
  new Uint8Array(await crypto.subtle.digest(name, data));
const sha256 = from({
  name: "sha2-256",
  code: 18,
  encode: sha("SHA-256"),
});

/**
 * Client for interacting with the `blob/*` capabilities.
 */
class BlobClient extends Base {
  /**
   * Store a Blob to the resource.
   *
   * Required delegated capabilities:
   * - `space/blob/add`
   *
   * @param {Blob} blob - blob data.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async add(blob, options = {}) {
    options = {
      receiptsEndpoint: this._receiptsEndpoint.toString(),
      connection: this._serviceConf.upload,
      ...options,
    };
    const conf = await this._invocationConfig([add$4.can]);
    const bytes = new Uint8Array(await blob.arrayBuffer());
    const digest = await sha256.digest(bytes);
    return { digest, ...(await add$2(conf, digest, bytes, options)) };
  }
  /**
   * List blobs stored to the resource.
   *
   * Required delegated capabilities:
   * - `space/blob/list`
   *
   * @param {import('../types.js').ListRequestOptions} [options]
   */
  async list(options = {}) {
    const conf = await this._invocationConfig([list$5.can]);
    options.connection = this._serviceConf.upload;
    return list$3(conf, options);
  }
  /**
   * Remove a stored blob by multihash digest.
   *
   * Required delegated capabilities:
   * - `space/blob/remove`
   *
   * @param {import('multiformats').MultihashDigest} digest - digest of blob to remove.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async remove(digest, options = {}) {
    const conf = await this._invocationConfig([remove$5.can]);
    options.connection = this._serviceConf.upload;
    return remove$3(conf, digest, options);
  }
  /**
   * Gets a stored blob by multihash digest.
   *
   * @param {import('multiformats').MultihashDigest} digest - digest of blob to get.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async get(digest, options = {}) {
    const conf = await this._invocationConfig([get$b.can]);
    options.connection = this._serviceConf.upload;
    return get$8(conf, digest, options);
  }
}

/**
 * Client for interacting with the `index/*` capabilities.
 */
class IndexClient extends Base {
  /**
   * Register an "index" to the resource.
   *
   * Required delegated capabilities:
   * - `space/index/add`
   *
   * @param {import('../types.js').CARLink} index - CID of the CAR file that contains the index data.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async add(index, options = {}) {
    const conf = await this._invocationConfig([add$5.can]);
    options.connection = this._serviceConf.upload;
    return add$1(conf, index, options);
  }
}

/**
 * Client for interacting with the `store/*` capabilities.
 */
class StoreClient extends Base {
  /**
   * Store a DAG encoded as a CAR file.
   *
   * Required delegated capabilities:
   * - `store/add`
   *
   * @deprecated Use `client.capability.blob.add()` instead.
   * @param {Blob} car - CAR file data.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async add(car, options = {}) {
    const conf = await this._invocationConfig([add$a.can]);
    options.connection = this._serviceConf.upload;
    return add$3(conf, car, options);
  }
  /**
   * Get details of a stored item.
   *
   * Required delegated capabilities:
   * - `store/get`
   *
   * @deprecated Use `client.capability.blob.get()` instead.
   * @param {import('../types.js').CARLink} link - Root data CID for the DAG that was stored.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async get(link, options = {}) {
    const conf = await this._invocationConfig([get$h.can]);
    options.connection = this._serviceConf.upload;
    return get$a(conf, link, options);
  }
  /**
   * List CAR files stored to the resource.
   *
   * Required delegated capabilities:
   * - `store/list`
   *
   * @deprecated Use `client.capability.blob.list()` instead.
   * @param {import('../types.js').ListRequestOptions} [options]
   */
  async list(options = {}) {
    const conf = await this._invocationConfig([list$9.can]);
    options.connection = this._serviceConf.upload;
    return list$4(conf, options);
  }
  /**
   * Remove a stored CAR file by CAR CID.
   *
   * Required delegated capabilities:
   * - `store/remove`
   *
   * @deprecated Use `client.capability.blob.remove()` instead.
   * @param {import('../types.js').CARLink} link - CID of CAR file to remove.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async remove(link, options = {}) {
    const conf = await this._invocationConfig([remove$8.can]);
    options.connection = this._serviceConf.upload;
    return remove$4(conf, link, options);
  }
}

/**
 * Client for interacting with the `upload/*` capabilities.
 */
class UploadClient extends Base {
  /**
   * Register an "upload" to the resource.
   *
   * Required delegated capabilities:
   * - `upload/add`
   *
   * @param {import('../types.js').UnknownLink} root - Root data CID for the DAG that was stored.
   * @param {import('../types.js').CARLink[]} shards - CIDs of CAR files that contain the DAG.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async add(root, shards, options = {}) {
    const conf = await this._invocationConfig([add$9.can]);
    options.connection = this._serviceConf.upload;
    return add(conf, root, shards, options);
  }
  /**
   * Get details of an "upload".
   *
   * Required delegated capabilities:
   * - `upload/get`
   *
   * @param {import('../types.js').UnknownLink} root - Root data CID for the DAG that was stored.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async get(root, options = {}) {
    const conf = await this._invocationConfig([get$g.can]);
    options.connection = this._serviceConf.upload;
    return get$7(conf, root, options);
  }
  /**
   * List uploads registered to the resource.
   *
   * Required delegated capabilities:
   * - `upload/list`
   *
   * @param {import('../types.js').ListRequestOptions} [options]
   */
  async list(options = {}) {
    const conf = await this._invocationConfig([list$8.can]);
    options.connection = this._serviceConf.upload;
    return list$2(conf, options);
  }
  /**
   * Remove an upload by root data CID.
   *
   * Required delegated capabilities:
   * - `upload/remove`
   *
   * @param {import('../types.js').UnknownLink} root - Root data CID to remove.
   * @param {import('../types.js').RequestOptions} [options]
   */
  async remove(root, options = {}) {
    const conf = await this._invocationConfig([remove$7.can]);
    options.connection = this._serviceConf.upload;
    return remove$2(conf, root, options);
  }
}

/**
 * Client for interacting with the `space/*` capabilities.
 */
class SpaceClient extends Base {
  /**
   * Get information about a space.
   *
   * Required delegated capabilities:
   * - `space/info`
   *
   * @param {import('../types.js').DID} space - DID of the space to retrieve info about.
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async info(space, options) {
    return await this._agent.getSpaceInfo(space, options);
  }
  /**
   * Record egress data for a served resource.
   * It will execute the capability invocation to find the customer and then record the egress data for the resource.
   *
   * Required delegated capabilities:
   * - `space/content/serve/egress/record`
   *
   * @param {object} egressData
   * @param {import('../types.js').SpaceDID} egressData.space
   * @param {API.UnknownLink} egressData.resource
   * @param {number} egressData.bytes
   * @param {string} egressData.servedAt
   * @param {object} [options]
   * @param {string} [options.nonce]
   * @param {API.Delegation[]} [options.proofs]
   * @returns {Promise<API.EgressRecordSuccess>}
   */
  async egressRecord(egressData, options) {
    const out = await egressRecord(
      { agent: this.agent },
      { ...egressData },
      { ...options }
    );
    if (!out.ok) {
      throw new Error(`failed ${egressRecord$1.can} invocation`, {
        cause: out.error,
      });
    }
    return /** @type {API.EgressRecordSuccess} */ (out.ok);
  }
}
/**
 * Record egress data for a resource from a given space.
 *
 * @param {{agent: API.Agent}} client
 * @param {object} egressData
 * @param {API.SpaceDID} egressData.space
 * @param {API.UnknownLink} egressData.resource
 * @param {number} egressData.bytes
 * @param {string} egressData.servedAt
 * @param {object} options
 * @param {string} [options.nonce]
 * @param {API.Delegation[]} [options.proofs]
 */
const egressRecord = async (
  { agent },
  { space, resource, bytes, servedAt },
  { nonce, proofs = [] }
) => {
  const receipt = await agent.invokeAndExecute(egressRecord$1, {
    with: space,
    proofs,
    nonce,
    nb: {
      resource,
      bytes,
      servedAt: Math.floor(new Date(servedAt).getTime() / 1000),
    },
  });
  return receipt.out;
};

/**
 * Client for interacting with the `filecoin/*` capabilities.
 */
class FilecoinClient extends Base {
  /**
   * Offer a Filecoin "piece" to the resource.
   *
   * Required delegated capabilities:
   * - `filecoin/offer`
   *
   * @param {import('multiformats').UnknownLink} content
   * @param {import('@web3-storage/capabilities/types').PieceLink} piece
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async offer(content, piece, options) {
    const conf = await this._invocationConfig([filecoinOffer$1.can]);
    return filecoinOffer(conf, content, piece, {
      ...options,
      connection: this._serviceConf.filecoin,
    });
  }
  /**
   * Request info about a content piece in Filecoin deals
   *
   * Required delegated capabilities:
   * - `filecoin/info`
   *
   * @param {import('@web3-storage/capabilities/types').PieceLink} piece
   * @param {object} [options]
   * @param {string} [options.nonce]
   */
  async info(piece, options) {
    const conf = await this._invocationConfig([filecoinInfo$1.can]);
    return filecoinInfo(conf, piece, {
      ...options,
      connection: this._serviceConf.filecoin,
    });
  }
}

class CouponAPI extends Base {
  /**
   * Redeems coupon from the the the archive. Throws an error if the coupon
   * password is invalid or if provided archive is not a valid.
   *
   * @param {Uint8Array} archive
   * @param {object} [options]
   * @param {string} [options.password]
   */
  async redeem(archive, options = {}) {
    const { agent } = this;
    const coupon = unwrap(await extract(archive));
    return unwrap(await redeem(coupon, { ...options, agent }));
  }
  /**
   * Issues a coupon for the given delegation.
   *
   * @param {Omit<CouponOptions, 'issuer'>} options
   */
  async issue({ proofs = [], ...options }) {
    const { agent } = this;
    return await issue({
      ...options,
      issuer: agent.issuer,
      proofs: [...proofs, ...agent.proofs(options.capabilities)],
    });
  }
}
/**
 * Extracts coupon from the archive.
 *
 * @param {Uint8Array} archive
 * @returns {Promise<API.Result<Coupon, Error>>}
 */
const extract = async (archive) => {
  const { ok: ok$1, error } = await extract$1(archive);
  return ok$1 ? ok(new Coupon({ proofs: [ok$1] })) : error$1(error);
};
/**
 * Encodes coupon into an archive.
 *
 * @param {Model} coupon
 */
const archive = async (coupon) => {
  const [delegation] = coupon.proofs;
  return await archive$2(delegation);
};
/**
 * Issues a coupon for the given delegation.
 *
 * @typedef {Omit<import('@ucanto/interface').DelegationOptions<API.Capabilities>, 'audience'> & { password?: string }} CouponOptions
 * @param {CouponOptions} options
 */
const issue = async ({ password = "", ...options }) => {
  const audience = await deriveSigner(password);
  const delegation = await delegate$3({
    ...options,
    audience,
  });
  return new Coupon({ proofs: [delegation] });
};
/**
 * @typedef {object} Model
 * @property {[API.Delegation]} proofs
 */
/**
 * Redeems granted access with the given agent from the given coupon.
 *
 * @param {Model} coupon
 * @param {object} options
 * @param {API.Agent} options.agent
 * @param {string} [options.password]
 * @returns {Promise<API.Result<GrantedAccess, Error>>}
 */
const redeem = async (coupon, { agent, password = "" }) => {
  const audience = await deriveSigner(password);
  const [delegation] = coupon.proofs;
  if (delegation.audience.did() !== audience.did()) {
    return error$1(
      new RangeError(
        password === ""
          ? "Extracting account requires a password"
          : "Provided password is invalid"
      )
    );
  } else {
    const authorization = await delegate$3({
      issuer: audience,
      audience: agent,
      capabilities: delegation.capabilities,
      expiration: delegation.expiration,
      notBefore: delegation.notBefore,
      proofs: [delegation],
    });
    return ok(new GrantedAccess({ agent, proofs: [authorization] }));
  }
};
/**
 * @param {string} password
 */
const deriveSigner = async (password) => {
  const { digest } = await sha256$4.digest(new TextEncoder().encode(password));
  return await derive(digest);
};
class Coupon {
  /**
   * @param {Model} model
   */
  constructor(model) {
    this.model = model;
  }
  get proofs() {
    return this.model.proofs;
  }
  /**
   *
   * @param {API.Agent} agent
   * @param {object} [options]
   * @param {string} [options.password]
   */
  redeem(agent, options = {}) {
    return redeem(this, { ...options, agent });
  }
  archive() {
    return archive(this);
  }
}

class Client extends Base {
  /**
   * @param {import('@web3-storage/access').AgentData} agentData
   * @param {object} [options]
   * @param {import('./types.js').ServiceConf} [options.serviceConf]
   * @param {URL} [options.receiptsEndpoint]
   */
  constructor(agentData, options) {
    super(agentData, options);
    this.capability = {
      access: new AccessClient(agentData, options),
      filecoin: new FilecoinClient(agentData, options),
      index: new IndexClient(agentData, options),
      plan: new PlanClient(agentData, options),
      space: new SpaceClient(agentData, options),
      blob: new BlobClient(agentData, options),
      store: new StoreClient(agentData, options),
      subscription: new SubscriptionClient(agentData, options),
      upload: new UploadClient(agentData, options),
      usage: new UsageClient(agentData, options),
    };
    this.coupon = new CouponAPI(agentData, options);
  }
  did() {
    return this._agent.did();
  }
  /* c8 ignore start - testing websockets is hard */
  /**
   * @deprecated - Use client.login instead.
   *
   * Authorize the current agent to use capabilities granted to the passed
   * email account.
   *
   * @param {`${string}@${string}`} email
   * @param {object} [options]
   * @param {AbortSignal} [options.signal]
   * @param {Iterable<{ can: import('./types.js').Ability }>} [options.capabilities]
   */
  async authorize(email, options) {
    await this.capability.access.authorize(email, options);
  }
  /**
   * @param {Account.EmailAddress} email
   * @param {object} [options]
   * @param {AbortSignal} [options.signal]
   */
  async login(email, options = {}) {
    const account = unwrap(await login(this, email, options));
    unwrap(await account.save());
    return account;
  }
  /* c8 ignore stop */
  /**
   * List all accounts that agent has stored access to.
   *
   * @returns {Record<DIDMailto, Account>} A dictionary with `did:mailto` as keys and `Account` instances as values.
   */
  accounts() {
    return list(this);
  }
  /**
   * Uploads a file to the service and returns the root data CID for the
   * generated DAG.
   *
   * Required delegated capabilities:
   * - `filecoin/offer`
   * - `space/blob/add`
   * - `space/index/add`
   * - `upload/add`
   *
   * @param {import('./types.js').BlobLike} file - File data.
   * @param {import('./types.js').UploadFileOptions} [options]
   */
  async uploadFile(file, options = {}) {
    const conf = await this._invocationConfig([
      add$4.can,
      add$5.can,
      filecoinOffer$1.can,
      add$9.can,
    ]);
    options = {
      receiptsEndpoint: this._receiptsEndpoint.toString(),
      connection: this._serviceConf.upload,
      ...options,
    };
    return uploadFile(conf, file, options);
  }
  /**
   * Uploads a directory of files to the service and returns the root data CID
   * for the generated DAG. All files are added to a container directory, with
   * paths in the file names preserved.
   *
   * Required delegated capabilities:
   * - `filecoin/offer`
   * - `space/blob/add`
   * - `space/index/add`
   * - `upload/add`
   *
   * @param {import('./types.js').FileLike[]} files - File data.
   * @param {import('./types.js').UploadDirectoryOptions} [options]
   */
  async uploadDirectory(files, options = {}) {
    const conf = await this._invocationConfig([
      add$4.can,
      add$5.can,
      filecoinOffer$1.can,
      add$9.can,
    ]);
    options = {
      receiptsEndpoint: this._receiptsEndpoint.toString(),
      connection: this._serviceConf.upload,
      ...options,
    };
    return uploadDirectory(conf, files, options);
  }
  /**
   * Uploads a CAR file to the service.
   *
   * The difference between this function and `capability.blob.add` is that
   * the CAR file is automatically sharded, an index is generated, uploaded and
   * registered (see `capability.index.add`) and finally an an "upload" is
   * registered, linking the individual shards (see `capability.upload.add`).
   *
   * Use the `onShardStored` callback to obtain the CIDs of the CAR file shards.
   *
   * Required delegated capabilities:
   * - `filecoin/offer`
   * - `space/blob/add`
   * - `space/index/add`
   * - `upload/add`
   *
   * @param {import('./types.js').BlobLike} car - CAR file.
   * @param {import('./types.js').UploadOptions} [options]
   */
  async uploadCAR(car, options = {}) {
    const conf = await this._invocationConfig([
      add$4.can,
      add$5.can,
      filecoinOffer$1.can,
      add$9.can,
    ]);
    options = {
      receiptsEndpoint: this._receiptsEndpoint.toString(),
      connection: this._serviceConf.upload,
      ...options,
    };
    return uploadCAR(conf, car, options);
  }
  /**
   * Get a receipt for an executed task by its CID.
   *
   * @param {import('multiformats').UnknownLink} taskCid
   */
  async getReceipt(taskCid) {
    const receiptsEndpoint = new URL(this._receiptsEndpoint).toString();
    return poll(taskCid, { receiptsEndpoint });
  }
  /**
   * Return the default provider.
   */
  defaultProvider() {
    return this._agent.connection.id.did();
  }
  /**
   * The current space.
   */
  currentSpace() {
    const agent = this._agent;
    const id = agent.currentSpace();
    if (!id) return;
    const meta = agent.spaces.get(id);
    return new Space({ id, meta, agent });
  }
  /**
   * Use a specific space.
   *
   * @param {import('./types.js').DID} did
   */
  async setCurrentSpace(did) {
    await this._agent.setCurrentSpace(/** @type {`did:key:${string}`} */ (did));
  }
  /**
   * Spaces available to this agent.
   */
  spaces() {
    return [...this._agent.spaces].map(([id, meta]) => {
      // @ts-expect-error id is not did:key
      return new Space({ id, meta, agent: this._agent });
    });
  }
  /**
   * Creates a new space with a given name.
   * If an account is not provided, the space is created without any delegation and is not saved, hence it is a temporary space.
   * When an account is provided in the options argument, then it creates a delegated recovery account
   * by provisioning the space, saving it and then delegating access to the recovery account.
   * In addition, it authorizes the listed Gateway Services to serve content from the created space.
   * It is done by delegating the `space/content/serve/*` capability to the Gateway Service.
   * User can skip the Gateway authorization by setting the `skipGatewayAuthorization` option to `true`.
   * If no gateways are specified or the `skipGatewayAuthorization` flag is not set, the client will automatically grant access
   * to the Storacha Gateway by default (https://freewaying.dag.haus/).
   *
   * @typedef {import('./types.js').ConnectionView<import('./types.js').ContentServeService>} ConnectionView
   *
   * @typedef {object} SpaceCreateOptions
   * @property {Account.Account} [account] - The account configured as the recovery account for the space.
   * @property {Array<ConnectionView>} [authorizeGatewayServices] - The DID Key or DID Web of the Gateway to authorize to serve content from the created space.
   * @property {boolean} [skipGatewayAuthorization] - Whether to skip the Gateway authorization. It means that the content of the space will not be served by any Gateway.
   *
   * @param {string} name - The name of the space to create.
   * @param {SpaceCreateOptions} [options] - Options for the space creation.
   * @returns {Promise<import("./space.js").OwnedSpace>} The created space owned by the agent.
   */
  async createSpace(name, options) {
    // Save the space to authorize the client to use the space
    const space = await this._agent.createSpace(name);
    const account = options?.account;
    if (account) {
      // Provision the account with the space
      const provisionResult = await account.provision(space.did());
      if (provisionResult.error) {
        throw new Error(
          `failed to provision account: ${provisionResult.error.message}`,
          { cause: provisionResult.error }
        );
      }
      // Save the space to authorize the client to use the space
      await space.save();
      // Create a recovery for the account
      const recovery = await space.createRecovery(account.did());
      // Delegate space access to the recovery
      const delegationResult = await this.capability.access.delegate({
        space: space.did(),
        delegations: [recovery],
      });
      if (delegationResult.error) {
        throw new Error(
          `failed to authorize recovery account: ${delegationResult.error.message}`,
          { cause: delegationResult.error }
        );
      }
    }
    // Authorize the listed Gateway Services to serve content from the created space
    if (options?.skipGatewayAuthorization !== true) {
      let authorizeGatewayServices = options?.authorizeGatewayServices;
      if (!authorizeGatewayServices || authorizeGatewayServices.length === 0) {
        // If no Gateway Services are provided, authorize the Storacha Gateway Service
        authorizeGatewayServices = [
          connect({
            id: {
              did: () =>
                /** @type {`did:${string}:${string}`} */ (
                  /* c8 ignore next - default prod gateway id is not used in tests */
                  process.env.DEFAULT_GATEWAY_ID ?? "did:web:w3s.link"
                ),
            },
            codec: outbound,
            channel: open$2({
              url: new URL(
                /* c8 ignore next - default prod gateway url is not used in tests */
                process.env.DEFAULT_GATEWAY_URL ?? "https://w3s.link"
              ),
            }),
          }),
        ];
      }
      // Save the space to authorize the client to use the space
      await space.save();
      for (const serviceConnection of authorizeGatewayServices) {
        await authorizeContentServe(this, space, serviceConnection);
      }
    }
    return space;
  }
  /**
     * Share an existing space with another Storacha account via email address delegation.
     * Delegates access to the space to the specified email account with the following permissions:
     * - space/* - for managing space metadata
     * - blob/* - for managing blobs
     * - store/* - for managing stores
     * - upload/*- for registering uploads
     * - access/* - for re-delegating access to other devices
     * - filecoin/* - for submitting to the filecoin pipeline
     * - usage/* - for querying usage
     * The default expiration is set to infinity.
     *
     * @typedef {object} ShareOptions
     * @property {import('./types.js').ServiceAbility[]} abilities - Abilities to delegate to the delegate account.
     * @property {number} expiration - Expiration time in seconds.
     
     * @param {import("./types.js").EmailAddress} delegateEmail - Email of the account to share the space with.
     * @param {import('./types.js').SpaceDID} spaceDID - The DID of the space to share.
     * @param {ShareOptions} [options] - Options for the delegation.
     *
     * @returns {Promise<import('./delegation.js').AgentDelegation<any>>} Resolves with the AgentDelegation instance once the space is successfully shared.
     * @throws {Error} - Throws an error if there is an issue delegating access to the space.
     */
  async shareSpace(
    delegateEmail,
    spaceDID,
    options = {
      abilities: [
        "space/*",
        "store/*",
        "upload/*",
        "access/*",
        "usage/*",
        "filecoin/*",
      ],
      expiration: Infinity,
    }
  ) {
    const { abilities, ...restOptions } = options;
    const currentSpace = this.agent.currentSpace();
    try {
      // Make sure the agent is using the shared space before delegating
      await this.agent.setCurrentSpace(spaceDID);
      // Delegate capabilities to the delegate account to access the **current space**
      const { root, blocks } = await this.agent.delegate({
        ...restOptions,
        abilities,
        audience: {
          did: () => fromEmail(email(delegateEmail)),
        },
        // @ts-expect-error audienceMeta is not defined in ShareOptions
        audienceMeta: options.audienceMeta ?? {},
      });
      const delegation = new AgentDelegation(root, blocks, {
        audience: delegateEmail,
      });
      const sharingResult = await this.capability.access.delegate({
        space: spaceDID,
        delegations: [delegation],
      });
      if (sharingResult.error) {
        throw new Error(
          `failed to share space with ${delegateEmail}: ${sharingResult.error.message}`,
          {
            cause: sharingResult.error,
          }
        );
      }
      return delegation;
    } finally {
      // Reset to the original space if it was different
      if (currentSpace && currentSpace !== spaceDID) {
        await this.agent.setCurrentSpace(currentSpace);
      }
    }
  }
  /* c8 ignore stop */
  /**
   * Add a space from a received proof.
   *
   * @param {import('./types.js').Delegation} proof
   */
  async addSpace(proof) {
    return await this._agent.importSpaceFromDelegation(proof);
  }
  /**
   * Get all the proofs matching the capabilities.
   *
   * Proofs are delegations with an _audience_ matching the agent DID.
   *
   * @param {import('./types.js').Capability[]} [caps] - Capabilities to
   * filter by. Empty or undefined caps with return all the proofs.
   */
  proofs(caps) {
    return this._agent.proofs(caps);
  }
  /**
   * Add a proof to the agent. Proofs are delegations with an _audience_
   * matching the agent DID.
   *
   * @param {import('./types.js').Delegation} proof
   */
  async addProof(proof) {
    await this._agent.addProof(proof);
  }
  /**
   * Get delegations created by the agent for others.
   *
   * @param {import('./types.js').Capability[]} [caps] - Capabilities to
   * filter by. Empty or undefined caps with return all the delegations.
   */
  delegations(caps) {
    const delegations = [];
    for (const { delegation, meta } of this._agent.delegationsWithMeta(caps)) {
      delegations.push(
        new AgentDelegation(delegation.root, delegation.blocks, meta)
      );
    }
    return delegations;
  }
  /**
   * Create a delegation to the passed audience for the given abilities with
   * the _current_ space as the resource.
   *
   * @param {import('./types.js').Principal} audience
   * @param {import('./types.js').ServiceAbility[]} abilities
   * @param {Omit<import('./types.js').UCANOptions, 'audience'> & { audienceMeta?: import('./types.js').AgentMeta }} [options]
   */
  async createDelegation(audience, abilities, options = {}) {
    const audienceMeta = options.audienceMeta ?? {
      name: "agent",
      type: "device",
    };
    const { root, blocks } = await this._agent.delegate({
      ...options,
      abilities,
      audience,
      audienceMeta,
    });
    return new AgentDelegation(root, blocks, { audience: audienceMeta });
  }
  /**
   * Revoke a delegation by CID.
   *
   * If the delegation was issued by this agent (and therefore is stored in the
   * delegation store) you can just pass the CID. If not, or if the current agent's
   * delegation store no longer contains the delegation, you MUST pass a chain of
   * proofs that proves your authority to revoke this delegation as `options.proofs`.
   *
   * @param {import('@ucanto/interface').UCANLink} delegationCID
   * @param {object} [options]
   * @param {import('@ucanto/interface').Delegation[]} [options.proofs]
   */
  async revokeDelegation(delegationCID, options = {}) {
    return this._agent.revoke(delegationCID, {
      proofs: options.proofs,
    });
  }
  /**
   * Removes association of a content CID with the space. Optionally, also removes
   * association of CAR shards with space.
   *
   * ⚠️ If `shards` option is `true` all shards will be deleted even if there is another upload(s) that
   * reference same shards, which in turn could corrupt those uploads.
   *
   * Required delegated capabilities:
   * - `space/blob/remove`
   * - `store/remove`
   * - `upload/get`
   * - `upload/remove`
   *
   * @param {import('multiformats').UnknownLink} contentCID
   * @param {object} [options]
   * @param {boolean} [options.shards]
   */
  async remove(contentCID, options = {}) {
    // Shortcut if there is no request to remove shards
    if (!options.shards) {
      // Remove association of content CID with selected space.
      await this.capability.upload.remove(contentCID);
      return;
    }
    // Get shards associated with upload.
    const upload = await this.capability.upload.get(contentCID);
    // Remove shards
    if (upload.shards?.length) {
      await Promise.allSettled(
        upload.shards.map(async (shard) => {
          try {
            const res = await this.capability.blob.remove(shard.multihash);
            /* c8 ignore start */
            // if no size, the blob was not found, try delete from store
            if (res.ok && res.ok.size === 0) {
              await this.capability.store.remove(shard);
            }
          } catch (/** @type {any} */ error) {
            // If not found, we can tolerate error as it may be a consecutive call for deletion where first failed
            if (error?.cause?.name !== "StoreItemNotFound") {
              throw new Error(`failed to remove shard: ${shard}`, {
                cause: error,
              });
            }
            /* c8 ignore next 4 */
          }
        })
      );
    }
    // Remove association of content CID with selected space.
    await this.capability.upload.remove(contentCID);
  }
}
/**
 * Authorizes an audience to serve content from the provided space and record egress events.
 * It also publishes the delegation to the content serve service.
 * Delegates the following capabilities to the audience:
 * - `space/content/serve/*`
 *
 * @param {Client} client - The w3up client instance.
 * @param {import('./types.js').OwnedSpace} space - The space to authorize the audience for.
 * @param {import('./types.js').ConnectionView<import('./types.js').ContentServeService>} connection - The connection to the Content Serve Service that will handle, validate, and store the access/delegate UCAN invocation.
 * @param {object} [options] - Options for the content serve authorization invocation.
 * @param {`did:${string}:${string}`} [options.audience] - The Web DID of the audience (gateway or peer) to authorize.
 * @param {number} [options.expiration] - The time at which the delegation expires in seconds from unix epoch.
 */
const authorizeContentServe = async (
  client,
  space,
  connection,
  options = {}
) => {
  const currentSpace = client.currentSpace();
  try {
    // Set the current space to the space we are authorizing the gateway for, otherwise the delegation will fail
    await client.setCurrentSpace(space.did());
    /** @type {import('@ucanto/client').Principal<`did:${string}:${string}`>} */
    const audience = {
      did: () => options.audience ?? connection.id.did(),
    };
    // Grant the audience the ability to serve content from the space, it includes existing proofs automatically
    const delegation = await client.createDelegation(
      audience,
      [contentServe.can],
      {
        expiration: options.expiration ?? Infinity,
      }
    );
    // Publish the delegation to the content serve service
    const accessProofs = client.proofs([
      { can: access$1.can, with: space.did() },
    ]);
    const verificationResult = await delegate$2
      .invoke({
        issuer: client.agent.issuer,
        audience,
        with: space.did(),
        proofs: [...accessProofs, delegation],
        nb: {
          delegations: {
            [delegation.cid.toString()]: delegation.cid,
          },
        },
      })
      .execute(connection);
    /* c8 ignore next 8 - can't mock this error */
    if (verificationResult.out.error) {
      throw new Error(
        `failed to publish delegation for audience ${audience.did()}: ${
          verificationResult.out.error.message
        }`,
        {
          cause: verificationResult.out.error,
        }
      );
    }
    return { ok: { ...verificationResult.out.ok, delegation } };
  } finally {
    if (currentSpace) {
      await client.setCurrentSpace(currentSpace.did());
    }
  }
};

new Set(abilitiesAsStrings);

/**
 * The main entry point for the `@web3-storage/w3up-client` package.
 *
 * Use the static {@link create} function to create a new {@link Client} object.
 *
 * @module
 */
/**
 * Create a new w3up client.
 *
 * If no backing store is passed one will be created that is appropriate for
 * the environment.
 *
 * If the backing store is empty, a new signing key will be generated and
 * persisted to the store. In the browser an unextractable RSA key will be
 * generated by default. In other environments an Ed25519 key is generated.
 *
 * If the backing store already has data stored, it will be loaded and used.
 *
 * @type {import('./types.js').ClientFactory}
 */
async function create(options = {}) {
  const store = options.store ?? new StoreIndexedDB("w3up-client");
  const raw = await store.load();
  if (raw) {
    const data = AgentData.fromExport(raw, { store });
    if (options.principal && data.principal.did() !== options.principal.did()) {
      throw new Error(
        `store cannot be used with ${options.principal.did()}, stored principal and passed principal must match`
      );
    }
    return new Client(data, options);
  }
  const principal = options.principal ?? (await generate());
  const data = await AgentData.create({ principal }, { store });
  return new Client(data, options);
}

// Import directly from internal node_modules

/**
 * Creates a new Web3 Storage client
 * @returns {Promise<Object>} The Web3 Storage client
 */
async function createW3Client() {
  try {
    // Create client
    const client = await create();
    return client;
  } catch (err) {
    console.error("Error creating Web3 Storage client:", err);
    throw err;
  }
}

/**
 * Authenticate with Web3 Storage using provided email
 * @param {Object} client - The Web3 Storage client
 * @param {string} email - Email for authentication
 * @returns {Promise<Object>} Authentication result
 */
async function authenticateWithEmail(client, email) {
  try {
    await client.login(email);
    return { success: true, email };
  } catch (err) {
    console.error("Error authenticating with Web3 Storage:", err);
    throw err;
  }
}

/**
 * Store content on Web3 Storage
 * @param {Object} client - The authenticated Web3 Storage client
 * @param {any} content - Content to store (File or Blob)
 * @returns {Promise<Object>} Result with CID
 */
async function storeContent(client, content) {
  try {
    // Make sure we have a space
    const space = await client.currentSpace();
    if (!space) {
      const spaces = await client.spaces();
      if (spaces.length === 0) {
        await client.createSpace("my-space");
      }
    }

    // Store the content
    const uploadable = new Blob([content]);
    const cid = await client.uploadBlob(uploadable);

    return {
      success: true,
      cid: cid.toString(),
    };
  } catch (err) {
    console.error("Error storing content:", err);
    throw err;
  }
}

/**
 * List all uploaded content
 * @param {Object} client - The authenticated Web3 Storage client
 * @returns {Promise<Array>} List of uploaded content
 */
async function listUploads(client) {
  try {
    const space = await client.currentSpace();
    const uploads = await client.capability.store.list({ space: space.did() });
    return uploads;
  } catch (err) {
    console.error("Error listing uploads:", err);
    throw err;
  }
}

// Export Web3 Storage utilities
const w3Storage = {
  create: createW3Client,
  authenticate: authenticateWithEmail,
  store: storeContent,
  list: listUploads,
};

export {
  authenticateWithEmail,
  createW3Client,
  w3Storage as default,
  listUploads,
  storeContent,
  w3Storage,
};
//# sourceMappingURL=index.js.map
